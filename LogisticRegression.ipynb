{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUtpGPexe9vjk5jII68soH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharanya-Parimanoharan/AI-Generated-Text-Detection/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op6r1AefpHHe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import  balanced_accuracy_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch"
      ],
      "metadata": {
        "id": "MZFd7DtSzTZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (rest of your code for data loading and preprocessing)\n",
        "df = pd.read_csv(\"drive/MyDrive/data_2.csv\")\n"
      ],
      "metadata": {
        "id": "btAXg46FzXew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "df['text'] = df['text'].apply(remove_punctuation)\n",
        "\n",
        "#Handle missing values\n",
        "missing_values = df.isnull().sum()\n",
        "df = df.dropna()\n",
        "df['text'].fillna(\"NA\", inplace=True)\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "\n",
        "\n",
        "print(df.tail())\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "df[\"label_int\"]= np.where(df[\"label\"]=='\"AI\"', 1, 0)\n",
        "\n",
        "print(df.head)\n",
        "\n",
        "print(df['label_int'].value_counts())\n"
      ],
      "metadata": {
        "id": "lK8z7hmVzcJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label_int'].describe())\n"
      ],
      "metadata": {
        "id": "RYdig_hZzf2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label_int\"], random_state=0)\n",
        "\n",
        "# Feature extraction (bag-of-words and TF-IDF)\n",
        "vect = CountVectorizer(min_df=5, ngram_range=(1, 2)).fit(X_train)\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(min_df=5, ngram_range=(1, 2)).fit(X_train)\n",
        "X_train_tfidf = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf = tfidf_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "RbaQjsghziFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Feature extraction for POS tags\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Extract the POS tags and concatenate them into a single string\n",
        "    pos_tags_str = ' '.join([tag for _, tag in pos_tags])\n",
        "\n",
        "    return pos_tags_str\n",
        "\n",
        "# Apply the preprocess_text function to your text data\n",
        "X_train_pos = [preprocess_text(text) for text in X_train]\n",
        "X_test_pos = [preprocess_text(text) for text in X_test]\n",
        "\n",
        "\n",
        "\n",
        "# Create a CountVectorizer for POS tags\n",
        "pos_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_pos_vec = pos_vectorizer.fit_transform(X_train_pos)\n",
        "\n",
        "# Transform test data\n",
        "X_test_pos_vec = pos_vectorizer.transform(X_test_pos)\n"
      ],
      "metadata": {
        "id": "JmyvzxcgzmFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have labels (y_train and y_test)\n",
        "\n",
        "# Extract indices for 'AI' and 'Human' labels in the training set\n",
        "indices_ai_train = [i for i, label in enumerate(y_train) if label == 1]\n",
        "indices_human_train = [i for i, label in enumerate(y_train) if label == 0]\n",
        "\n",
        "# Extract indices for 'AI' and 'Human' labels in the test set\n",
        "indices_ai_test = [i for i, label in enumerate(y_test) if label == 1]\n",
        "indices_human_test = [i for i, label in enumerate(y_test) if label == 0]\n",
        "\n",
        "# Plot POS tag distribution for 'AI' and 'Human' in the training set\n",
        "counts_ai_train = np.array(X_train_pos_vec[indices_ai_train].sum(axis=0))[0]\n",
        "counts_human_train = np.array(X_train_pos_vec[indices_human_train].sum(axis=0))[0]\n",
        "\n",
        "bar_width = 0.35\n",
        "r1 = np.arange(len(pos_tags_train))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(r1, counts_ai_train, color='blue', width=bar_width, edgecolor='grey', label='AI - Train')\n",
        "plt.bar(r2, counts_human_train, color='orange', width=bar_width, edgecolor='grey', label='Human - Train')\n",
        "\n",
        "plt.title('POS Tag Distribution - Training Set')\n",
        "plt.xlabel('POS Tags')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks([r + bar_width/2 for r in range(len(pos_tags_train))], pos_tags_train, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot POS tag distribution for 'AI' and 'Human' in the test set\n",
        "counts_ai_test = np.array(X_test_pos_vec[indices_ai_test].sum(axis=0))[0]\n",
        "counts_human_test = np.array(X_test_pos_vec[indices_human_test].sum(axis=0))[0]\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(r1, counts_ai_test, color='blue', width=bar_width, edgecolor='grey', label='AI - Test')\n",
        "plt.bar(r2, counts_human_test, color='orange', width=bar_width, edgecolor='grey', label='Human - Test')\n",
        "\n",
        "plt.title('POS Tag Distribution - Test Set')\n",
        "plt.xlabel('POS Tags')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks([r + bar_width/2 for r in range(len(pos_tags_test))], pos_tags_test, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CVNgtG2iyMlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Extract indices for 'AI' and 'Human' labels in the training set\n",
        "indices_ai_train = [i for i, label in enumerate(y_train) if label == 1]\n",
        "indices_human_train = [i for i, label in enumerate(y_train) if label == 0]\n",
        "\n",
        "# Extract indices for 'AI' and 'Human' labels in the test set\n",
        "indices_ai_test = [i for i, label in enumerate(y_test) if label == 1]\n",
        "indices_human_test = [i for i, label in enumerate(y_test) if label == 0]\n",
        "\n",
        "# Plot average TF-IDF values for 'AI' and 'Human' in the training set\n",
        "tfidf_ai_train = X_train_tfidf[indices_ai_train].mean(axis=0).A[0]\n",
        "tfidf_human_train = X_train_tfidf[indices_human_train].mean(axis=0).A[0]\n",
        "\n",
        "bar_width = 0.35\n",
        "r1 = np.arange(X_train_tfidf.shape[1])\n",
        "r2 = [x + bar_width for x in r1]\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(r1, tfidf_ai_train, color='blue', width=bar_width, edgecolor='grey', label='AI - Train - TF-IDF')\n",
        "plt.bar(r2, tfidf_human_train, color='orange', width=bar_width, edgecolor='grey', label='Human - Train - TF-IDF')\n",
        "\n",
        "plt.title('TF-IDF Analysis - Training Set')\n",
        "plt.xlabel('TF-IDF Features')\n",
        "plt.ylabel('Average TF-IDF Value')\n",
        "plt.xticks([r + bar_width/2 for r in range(X_train_tfidf.shape[1])], tfidf_vectorizer.get_feature_names_out(), rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Repeat the process for the test set\n",
        "tfidf_ai_test = X_test_tfidf[indices_ai_test].mean(axis=0).A[0]\n",
        "tfidf_human_test = X_test_tfidf[indices_human_test].mean(axis=0).A[0]\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(r1, tfidf_ai_test, color='blue', width=bar_width, edgecolor='grey', label='AI - Test - TF-IDF')\n",
        "plt.bar(r2, tfidf_human_test, color='orange', width=bar_width, edgecolor='grey', label='Human - Test - TF-IDF')\n",
        "\n",
        "plt.title('TF-IDF Analysis - Test Set')\n",
        "plt.xlabel('TF-IDF Features')\n",
        "plt.ylabel('Average TF-IDF Value')\n",
        "plt.xticks([r + bar_width/2 for r in range(X_test_tfidf.shape[1])], tfidf_vectorizer.get_feature_names_out(), rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hWdJVCSR3-UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Concatenate features\n",
        "X_train_combined = np.hstack((X_train_vectorized.toarray(), X_train_tfidf.toarray(), X_train_pos_vec.toarray()))\n",
        "X_test_combined = np.hstack((X_test_vectorized.toarray(), X_test_tfidf.toarray(), X_test_pos_vec.toarray()))\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
        "X_test_scaled = scaler.fit_transform(X_test_combined)\n"
      ],
      "metadata": {
        "id": "p0dZ8DGKzp2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a simple logistic regression model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))"
      ],
      "metadata": {
        "id": "zDJqPdTKz0sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your data to PyTorch tensors if not done already\n",
        "X_train_tensor = torch.Tensor(X_train_scaled)\n",
        "y_train_tensor = torch.Tensor(y_train.values)\n",
        "\n",
        "X_val_tensor = torch.Tensor(X_val_scaled)\n",
        "y_val_tensor = torch.Tensor(y_val)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = LogisticRegressionModel(input_size)\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "6L8Z4r58z4xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_loss_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor.view(-1, 1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss_values.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor.view(-1, 1))\n",
        "        val_loss_values.append(val_loss.item())\n",
        "\n",
        "    # Print training and validation loss at the end of each epoch\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "# After the training loop, you can plot the loss values, make predictions, and evaluate the model further."
      ],
      "metadata": {
        "id": "r79JFuu0z7sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_values, label='Training Loss')\n",
        "plt.plot(val_loss_values, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NDd_vKuAz_cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert your test data to PyTorch tensor if not done already\n",
        "X_test_tensor = torch.Tensor(X_test_scaled)\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test_tensor)\n",
        "\n",
        "binary_predictions = (predictions >= 0.5).int().squeeze().numpy()\n",
        "\n",
        "print(binary_predictions)\n",
        "print(y_test)\n",
        "report = classification_report(y_test, binary_predictions)\n",
        "\n",
        "\n",
        "print(\"AUC score is\", roc_auc_score(y_test, binary_predictions))\n",
        "print(\"accuracy is \",balanced_accuracy_score(y_test,binary_predictions))\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n"
      ],
      "metadata": {
        "id": "eMNVpLbK0BXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "     # Calculate ROC curve\n",
        "pr, tpr, thresholds = roc_curve(y_test, binary_predictions)\n",
        "\n",
        "    # Calculate AUC\n",
        "auc_value = roc_auc_score(y_test, binary_predictions)\n",
        "\n",
        "        # Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(pr, tpr, color='darkorange', lw=2, label=f'AUC = {auc_value:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vwd7W5Rw0F9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Save the trained model to a file\n",
        "model_filename = \"drive/MyDrive/logistic_regression_model.joblib\"\n",
        "joblib.dump(model, model_filename)# Load the saved model\n",
        "\n"
      ],
      "metadata": {
        "id": "jwmrjNII0KIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_filename = \"drive/MyDrive/logistic_regression_model.joblib\"\n",
        "\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv(\"drive/MyDrive/test.csv\")\n",
        "\n",
        "# Apply the same preprocessing steps\n",
        "test_df['text'] = test_df['text'].apply(remove_punctuation)\n",
        "test_df['text'].fillna(\"NA\", inplace=True)\n",
        "\n",
        "test_df[\"label_int\"]= np.where(test_df[\"label\"]=='\"AI\"', 1, 0)\n",
        "\n",
        "# Add any additional preprocessing steps if needed\n",
        "# Feature extraction (bag-of-words) on test data\n",
        "X_test_counts = vect.transform(test_df[\"text\"])\n",
        "\n",
        "# Feature extraction using TF-IDF on test data\n",
        "X_test_tfidf = tfidf_vect.transform(test_df[\"text\"])\n",
        "\n",
        "# POS tagging and vectorization on test data\n",
        "X_test_pos = [preprocess_text(text) for text in test_df[\"text\"]]\n",
        "X_test_pos_vec = pos_vectorizer.transform(X_test_pos)\n",
        "\n",
        "# Concatenate features for test data\n",
        "X_test_combined = np.hstack((X_test_counts.toarray(), X_test_tfidf.toarray(), X_test_pos_vec.toarray()))\n",
        "\n",
        "# Scale the features for test data\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "\n",
        "X_test_tensor = torch.Tensor(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    predictions = loaded_model(X_test_tensor)\n",
        "##\n",
        "binary_predictions = (predictions >= 0.5).int().squeeze().numpy()\n",
        "##\n",
        "print(binary_predictions)\n",
        "print(test_df[\"label_int\"])\n",
        "report = classification_report(test_df['label_int'], binary_predictions)\n",
        "##\n",
        "##\n",
        "print(\"AUC score is\", roc_auc_score(test_df['label_int'], binary_predictions))\n",
        "print(\"accuracy is \",balanced_accuracy_score(test_df['label_int'],binary_predictions))\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YmZjFSmO0KKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some text, true labels, and predictions\n",
        "for i in range(min(10, len(X_test_tensor))):  # Print the first 5 samples\n",
        "    print(f\"Text: {test_df['text'].iloc[i]}\")\n",
        "    print(f\"True Label: {test_df['label'].iloc[i]}, Predicted Label: {binary_predictions[i]}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "-0x_AMGs0F--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_filename = \"drive/MyDrive/logistic_regression_model.joblib\"\n",
        "\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv(\"drive/MyDrive/test_new.csv\")\n",
        "\n",
        "# Apply the same preprocessing steps\n",
        "test_df['text'] = test_df['text'].apply(remove_punctuation)\n",
        "test_df['text'].fillna(\"NA\", inplace=True)\n",
        "\n",
        "test_df[\"label_int\"]= np.where(test_df[\"label\"]=='\"AI\"', 1, 0)\n",
        "\n",
        "# Add any additional preprocessing steps if needed\n",
        "# Feature extraction (bag-of-words) on test data\n",
        "X_test_counts = vect.transform(test_df[\"text\"])\n",
        "\n",
        "# Feature extraction using TF-IDF on test data\n",
        "X_test_tfidf = tfidf_vect.transform(test_df[\"text\"])\n",
        "\n",
        "# POS tagging and vectorization on test data\n",
        "X_test_pos = [preprocess_text(text) for text in test_df[\"text\"]]\n",
        "X_test_pos_vec = pos_vectorizer.transform(X_test_pos)\n",
        "\n",
        "# Concatenate features for test data\n",
        "X_test_combined = np.hstack((X_test_counts.toarray(), X_test_tfidf.toarray(), X_test_pos_vec.toarray()))\n",
        "\n",
        "# Scale the features for test data\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "\n",
        "X_test_tensor = torch.Tensor(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    predictions = loaded_model(X_test_tensor)\n",
        "##\n",
        "binary_predictions = (predictions >= 0.5).int().squeeze().numpy()\n",
        "np.savetxt('/content/drive/MyDrive/pred_LR.csv', binary_predictions, delimiter=',',  fmt='%.2f')\n",
        "\n",
        "#binary_predictions.save('/content/drive/MyDrive/predictions_LR.csv')\n",
        "\n",
        "##\n",
        "print(binary_predictions)\n",
        "print(test_df[\"label_int\"])\n",
        "report = classification_report(test_df['label_int'], binary_predictions)\n",
        "##\n",
        "##\n",
        "print(\"AUC score is\", roc_auc_score(test_df['label_int'], binary_predictions))\n",
        "print(\"accuracy is \",balanced_accuracy_score(test_df['label_int'],binary_predictions))\n",
        "precision_ai = precision_score(test_df['label_int'], binary_predictions, pos_label=1)  # Assuming \"AI\" is the positive class\n",
        "precision_human = precision_score(test_df['label_int'], binary_predictions, pos_label=0)  # Assuming \"AI\" is the positive class\n",
        "\n",
        "recall_ai = recall_score(test_df['label_int'], binary_predictions, pos_label=1)  # Assuming \"AI\" is the positive class\n",
        "recall_human = recall_score(test_df['label_int'], binary_predictions, pos_label=0)  # Assuming \"AI\" is the positive class\n",
        "\n",
        "f1_ai = f1_score(test_df['label_int'], binary_predictions, pos_label=1)  # Assuming \"AI\" is the positive class\n",
        "f1_human = f1_score(test_df['label_int'], binary_predictions, pos_label=0)  # Assuming \"AI\" is the positive class\n",
        "conf_matrix = confusion_matrix(test_df['label_int'], binary_predictions)\n",
        "\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "46hP58zfHyVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your actual values)\n",
        "ai_values = [precision_ai,recall_ai,f1_ai]\n",
        "human_values = [precision_human,recall_human,f1_human]\n",
        "\n",
        "# Metrics names\n",
        "metrics = [ 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "# Number of metrics\n",
        "num_metrics = len(metrics)\n",
        "\n",
        "# Create an array of indices for each metric\n",
        "indices = np.arange(num_metrics)\n",
        "\n",
        "# Bar width\n",
        "bar_width = 0.1\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "# Plot AI values\n",
        "ax.bar(indices, ai_values, bar_width, label='AI', color='blue',alpha=0.5 )\n",
        "\n",
        "# Plot Human values\n",
        "ax.bar(indices + bar_width, human_values, bar_width, label='Human', color='orange')\n",
        "\n",
        "# Customize the plot\n",
        "# Make the y-axis tick labels darker and bigger\n",
        "ax.tick_params(axis='both', labelsize=15, labelcolor='black')\n",
        "\n",
        "# Make the axis labels (x and y) bold\n",
        "ax.xaxis.label.set_fontweight('bold')\n",
        "ax.yaxis.label.set_fontweight('bold')\n",
        "ax.set_xticks(indices + bar_width / 2)\n",
        "\n",
        "\n",
        "# Make the ticks on both axes thicker and darker\n",
        "#ax.tick_params(axis='both', which='both', width=4, colors='black')\n",
        "\n",
        "\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend(fontsize=6,loc=\"best\")\n",
        "ax.set_ylabel('Metric Values')\n",
        "#ax.set_title('Comparison of AI and Human Performance Metrics')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6YEtK58hH7q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract values from confusion matrix\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Plotting\n",
        "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "values = [tn, fp, fn, tp]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.bar(labels, values, color=['pink', 'yellow', 'orange', 'blue'],alpha=0.8,width=0.3)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Add value annotations on top of the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
        "ax.tick_params(axis='y', labelsize=20, labelcolor='black')\n",
        "ax.set_xticklabels(labels, fontweight='bold')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvXkQvk2Uz2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}