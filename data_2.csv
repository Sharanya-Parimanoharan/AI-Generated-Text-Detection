label,text
"""Human""","The focus of this paper is on a new concept framework and an architecture of an intelligent decision support 
syetem generator (DSSG). The framework results from a synthesis of two existing frameworks: 
Spragae and Bonczek, while the architecture is a rooted partial order network. From our experience which comes 
out of the project of DSSG, we consider that they are keys of further research and development of DSS"
"""AI""","This paper centers on introducing a fresh conceptual framework and unveiling the architecture behind the 
Intelligent Decision Support System Generator (DSSG). This framework emerges through amalgamating two 
established models—Spragae and Bonczek—exploiting their collective strengths, while the architecture itself 
takes form as a rooted partial order network. Rooted in our practical insights from the DSSG project, we firmly 
ascertain these elements as pivotal for advancing both research and development in Decision Support Systems (DSS). 
The symbiosis of the synthesized framework and the intricate architecture lays the groundwork for groundbreaking 
progress in refining decision-making processes across diverse domains. Envisioning the potential of these innovations, 
we anticipate transformative effects and a profusion of novel pathways within the DSS realm."
"""Human""","Heuristic optimization methods provide a robust and efficient approach to solving complex optimization problems
. This paper presents a hybrid optimization technique combining two heuristic optimization methods, artificial
 immune system (AIS) and particle swarm optimization (PSO), together in searching for the global optima of 
nonlinear functions. The proposed algorithm, namely hybrid anti-prematuration optimization method, contains 
four significant operators, i.e. swarm operator, cloning operator, suppression operator, and receptor editing 
operator. The swarm operator is inspired by the particle swarm intelligence, and the clone operator, suppression 
operator, and receptor editing operator are gleaned by the artificial immune system. The simulation results of 
three representative nonlinear test functions demonstrate the superiority of the hybrid optimization algorithm 
over the conventional methods with regard to both the solution quality and convergence rate. It is also employed 
to cope with a real-world optimization problem."
"""AI""","This paper introduces a robust and efficient approach to tackle intricate optimization challenges through heuristic 
optimization methods. The study unveils a hybrid optimization technique that merges two heuristic optimization 
methods, artificial immune system (AIS), and particle swarm optimization (PSO), synergizing their strengths to 
explore global optima for nonlinear functions. Referred to as the hybrid anti-prematuration optimization method,
 the proposed algorithm integrates four pivotal operators: the swarm operator inspired by particle swarm intelligence, 
the clone operator, suppression operator, and receptor editing operator drawn from the artificial immune system
 paradigm. By conducting simulations on three representative nonlinear test functions, the hybrid optimization 
algorithm demonstrates its clear edge over conventional methods in terms of solution quality and convergence rate. 
Furthermore, the algorithm proves its efficacy in addressing real-world optimization challenges."
"""Human""","In this paper the nature of predatory pricing is analyzed with genetic algorithms. It is found that, even under the
 same payoff structure, the results of the co-evolution of weak monopolists and entrants are sensitive to the 
representation of the decision-making process. Two representations are studied in this paper. One is the 
action-based representation and the other the strategy-based representation. The former is to represent a
 naive mind and the latter is to capture a sophisticated mind. For the action-based representation, the 
convergence results are easily obtained and predatory pricing is only temporary in all simulations. However, 
for the strategy-based representation, predatory pricing is not a rare phenomenon and its appearance is 
cyclical but not regular. Therefore, the snowball effect of a little craziness observed in the experimental 
game theory wins its support from this representation. Furthermore, the nature of predatory pricing has 
something to do with the evolution of the sophisticated rather than the naive minds."
"""AI""","This paper employs genetic algorithms to scrutinize the essence of predatory pricing, revealing its intricacies. 
The study underscores that the co-evolution of weak monopolists and entrants hinges greatly on how their 
decision-making processes are represented. Two distinct representations, the action-based and strategy-based, 
are explored. The former, reflecting a simpler mindset, demonstrates that predatory pricing is merely a 
transient tactic. Conversely, the strategy-based representation uncovers a cyclical, albeit irregular, pattern 
for predatory pricing. This substantiates the ""snowball effect of a little craziness"" from experimental game 
theory. Crucially, the research highlights that predatory pricing's nature is entwined with the evolution of 
sophisticated cognitive processes rather than naive approaches, offering a multifaceted perspective on 
competitive strategy dynamics."
"""Human""","This paper describes a channel estimation and equalization algorithm using three-layer artificial neural
 networks (ANNs) with feedback for multiple input multiple output wireless communication systems. 
An ANN structure with feedback was designed to use different learning algorithms in the different ANN
 layers. This actually forms a Turbo iteration process between the different algorithms which effectively 
improves the estimation performance of the channel equalizer. Simulation results show that this channel 
equalization algorithm has better computational efficiency and faster convergence than higher order statistics 
based algorithms"
"""AI""","This paper presents an innovative channel estimation and equalization algorithm for multiple input multiple output 
(MIMO) wireless communication systems. The approach employs three-layer artificial neural networks (ANNs) 
with feedback. The unique design incorporates distinct learning algorithms in the various ANN layers, thereby
 creating a Turbo iteration process. This intricate interplay between algorithms notably enhances the channel 
equalizer's estimation performance. Notably, simulation outcomes validate the algorithm's superior computational 
efficiency and quicker convergence compared to algorithms grounded in higher order statistics"
"""Human""","User acceptance is one of the key fundamentals for development and success of mobile commerce (m-commerce).
 Based on the revised unified theory of acceptance and use of technology (UTAUT) model, we described a theoretical
 framework that incorporates the unique characteristics of m-commerce to enhance our understanding of 
m-commerce acceptance and usage in China. The revised UT AUT includes the characteristics of m-commerce, 
inclusion and consideration of information system user satisfaction theory, and Chinese culture and context"
"""AI""","Centered on the development and triumph of mobile commerce (m-commerce), user acceptance stands as a pivotal cornerstone. Drawing from the revised Unified Theory of Acceptance and Use of Technology (UTAUT) model, this study formulates a theoretical framework that seamlessly integrates the distinctive attributes of m-commerce. This endeavor augments our comprehension of m-commerce acceptance and utilization in the context of China. The revamped UTAUT framework intricately incorporates the idiosyncrasies of m-commerce, embraces the principles of information system user satisfaction theory, and pays heed to the nuances of Chinese culture and context."
"""Human""","Personalized service systems are an effective way to help users obtain recommendations for unseen items, within
 the enormous volume of information available based on their preferences. The most commonly used personalized
 service system methods are collaborative filtering, content-based filtering, and hybrid filtering. Unfortunately, 
each method has its drawbacks. This paper proposes a new method which unified partition-based collaborative
 filtering and meta-information filtering. In partition-based collaborative filtering the user-item rating matrix can 
be partitioned into low-dimensional dense matrices using a matrix clustering algorithm. Recommendations are 
generated based on these low-dimensional matrices. Additionally, the very low ratings problem can be solved
 using meta-information filtering. The unified method is applied to a digital resource management system. 
The experimental results show the high efficiency and good performance of the new approach"
"""AI""","Personalized service systems have emerged as a potent means to aid users in discovering recommendations from
 the overwhelming sea of available information, tailored to their preferences. While collaborative filtering,
 content-based filtering, and hybrid filtering are widely used, each bears its own limitations. This paper
 introduces a novel approach that combines partition-based collaborative filtering and meta-information 
filtering to address these shortcomings. The methodology entails partitioning the user-item rating matrix 
into compact, low-dimensional matrices using a matrix clustering algorithm, forming the foundation for
 generating recommendations. Moreover, the challenge of very low ratings is tackled through meta-information
 filtering. The proposed unified technique finds application in a digital resource management system, yielding 
experimental results that underscore its efficiency and commendable performance"
"""Human""","The features of electromyographic (EMG) signals were investigated while people walking on different terrains, including up and down slopes, up and down stairs, and during level walking at different speeds. The features were used to develop a terrain identification method. The technology can be used to develop an intelligent transfemoral prosthetic limb with terrain identification capability. The EMG signals from 8 hip muscles of 13 healthy persons were recorded as they walked on the different terrains. The signals from the sound side of a transfemoral amputee were also recorded. The features of these signals were obtained using data processing techniques with an identification process developed for the identification of the terrain type. The procedure was simplified by using only the signals from three muscles. The identification process worked well in an intelligent prosthetic knee in a laboratory setting"
"""AI""","This study delved into the characteristics of electromyographic (EMG) signals during various walking scenarios on diverse terrains, encompassing slopes, stairs, and level walking at different speeds. These features were harnessed to devise a method for identifying terrains. The potential application of this technology lies in the creation of an intelligent transfemoral prosthetic limb endowed with terrain recognition capabilities. EMG signals from 8 hip muscles of 13 healthy individuals and a transfemoral amputee's unaffected side were recorded during walking on distinct terrains. Employing data processing techniques, features were extracted from these signals, and an identification process was developed for terrain classification. A simplified approach utilizing signals from three muscles streamlined the procedure. In a laboratory context, the identification process showcased promising performance in an intelligent prosthetic knee."
"""Human""","Fault diagnosis is very important for development and maintenance of safe and reliable electronic circuits and systems. This paper describes an approach of soft fault diagnosis for analog circuits based on slope fault feature and back propagation neural networks (BPNN). The reported approach uses the voltage relation function between two nodes as fault features; and for linear analog circuits, the voltage relation function is a linear function, thus the slope is invariant as fault feature. Therefore, a unified fault feature for both hard fault (open or short fault) and soft fault (parametric fault) is extracted. Unlike other NN-based diagnosis methods which utilize node voltages or frequency response as fault features, the reported BPNN is trained by the extracted feature vectors, the slope features are calculated by just simulating once for each component, and the trained BPNN can achieve all the soft faults diagnosis of the component. Experiments show that our approach is promising"
"""AI""","In the realm of ensuring the safety and reliability of electronic circuits and systems, fault diagnosis stands as a crucial pursuit. This paper introduces a novel approach to soft fault diagnosis in analog circuits, leveraging slope fault features and the power of Back Propagation Neural Networks (BPNN). The proposed method capitalizes on the voltage relation function between two nodes to serve as fault features. Notably, for linear analog circuits, the voltage relation function manifests as a linear function, ensuring that the slope remains invariant and forms a consistent fault feature. This innovation provides a unified fault feature encompassing both hard faults (open or short circuits) and soft faults (parametric faults). Unlike alternative neural network-based diagnostic methods that rely on node voltages or frequency response as fault features, the described BPNN is trained using the derived feature vectors. The beauty of this approach lies in its efficiency: slope features are computed through a single simulation per component, and the trained BPNN encompasses the diagnosis of all soft faults associated with the component. Empirical trials validate the promise of this approach, underscoring its potential in the realm of fault diagnosis."
"""Human""","The growing interest in energy-efficient buildings is driving changes in investment, design, and occupant behavior. To better focus cost and resource conservation efforts, electricity consumption feedback can be used to provide motivation, guidance, and verification. Disaggregating by end-use helps both consumers and producers to identify targets for conservation. While hardware-based sub-metering is costly and labor-intensive, non-intrusive load monitoring (N I LM) is capable of gathering detailed energy-use data with minimal equipment cost and installation time. However, variations in measurements between metering devices complicate the process of compiling the necessary appliance profiles. Future work involves the development of NILM algorithms using sensor fusion and detailed appliance-level data gathered from a highly-sensed house currently being constructed near Pittsburgh, Pennsylvania."
"""AI""","The increasing emphasis on energy-efficient buildings has sparked transformations in investment, design, and occupant behavior. To streamline efforts for cost-effectiveness and resource preservation, leveraging electricity consumption feedback has emerged as a valuable strategy for motivation, guidance, and validation. The process of disaggregating energy consumption by end-use enables both consumers and providers to pinpoint areas for conservation. While hardware-intensive sub-metering approaches incur high costs and demand labor, non-intrusive load monitoring (NILM) stands out by offering detailed energy usage data with minimal equipment expenses and installation time. Nonetheless, the challenge arises from inconsistencies in measurements across metering devices, complicating the task of compiling essential appliance profiles. The upcoming agenda centers on developing NILM algorithms by amalgamating sensor fusion and intricate appliance-level data obtained from an extensively-sensed house currently under construction in the vicinity of Pittsburgh, Pennsylvania. This endeavor holds the promise of refining energy monitoring and optimization strategies."
"""Human""","The imminence of terrorist activities and the necessity of the maximum possible disaster preparedness in the sense of indoor-navigation support have been brought to evidence by several catastrophes, e.g., the fire at Istanbul Airport in May 2006 or the terror attacks on the London Underground on July 7, 2005. Since 2001 ten terror attacks have been thwarted only in Great Britain. For that reason the aim of the presented research project is to develop a solution for response and recovery to support rescuers in finding the shortest way within a public building and provide them with important information in their particular spatial context. Existing building information models (BIM) are used for displaying plans on mobile devices and for routing purposes. The indoor navigation system is based on wireless LAN (WLAN), ultra-wide-band (UWB), and radio frequency identification (RFID). These technologies are described in detail and an overview on data formats which are used to retrieve building data out of the BIM for generating routing networks is given"
"""AI""","The urgency of countering terrorist activities and the imperative for comprehensive disaster preparedness, particularly in terms of indoor navigation assistance, have been underscored by numerous catastrophic incidents. Notable examples include the 2006 fire at Istanbul Airport and the terror attacks on the London Underground in July 2005. This is further emphasized by the thwarting of ten terror attacks in Great Britain alone since 2001. Against this backdrop, the presented research project aims to develop a robust solution for effective response and recovery, aiding rescuers in swiftly navigating within public buildings and furnishing them with critical spatial context information. The endeavor capitalizes on existing Building Information Models (BIM) to showcase plans on mobile devices and facilitate routing. The indoor navigation system is underpinned by wireless LAN (WLAN), ultra-wide-band (UWB), and radio frequency identification (RFID) technologies. The intricacies of these technologies are thoroughly expounded upon, and a comprehensive overview of data formats employed to extract building data from the BIM, thereby generating routing networks, is provided. This project harbors the potential to significantly enhance the capabilities of rescuers in managing crises and optimizing response efforts."
"""Human""","Context-awareness enhances human-centric, intelligent behavior in a smart environment; however, context-awareness is not widely used due to the lack of effective infrastructure to support context-aware applications. This paper presents an agent-based middleware for providing context-aware services for smart spaces to afford effective support for context acquisition, representation, interpretation, and utilization to applications. The middleware uses a formal context model, which combines first order probabilistic logic (FOPL) and web ontology language (OWL) ontologies, to provide a common understanding of contextual information to facilitate context modeling and reasoning about imperfect and ambiguous contextual information and to enable context knowledge sharing and reuse. A context inference mechanism based on an extended Bayesian network approach is used to enable automated reactive and deductive reasoning. The middleware is used in a case study in a smart classroom, and performance evaluation result shows that the context reasoning algorithm is good for non-time-critical applications and that the complexity is highly sensitive to the size of the context dataset"
"""AI""","Context-awareness plays a pivotal role in enhancing intelligent, human-centric behavior within smart environments. Nonetheless, the limited adoption of context-awareness stems from the lack of an effective infrastructure to support context-aware applications. This paper introduces an agent-based middleware designed to provide context-aware services for smart spaces, thereby furnishing robust support for various facets of context management, including acquisition, representation, interpretation, and utilization for applications. The middleware hinges on a formal context model, which harmoniously amalgamates first-order probabilistic logic (FOPL) and the web ontology language (OWL) ontologies, thus creating a shared foundation for comprehending contextual information. This paves the way for efficient context modeling, handling imperfect and ambiguous contextual data, and facilitating context knowledge sharing and reusability. A context inference mechanism, built on an extended Bayesian network approach, powers automated reactive and deductive reasoning. A case study set within a smart classroom showcases the middleware's practical application, and performance evaluations affirm the effectiveness of the context reasoning algorithm, particularly for non-time-critical applications. Furthermore, the evaluations underscore the sensitivity of algorithmic complexity to the dimensions of the context dataset. This middleware holds the potential to catalyze advancements in context-aware technology within smart environments."
"""Human""","Let G = (V, E) be a complete undirected graph with vertex set V, edge set E, and edge weights /(e) satisfying the triangle inequality. The vertex set V is partitioned into clusters V 1 , V 2 , …, V k . The clustered traveling salesman problem (CTSP) seeks to compute the shortest Hamiltonian tour that visits all the vertices, in which the vertices of each cluster are visited consecutively. A two-level genetic algorithm (TLGA) was developed for the problem, which favors neither intra-cluster paths nor inter-cluster paths, thus realized integrated evolutionary optimization for both levels of the CTSP. Results show that the algorithm is more effective than known algorithms. A large-scale traveling salesman problem (TSP) can be converted into a CTSP by clustering so that it can then be solved by the algorithm. Test results demonstrate that the clustering TLGA for large TSPs is more effective and efficient than the classical genetic algorithm."
"""AI""","Consider a complete undirected graph G = (V, E) featuring a vertex set V, an edge set E, and edge weights satisfying the triangle inequality. The vertex set V is segregated into clusters V1, V2, ..., Vk. In the context of the Clustered Traveling Salesman Problem (CTSP), the primary objective revolves around determining the shortest Hamiltonian tour that encompasses all vertices, ensuring that vertices within each cluster are traversed consecutively. To address this problem, a Two-Level Genetic Algorithm (TLGA) was conceived. Unlike favoring solely intra-cluster or inter-cluster paths, this algorithm integrates evolutionary optimization for both CTSP levels, achieving a harmonized approach. Empirical outcomes affirm the algorithm's effectiveness compared to existing algorithms. Importantly, by applying clustering, a substantial Traveling Salesman Problem (TSP) can be transformed into a CTSP, thereby allowing the algorithm to solve it. Extensive tests reveal that the Clustered TLGA yields enhanced efficiency and effectiveness when solving large TSPs, outperforming the classical genetic algorithm."
"""Human""","In this paper, symbol error probability (SEP) for generalized selection combining (GSC) reception of M-ary quadrature amplitude modulation (M-QAM) signals in a block frequency-flat Rayleigh fading channel is considered. The paper presents a closed-form approximate SEP expression for GSC reception of M-QAM signals in the fading channel. The SEP expression is explicit and simulation results validate that it is very accurate"
"""AI""","This paper delves into the Symbol Error Probability (SEP) aspect of Generalized Selection Combining (GSC) reception for M-ary Quadrature Amplitude Modulation (M-QAM) signals within a block frequency-flat Rayleigh fading channel. The primary focus lies in presenting a closed-form approximate expression for SEP in the context of GSC reception of M-QAM signals amidst fading channels. This SEP expression is notable for its explicit nature, and subsequent simulation results corroborate its impressive accuracy."
"""Human""","The paper analyzes the problem of blind source separation (BSS) based on the nonlinear principal component analysis (NPCA) criterion. An adaptive strong tracking filter (STF) based algorithm was developed, which is immune to system model mismatches. Simulations demonstrate that the algorithm converges quickly and has satisfactory steady-state accuracy. The Kalman filtering algorithm and the recursive leastsquares type algorithm are shown to be special cases of the STF algorithm. Since the forgetting factor is adaptively updated by adjustment of the Kalman gain, the STF scheme provides more powerful tracking capability than the Kalman filtering algorithm and recursive least-squares algorithm"
"""AI""","This paper addresses the blind source separation (BSS) problem through an analysis of the Nonlinear Principal Component Analysis (NPCA) criterion. The research introduces an adaptive Strong Tracking Filter (STF) based algorithm designed to handle system model mismatches effectively. Notably, this algorithm showcases resilience against such mismatches. Through simulations, the algorithm's rapid convergence and commendable steady-state accuracy are demonstrated. The paper reveals that the Kalman filtering algorithm and the recursive least-squares type algorithm are special instances of the STF algorithm. An intriguing aspect of the STF approach lies in its adaptive update of the forgetting factor via Kalman gain adjustment, endowing it with a robust tracking capability that surpasses that of the Kalman filtering and recursive least-squares algorithms."
"""Human""","To get a better understanding of user behavior towards online learning systems, the technology acceptance model (TAM) was extended to include an intrinsic motivational factor. An online survey posted on a campus BBS was conducted to collect research data with a total of 121 usable responses. The results support the motivational model and show that the explained variance of online learning system use behavior is 71.30/0 higher than that of the original TAM explanation."
"""AI""","In order to gain deeper insights into user behavior with respect to online learning systems, an extension of the Technology Acceptance Model (TAM) was undertaken, incorporating an intrinsic motivational factor. This enriched model sought to encompass not just the technological aspects but also the intrinsic motivation driving users. A comprehensive online survey was conducted, hosted on a campus Bulletin Board System (BBS), yielding 121 usable responses for research data. The findings affirm the validity of the motivational model and present compelling evidence: the explained variance in online learning system usage behavior is significantly higher at 71.30% compared to the explanation provided by the original TAM framework. This underscores the importance of incorporating intrinsic motivation considerations for a more comprehensive understanding of user behavior within online learning systems."
"""Human""","This paper presents a simulation system for the disaster evacuation based on multi-agent model considering geographical information. This system consists of three parts, the modeling for the land and buildings using GIS data, the analysis of disaster evacuation using multi-agent model, and the visualization for the numerical results using the virtual reality technique. By introducing the numerical solver of the natural disaster to the present system, it is possible to evaluate not only the damage of structure but also the damage of human being. Furthermore, it is possible to investigate the appropriate evacuation route by the simulation. The Dijkstra algorithm is used to obtain shortest route to the refuge. In addition, the visualization using virtual reality technique is curried out to understand the feeling of refugee. The present system is applied to the evacuation analysis by the flood flow in urban area and is shown to be a useful tool to investigate the damage by natural disasters"
"""AI""","This paper introduces a comprehensive simulation system designed for disaster evacuation scenarios, rooted in a multi-agent model that takes into account geographical information. Comprising three key components, the system begins with GIS data-driven modeling of land and structures, progresses to the analysis of disaster evacuation via a multi-agent model, and culminates in the visualization of numerical outcomes using virtual reality techniques. A significant enhancement emerges through the integration of a numerical solver for natural disasters, enabling not only the assessment of structural damage but also the evaluation of human impact. Furthermore, the system facilitates the exploration of optimal evacuation routes through simulation. Employing the Dijkstra algorithm, the system computes the shortest path to safety. The integration of virtual reality visualization provides a means to empathetically understand the experiences of refugees. The system's applicability is demonstrated through its application to flood-induced evacuation analysis within an urban environment, showcasing its efficacy as a versatile tool for gauging the repercussions of natural disasters."
"""Human""","Quay crane and yard truck scheduling are two important subproblems in container terminal operations which have been studied separately in previous research. This paper proposes a new problem for the integrated quay crane and yard truck scheduling for inbound containers. The problem is formulated as a mixed integer programming (MIP) model. Due to the intractability, a genetic algorithm (GA) and a modified Johnson's Rule-based heuristic algorithm (MJRHA) are used for the problem solution. In addition, two closed form lower bounds are given to evaluate the solution accuracy. Computational experiments show that the solution algorithm can efficiently handle the scheduling problem and that the integrated methods are very useful"
"""AI""","Container terminal operations involve critical tasks such as quay crane and yard truck scheduling, which have traditionally been explored in isolation in prior research. This paper introduces a novel problem by integrating quay crane and yard truck scheduling for inbound containers. The problem is structured as a Mixed Integer Programming (MIP) model. Given its complexity, the paper employs a Genetic Algorithm (GA) and a modified version of Johnson's Rule-based heuristic algorithm (MJRHA) to tackle the problem. Furthermore, to assess solution accuracy, the paper presents two closed-form lower bounds. Computational experiments affirm the efficiency of the solution algorithm in handling the scheduling challenge. Moreover, the integrated methodologies demonstrate their efficacy in addressing the problem at hand, shedding light on their utility in container terminal operations."
"""Human""","Fuzzy clustering has been used widely in pattern recognition, image processing, and data analysis. An improved fuzzy clustering algorithm was developed based on the conventional fuzzy c-means (FCM) to obtain better quality clustering results. The update equations for the membership and the cluster center are derived from the alternating optimization algorithm. Two fuzzy scattering matrices in the objective function assure the compactness between data points and cluster centers, and also strengthen the separation between cluster centers in terms of a novel separable criterion. The clustering algorithm properties are shown to be an improvement over the FCM method's properties. Numerical simulations show that the clustering algorithm gives more accurate clustering results than the FCM method."
"""AI""","Fuzzy clustering has found wide application in fields such as pattern recognition, image processing, and data analysis. In this context, an enhanced fuzzy clustering algorithm has been developed as an advancement upon the conventional Fuzzy C-Means (FCM) approach, with the goal of achieving higher-quality clustering outcomes. The update equations governing membership and cluster center adjustments are deduced via an alternating optimization algorithm. The algorithm incorporates two fuzzy scattering matrices within the objective function to enhance both the cohesion between data points and cluster centers, as well as the distinctiveness among cluster centers, as indicated by a novel separable criterion. The algorithm's characteristics exhibit notable improvements over those of the FCM method. Empirical assessments through numerical simulations demonstrate that the proposed clustering algorithm yields more precise clustering results compared to the conventional FCM technique."
"""Human""","The two important features of self-organizing maps (SOM), topological preservation and easy visualization, give it great potential for analyzing multi-dimensional time series, specifically traffic flow time series in an urban traffic network. This paper investigates the application of SOM in the representation and prediction of multi-dimensional traffic time series. First, SOMs are applied to cluster the time series and to project each multi-dimensional vector onto a two-dimensional SOM plane while preserving the topological relationships of the original data. Then, the easy visualization of the SOMs is utilized and several exploratory methods are used to investigate the physical meaning of the clusters as well as how the traffic flow vectors evolve with time. Finally, the k-nearest neighbor (kNN) algorithm is applied to the clustering result to perform short-term predictions of the traffic flow vectors. Analysis of real world traffic data shows the effectiveness of these methods for traffic flow predictions, for they can capture the nonlinear information of traffic flows data and predict traffic flows on multiple links simultaneously."
"""AI""","Self-Organizing Maps (SOM) possess two key attributes - topological preservation and simple visualization - which render them highly promising for the analysis of multi-dimensional time series, particularly traffic flow time series within an urban traffic network. This paper delves into the application of SOM for the representation and prediction of multi-dimensional traffic time series. The process begins by utilizing SOMs to cluster time series data and project each multi-dimensional vector onto a two-dimensional SOM plane, preserving the original data's topological relationships. The user-friendly visualization capabilities of SOMs are then leveraged, coupled with various exploratory methods, to uncover the clusters' physical significance and the temporal evolution of traffic flow vectors. Subsequently, the k-nearest neighbor (kNN) algorithm is employed on the clustering outcome to facilitate short-term predictions of traffic flow vectors. Empirical examination of real-world traffic data demonstrates the effectiveness of these methodologies in traffic flow predictions. They adeptly capture nonlinear information within traffic flow data and can predict traffic flows across multiple links concurrently."
"""Human""","The busbars in high power neutral point clamped three-level inverters are modeled using the Maxwell Q3D Extractor software, which is based on the partial element equivalent circuits method. The equivalent circuits of the busbars and devices model are simulated in the electric simulator PSIM to analyze the effects of the parasitic inductance on the switching characteristics of the integrated gate commutated thyristor (IGCT) in different topology positions. The simulation results agree well with the measured impedance analyzer results and the IGCT test results, which proves the effectiveness of the modeling method for the large, complex busbars."
"""AI""","The modeling of busbars within high-power neutral point clamped three-level inverters is undertaken using the Maxwell Q3D Extractor software, which employs the partial element equivalent circuits method. This approach enables the creation of equivalent circuits representing both the busbars and the associated devices. The next step involves simulating these equivalent circuits within the electric simulator PSIM. The objective is to scrutinize the impact of parasitic inductance on the switching characteristics of the Integrated Gate Commutated Thyristor (IGCT) in various positions within different topology setups. Through these simulations, the outcomes are compared against measurements from impedance analyzer tests and IGCT performance assessments. The findings reveal a high level of agreement between simulation results and experimental data, thus validating the efficacy of this modeling technique for handling intricate and expansive busbar systems."
"""Human""","This paper presents an optimized topology for urban traffic sensor networks. Small world theory is used to improve the performance of the wireless communication system with a heterogeneous transmission model and an optimal transmission radius. Furthermore, a series of simulations based on the actual road network around the 2 nd Ring Road in Beijing demonstrate the practicability of constructing artificial “small worlds”. Moreover, the particle swarm optimization method is used to calculate the globally best distribution of the nodes with the large radius. The methods proposed in this paper will be helpful to the sensor nodes deployment of the new urban traffic sensor networks"
"""AI""","This paper introduces an optimized topology design for urban traffic sensor networks, aiming to enhance wireless communication system performance. Leveraging the principles of small-world theory, improvements are achieved through a heterogeneous transmission model and an optimal transmission radius. Through simulations conducted using the road network surrounding Beijing's 2nd Ring Road, the feasibility of creating artificial ""small worlds"" is demonstrated. Additionally, the particle swarm optimization method is employed to determine the globally optimal distribution of nodes with larger radii. The methodologies presented herein hold potential for guiding the deployment of sensor nodes within new urban traffic sensor networks, contributing to their effectiveness and efficiency."
"""Human""","This paper presents a fault-tolerant computer system. It is designed as a double 2-out-of-2 architecture based on component redundant technique. Also, a quantitative probabilistic model is presented for evaluating the reliability, availability, maintainability and safety (RAMS) of this architecture. Hierarchical modeling method and Markov modeling method are used in RAMS analysis to evaluate the system characteristics. The double 2-out-of-2 system is compared with the other two systems, all voting triple modular redundancy (AVTMR) system and dual-duplex system. According to the result, the double 2-out-of-2 system has the highest dependability. Especially, the system can satisfy the safety integrity level (SIL) 4, which means the system's probability of catastrophic failure less than or equal to 10–8 per hour, therefore, it can be applied to life critical systems such as high-speed railway systems."
"""AI""","This paper introduces a fault-tolerant computer system design utilizing a double 2-out-of-2 architecture based on component redundancy techniques. Additionally, the paper presents a quantitative probabilistic model for assessing the system's reliability, availability, maintainability, and safety (RAMS). Both hierarchical modeling and Markov modeling methods are applied to evaluate various system characteristics within the RAMS framework. The performance of the double 2-out-of-2 system is compared with that of two other systems: the All Voting Triple Modular Redundancy (AVTMR) system and the dual-duplex system. The findings indicate that the double 2-out-of-2 system exhibits the highest dependability. Notably, this system meets the stringent safety integrity level (SIL) 4 requirement, ensuring the probability of catastrophic failure is less than or equal to 10^-8 per hour. As such, it becomes a viable solution for life-critical systems, including high-speed railway systems."
"""Human""","A general review of the socio-economic impact of the intelligent transport system (ITS) is presented with a case study to demonstrate the data envelopment analysis method. Cost-benefit analyses are still the dominant method for evaluating ITS and other transport engineering projects, while cost effective analyses and multi-criteria appraisals are widely used to define and prioritize objectives by providing useful information for the most promising policy decisions. Both cost-benefit analyses and a data envelopment analysis method are applied to analyze the socio-economic impact of convoy driving systems. The main findings are that a convoy provides a worthwhile benefit-cost ratio when more than 30% of the traffics in the convoys and the traffic load exceeds 5500 vehicles/h for a three-lane motorway. The results also show that for a fixed percentage of convoys, increased demand will increase the data envelopment analysis method relative efficiency and that the neglect of certain output indicators of an ITS may result in underestimation of the system effects."
"""AI""","This study offers a comprehensive overview of the socio-economic influence of Intelligent Transport Systems (ITS) while employing a case study to illustrate the Data Envelopment Analysis (DEA) method. Within the realm of evaluating ITS and similar transport engineering projects, cost-benefit analyses remain the predominant approach. Concurrently, cost-effective analyses and multi-criteria appraisals are widely employed for goal definition and prioritization, supplying valuable insights for informed policy decisions. In this context, both cost-benefit analyses and the DEA method are employed to examine the socio-economic implications of convoy driving systems. The primary findings reveal that convoy systems yield a beneficial benefit-cost ratio, specifically when over 30% of traffic forms convoys, and traffic load exceeds 5500 vehicles/hour for a three-lane motorway. Furthermore, the results indicate that augmenting demand leads to an increase in DEA method relative efficiency, for a fixed percentage of convoys. Importantly, disregarding certain output indicators of an ITS may lead to an underestimation of system effects. This research highlights the multifaceted evaluation approaches necessary to grasp the holistic impact of intelligent transport systems."
"""Human""","This paper presents a Kalman filter to effectively and economically determine the Euler angles for micro aerial vehicles (MAVs), whose size and payload are severely limited. The filter uses data from a series of micro-electro mechanical system sensors to determine the selected 3 variables of the direction cosine matrix and the bias of the rate gyro sensors as state elements in a dynamic model, with the gravitational acceleration to build a measurement model. For high speed maneuvers, rigid motion equations are used to correct the measurements of the gravitational acceleration. The filter is designed to automatically tune its gain based on the dynamic system state. Simulations indicate that the Euler angles can be determined with standard deviations less than 3°. The algorithm was successfully implemented in a miniature attitude measurement system suitable for MAVs. Aerobatic flights show that the attitude determination algorithm works effectively. The attitude determination algorithm is effective and economical, and can also be applied to bionic robofishs and land vehicles, whose size and payload are also greatly limited"
"""AI""","This paper introduces a Kalman filter as a means to accurately and cost-efficiently ascertain Euler angles for micro aerial vehicles (MAVs), which are subject to severe size and payload limitations. The filter leverages data from a sequence of micro-electro mechanical system (MEMS) sensors to establish the selected three variables of the direction cosine matrix, along with the bias of rate gyro sensors, as state elements within a dynamic model. In conjunction with the gravitational acceleration, this information forms the basis of a measurement model. To address high-speed maneuvers, rigid motion equations are employed to refine measurements of gravitational acceleration. The filter is adept at automatically adjusting its gain in response to the dynamic system's state. Through simulations, Euler angles can be determined with standard deviations under 3°. The algorithm's practicality is validated through its successful integration into a miniature attitude measurement system suited for MAVs. Aerobatic flights provide empirical evidence of the algorithm's effective operation. The applicability of this attitude determination algorithm extends beyond MAVs to bionic robofish and land vehicles, both of which grapple with size and payload limitations."
"""Human""","A comprehensive description of the key factors affecting evacuations at fire scenes is necessary for accurate simulations. An agent-based simulation model which incorporates the fire scene and the building geometry is developed using a fire dynamics simulator (FDS) based on the computational fluid dynamics and geographic information system (GIS) data to model the occupant response. The building entities are generated for FDS simulation while the spatial analysis on GIS data represents the occupant's knowledge of the building. The influence of the fire is based on a hazard assessment of the combustion products. The agent behavior and decisions are affected by environmental features and the fire field. A case study demonstrates that the evacuation model effectively simulates the coexistence and interactions of the major factors including occupants, building geometry, and fire disaster during the evacuation. The results can be used for the assessments of building designs regarding fire safety."
"""AI""","For accurate evacuation simulations during fire incidents, a comprehensive understanding of the pivotal factors is essential. This paper introduces an agent-based simulation model that amalgamates the fire scene and building geometry. The model capitalizes on a Fire Dynamics Simulator (FDS), rooted in computational fluid dynamics, coupled with Geographic Information System (GIS) data to simulate occupant responses. The FDS generates building entities for simulation, while spatial analysis of GIS data represents occupants' knowledge of the building layout. The influence of the fire hinges on a hazard assessment of combustion products. Agent behavior and decisions are shaped by environmental elements and the fire's dynamics. A case study serves to showcase the effectiveness of the evacuation model in capturing the interplay and interconnections between critical elements such as occupants, building geometry, and fire emergencies during evacuation scenarios. The model's outcomes offer insights applicable to evaluating building designs in terms of fire safety considerations."
"""Human""","Conflicting results from previous research relating information technology (IT) investments and firm performance suggest that there is no direct relationship between IT investments and firm performance. The resource-based view (RBV) of the firm is introduced as a research tool to examine how IT resources and capabilities affect firm performance. A theoretical rationale model is then used to investigate the relationship between IT investment and firm performance. The IT capability is assumed to be an important moderator variable rather than a mediator variable linking IT investments to firm performance, while the time period, the firm size, and the industry type, all recognized as factors influencing performance, are treated as control variables. The model and hypotheses are verified by sample data from leading IT firms in China. The data confirms the moderating effect of the IT capability."
"""AI""","The conflicting outcomes seen in prior research concerning the correlation between Information Technology (IT) investments and firm performance highlight the absence of a straightforward link between the two. This study introduces the Resource-Based View (RBV) of the firm as a tool to delve into how IT resources and capabilities impact firm performance. A theoretical model rooted in rationality is then employed to explore the connection between IT investment and firm performance. Notably, the IT capability is deemed a crucial moderator variable rather than a mediating one that bridges IT investments and firm performance. Meanwhile, the time period, firm size, and industry type—factors acknowledged to influence performance—are handled as control variables. To validate the model and hypotheses, data from prominent IT firms in China is utilized. The analysis substantiates the moderating impact of the IT capability, shedding light on the multifaceted dynamics that affect the relationship between IT investments and firm performance."
"""Human""","State-space equations were applied to formulate the queuing and delay of traffic at a single intersection in this paper. The signal control of a single intersection was then modeled as a discrete-time optimal control problem, with consideration of the constraints of stream conflicts, saturation flow rate, minimum green time, and maximum green time. The problem cannot be solved directly due to the nonlinear constraints. However, the results of qualitative analysis were used to develop a first-phase signal control algorithm. Simulation results show that the algorithm substantially reduces the total delay compared to fixed-time control."
"""AI""","In this paper, state-space equations were employed to create a formulation for the queuing and delay of traffic at a single intersection. Subsequently, the signal control for this single intersection was conceptualized as a discrete-time optimal control issue, accounting for factors such as stream conflicts, saturation flow rate, minimum green time, and maximum green time. The presence of nonlinear constraints complicates direct solutions to this problem. However, leveraging qualitative analysis outcomes, a first-phase signal control algorithm was devised. Through simulations, the algorithm demonstrated significant reductions in total delay when compared to fixed-time control strategies. This work showcases the potential of using state-space equations and optimization techniques to enhance traffic management at intersections, leading to tangible improvements in traffic flow and congestion mitigation."
"""Human""","The technology acceptance model (TAM) is an important tool in information technology research. Many scholars have applied the classical TAM to various research domains. However, the relationships between variables in these TAM models are not strongly desired. Thirty-four articles selected from international journals were analyzed to show that most of the relationships in the classical TAM are significant, but the stabilities of these relationships differ. The significant positive relationships between perceived ease of use and its independent variables are more stable than the others. Various factors can strengthen or weaken these relationships."
"""AI""","The Technology Acceptance Model (TAM) stands as a pivotal instrument in information technology research, widely utilized across diverse research domains. Nonetheless, the exact interconnections between variables within these TAM models often lack robust clarity. A comprehensive analysis of thirty-four articles extracted from international journals sheds light on this issue. It is evident that while most relationships within the classical TAM exhibit significance, their degrees of stability diverge. Specifically, the substantial positive correlations between perceived ease of use and its associated independent variables are notably more consistent than other associations. Numerous factors contribute to the augmentation or diminution of these linkages, underscoring the intricate dynamics at play in technology acceptance and use. This study provides crucial insights into the nuanced relationships within the TAM framework, offering a more comprehensive understanding of its application in various contexts."
"""Human""","Lack of trust is one of the most important obstructions to consumers' acceptance of e-commerce. With the increase of companies going online from offline, trust transfer has been found between offline presence and online presence. By applying the entitativity theories, this study empirically tested the trust transference from offline presence to online presence and addressed the underlying reasons through a questionnaire investigation on the customers of Lianhua supermarkets and Lianhua OK online supermarkets. Process integration was used as a useful means to promote this transfer process. For a click-and-mortar company such as Lianhua Group, its established offline trust is one of the major sources contributing to its initial online trust building. The results from this study suggest a new way to enhance e-commerce acceptance"
"""AI""","The lack of trust remains a significant obstacle in fostering consumer acceptance of e-commerce. As more businesses transition from offline to online operations, a noteworthy phenomenon emerges: trust transfer between offline and online realms. Drawing on entitativity theories, this study delves into the empirical examination of trust transference from offline to online presence. To elucidate the underlying factors, a questionnaire survey was administered to customers of both Lianhua supermarkets and Lianhua OK online supermarkets. Leveraging process integration as a facilitative approach, the study expounds on mechanisms driving this transfer. For entities straddling both online and offline spheres, such as Lianhua Group, the established trust from their offline ventures significantly contributes to the establishment of initial online trust. Findings from this study underscore a novel avenue for bolstering e-commerce acceptance, offering valuable insights into trust dynamics within the digital landscape."
"""Human""","As one of the most widely accepted adoption models in information systems research, the technology acceptance model (TAM) focuses exclusively on cognition-oriented constructs such as perceived usefulness and perceived ease of use. This perspective may have limited the explanatory power of TAM when it is utilized in studying consumers' adoption intentions of online shopping. Based on the contrasts between e-commerce systems and traditional workplace information systems as well as empirical findings from a variety of recent e-commerce research works, this paper analyzes an extended model which integrates three additional constructs: trust, social presence, and perceived enjoyment. The interrelationship between these constructs is also explained. Empirical validations of this extended model are expected in future research."
"""AI""","Regarded as one of the most prevalent adoption models in information systems research, the Technology Acceptance Model (TAM) predominantly centers around cognition-centric constructs such as perceived usefulness and perceived ease of use. This orientation, however, might curtail TAM's explanatory efficacy in investigating consumer adoption intentions within online shopping contexts. Drawing on distinctions between e-commerce systems and traditional workplace information systems, as well as insights gleaned from contemporary e-commerce research, this paper advocates an extended model. This augmented framework incorporates three supplementary constructs: trust, social presence, and perceived enjoyment. Moreover, the interplay among these constructs is elucidated. The envisaged empirical validation of this extended model is slated for future research endeavors. In considering the dynamics of online shopping, this paper marks a stride towards comprehensively capturing the multifaceted factors influencing consumer adoption behaviors."
"""Human""","As semiconductor technology advances, there will be billions of transistors on a single chip. Chip many-core processors are emerging to take advantage of these greater transistor densities to deliver greater performance. Effective fault tolerance techniques are essential to improve the yield of such complex chips. In this paper, a core-level redundancy scheme called N+Mis proposed to improve N-coreprocessors' yield by providing M spare cores. In such architecture, topology is an important factor because it greatly affects the processors' performance. The concept of logical topology and a topology reconfiguration problem are introduced, which is able to transparently provide target topology with lowest performance degradation as the presence of faulty cores on-chip. A row rippling and column stealing (RRCS) algorithm is also proposed. Results show that PRCS can give solutions with average 13.80/0 degradation with negligible computing time"
"""AI""","With the relentless progress of semiconductor technology, the advent of chips hosting billions of transistors is imminent. This surge in transistor density lays the groundwork for chip many-core processors to harness heightened performance levels. Consequently, adept fault tolerance techniques are imperative to enhance the yield of these intricate chips. This paper introduces an innovative core-level redundancy strategy named N+M, aimed at bolstering the yield of N-core processors by integrating M spare cores. Within such architectures, the role of topology looms large, exerting a profound influence on processor performance. In this vein, the paper introduces the concept of logical topology, coupled with a pertinent topology reconfiguration challenge. This reconfiguration seeks to seamlessly furnish the desired topology, with minimal performance repercussions in the face of on-chip faulty cores. A solution is proffered via the row rippling and column stealing (RRCS) algorithm. Empirical findings highlight that this approach can achieve solutions with a modest average performance degradation of 13.80%, underpinned by negligible computational time investments. This research underscores the criticality of effective fault tolerance strategies in propelling the capabilities of many-core processors while maintaining performance integrity."
"""Human""","The cellular automata (CA) micro-simulation model was used to describe the behavior of the mixed traffic flows at crosswalks where the pedestrians compete with the vehicles to cross the roadway. The focus of this paper is the behavior of pedestrians and the influence of pedestrians' behavior on the vehicle flow, pedestrian flows, and the vehicle waiting time. The proportion of pedestrians who do not obey traffic laws, the group effect, and expected waiting time of pedestrians, regarded as the most important pedestrian characteristics, are taken into consideration in the analysis. Simulation results show the ability of the microsimulation to capture the most important features of mixed traffic flow."
"""AI""","This study employs a cellular automata (CA) micro-simulation model to capture the dynamics of mixed traffic interactions at crosswalks, where pedestrians and vehicles contend for the right of way. Central to this paper is the examination of pedestrians' behavior and its ripple effects on vehicle and pedestrian flows, as well as vehicle waiting times. The analysis embraces key pedestrian attributes, including the proportion of pedestrians flouting traffic regulations, the group effect, and pedestrians' expected waiting times – all of which exert notable influence. The simulation outcomes underscore the microsimulation's proficiency in capturing pivotal aspects of mixed traffic flow dynamics. Through this research, a comprehensive understanding emerges of the intricate interplay between pedestrians and vehicles at crosswalks, fostering insights into optimizing traffic management strategies."
"""Human""","This paper presents a novel method of lossy image compression for digital colorful image sensors with Bayer color filter arrays (CFAs) based on an analysis of existing compression-first methods. An improved compression and decompression structure is described. The captured CFA raw data are firstly lowpass filtered in RGB space by a smooth filter followed by a down-sampling operation. Next, the data are transformed from RGB space to YCbCr space. Lastly, the filtered data in YCbCr space are compressed directly before full color interpolation to avoid redundancy. The presented method can provide a lower compression ratio and has lower complexity than both conventional interpolation-first image compression methods and other similar existing compression-first methods."
"""AI""","This paper introduces an innovative approach to lossy image compression tailored for digital colorful image sensors featuring Bayer color filter arrays (CFAs). The method stems from a meticulous examination of existing compression-first techniques and offers an enhanced compression and decompression framework. The process commences with the application of a smooth filter to the captured CFA raw data within the RGB space, followed by a down-sampling operation. Subsequently, the data undergo a conversion from RGB space to YCbCr space. Ultimately, the filtered YCbCr data are compressed prior to full color interpolation, mitigating redundancy. This novel methodology exhibits distinct advantages, including a reduced compression ratio and lower complexity when contrasted with conventional interpolation-first image compression techniques, as well as other comparable compression-first strategies prevalent in the field. By sidestepping redundancy through this innovative approach, the proposed method presents a promising avenue for efficient and effective image compression, well-suited to digital colorful image sensors with Bayer CFAs."
"""Human""","This paper describes a circular first in first out (FIFO) and its protocols which have a very low latency while still maintaining high throughput. Unlike the existing serial FIFOs based on asynchronous micropipelines, this FIFO's cells communicate directly with the input and output ports through a common bus, which effectively eliminates the data movement from the input port to the output port, thereby reducing the latency and the power consumption. Furthermore, the latency does not increase with the number of FIFO stages. Single-track asynchronous protocols are used to simplify the FIFO controller design, with only three C-gates needed in each cell controller, which substantially reduces the area. Simulations with the TSMC 0.25 μm CMOS logic process show that the latency of the 4-stage FIFO is less than 581 ps and the throughput is higher than 2.2 GHz."
"""AI""","This paper outlines a circular First In First Out (FIFO) structure alongside its associated protocols, demonstrating remarkably low latency paired with sustained high throughput. Unlike prevalent serial FIFOs that rely on asynchronous micropipelines, this innovation facilitates direct communication between the FIFO's cells and input/output ports through a shared bus. This streamlined architecture negates the necessity for data movement between input and output, culminating in reduced latency and power consumption. Notably, the latency remains unaffected by the number of FIFO stages. The design leverages single-track asynchronous protocols to simplify FIFO controller design, with each cell controller necessitating just three C-gates, substantially curtailing area requirements. Employing simulations with the TSMC 0.25 μm CMOS logic process, results indicate a latency of under 581 picoseconds and a throughput exceeding 2.2 gigahertz for the 4-stage FIFO. This breakthrough offers a compelling prospect for efficient data management, striking an optimal balance between latency and throughput in digital systems."
"""Human""","Truck scheduling and storage allocation, as two separate subproblems in port operations, have been deeply studied in past decades. However, from the operational point of view, they are highly interdependent. Storage allocation for import containers has to balance the travel time and queuing time of each container in yard. This paper proposed an integer programming model handling these two problems as a whole. The objective of this model is to reduce congestion and waiting time of container trucks in the terminal so as to decrease the makespan of discharging containers. Due to the inherent complexity of the problem, a genetic algorithm and a greedy heuristic algorithm are designed to attain near optimal solutions. It shows that the heuristic algorithm can achieve the optimal solution for small-scale problems. The solutions of small- and large-scale problems obtained from the heuristic algorithm are better than those from the genetic algorithm."
"""AI""","Truck scheduling and storage allocation, traditionally treated as distinct subproblems in port operations, have undergone exhaustive investigation over the years. Yet, when examined from an operational lens, their interdependence becomes evident. The allocation of storage for import containers mandates a delicate equilibrium between travel time and queuing time, as each container navigates the yard. This paper introduces an integer programming model that seamlessly addresses these dual challenges. The model strives to alleviate congestion and waiting times endured by container trucks within the terminal, thereby diminishing the overall discharging time of containers. Given the inherent intricacies of the problem, the study presents a genetic algorithm and a pragmatic heuristic algorithm, both aimed at deriving nearly optimal solutions. Impressively, the heuristic algorithm attains optimal solutions for small-scale problems, demonstrating its effectiveness. Moreover, for problems of varying scales, solutions from the heuristic algorithm outperform those generated by the genetic algorithm. This research underscores the efficacy of an integrated approach in optimizing truck scheduling and storage allocation within port operations."
"""Human""","Tangible augmented reality (TAR) technology opens a novel realm which integrates the computergenerated elements into the real word. Its applications into design education have been explored with a limitation to this entire area. TAR offers an innovative learning space by merging digital learning materials into the format of media with tools or objects which are direct parts of the physical space. It is therefore conceived that such combination opens new perspectives in teaching and learning. This paper presented and evaluated one TAR system to improve the pedagogical effectiveness of experiential and collaborative learning process in urban design education. The results from the experiments were analyzed under a previously developed theoretical framework, which show that TAR can enhance the design activities in some collaborative work."
"""AI""","Tangible Augmented Reality (TAR) technology introduces a groundbreaking dimension by seamlessly integrating computer-generated elements into the real world. While its applications in design education have been explored, the scope remains somewhat confined. TAR ushers in a pioneering learning environment, merging digital learning materials with tangible tools or objects inherent to the physical space. This amalgamation sparks fresh perspectives in teaching and learning. This paper introduces and assesses a TAR system aimed at enhancing the pedagogical efficacy of experiential and collaborative learning in urban design education. The experiments' outcomes were scrutinized using an established theoretical framework, revealing that TAR has the potential to enrich design activities within collaborative contexts. This research underscores the transformative role of TAR in fostering enhanced learning experiences within design education."
"""Human""","We evaluate the technical feasibility of applying emerging wireless network technologies for resources tracking at building construction sites. We first identify practical constraints in solving resourcetracking problems in an enclosed or partially covered environment. We then compare pros and cons of available localization principles and examine the latest wireless communication technologies, including Wi-Fi, Bluetooth, Ultra-Wideband (UWB) and ZigBee. We find that the ZigBee-based wireless sensor network and the received signal strength indicator (RSSI) localization method are most promising to tackle on-site tracking of construction resources. Finally, we anticipate some application challenges associated with deploying wireless sensor networks for resources tracking in the practical context"
"""AI""","We assess the technical viability of utilizing emerging wireless network technologies to facilitate resource tracking within building construction sites. Our analysis commences by identifying practical constraints encountered when addressing resource tracking issues within enclosed or partially covered environments. Subsequently, we scrutinize the merits and drawbacks inherent to available localization principles, juxtaposing these against the backdrop of contemporary wireless communication technologies, such as Wi-Fi, Bluetooth, Ultra-Wideband (UWB), and ZigBee. Our investigation reveals that the ZigBee-based wireless sensor network, coupled with the received signal strength indicator (RSSI) localization method, emerges as a promising avenue for effectively addressing on-site resource tracking challenges within construction contexts. As a culmination, we anticipate potential application challenges tied to the deployment of wireless sensor networks for resource tracking, firmly rooted in practical implementation realities."
"""Human""","In order to improve performance and security of image encryption algorithm effectively based on chaotic sequences, an extended chaotic sequence generating method is presented based on logistic chaotic system using Bernstein form Bézier curve generating algorithm. In order to test the pseudorandom performance of the extended chaotic sequence, we also analyze random performance, autocorrelation performance, and balance performance of the extended chaotic sequence. Simulation results show that the extended chaotic sequence generated using our method is pseudorandom and its correlation performance and balance performance are good. As an application, we apply the extended chaotic sequence in image encryption algorithm, the simulation results show that the performance of the encrypted image using our method is better than that using logistic chaotic sequence."
"""AI""","To enhance the efficacy and security of image encryption algorithms rooted in chaotic sequences, this study introduces an expanded method for generating chaotic sequences. Leveraging the logistic chaotic system and the Bernstein form Bézier curve generating algorithm, the extended chaotic sequence generation technique is advanced. The performance of this extended chaotic sequence is thoroughly evaluated through an assessment of its pseudorandomness, as well as its autocorrelation and balance characteristics. Simulation outcomes affirm that the generated extended chaotic sequence exhibits pseudorandom behavior, displaying favorable autocorrelation and balance performance. Furthermore, the study applies the extended chaotic sequence within an image encryption algorithm, revealing superior performance compared to the utilization of a traditional logistic chaotic sequence. By introducing this advanced chaotic sequence generation method, the study contributes to the advancement of image encryption techniques with improved performance and security."
"""Human""","Automatic gain control (AGC) has been used in many applications. The key features of AGC, including a steady state output and static/dynamic timing response, depend mainly on key parameters such as the reference and the filter coefficients. A simple model developed to describe AGC systems based on several simple assumptions shows that AGC always converges to the reference and that the timing constant depends on the filter coefficients. Measures are given to prevent oscillations and limit cycle effects. The simple AGC system is adapted to a multiple AGC system for a TV tuner in a much more efficient model. Simulations using the C language are 16 times faster than those with MATLAB, and 10 times faster than those with a mixed register transfer level (RTL)-simulation program with integrated circuit emphasis (SPICE) model."
"""AI""","Automatic Gain Control (AGC) finds widespread application across various domains. AGC's defining attributes, encompassing its steady-state output and static/dynamic timing response, pivot significantly around pivotal parameters like the reference and filter coefficients. A streamlined model, fashioned to encapsulate AGC systems, based on a series of elementary assumptions, reveals that AGC consistently converges towards the reference point, and the timing constant hinges on filter coefficients. Strategies are outlined to stave off oscillations and confine the impact of limit cycle effects. This rudimentary AGC blueprint transitions into a more efficient model, catering to a multiple AGC configuration for a television tuner. Utilizing the C language for simulations expedites processes, rendering outcomes 16 times swifter than MATLAB and 10 times faster than a mixed Register Transfer Level (RTL) simulation program interfaced with a model focused on Integrated Circuit Emphasis (SPICE)."
"""Human""","Semi-solid metal casting and forming is a promising production method for a wide range of metal alloys. In spite of many applications for semi-solid processed light alloys, few works have reported on the semi-solid processing of iron and steel. In this research, an inclined plate was used to change the dendritic structure of iron to globular. The effects of the length and slope of the plate on the casting structure were examined. The results show that the process effectively changes the dendritic structure to globular. A sloped plate angle of 7.5° and length of 560 mm with a cooling rate of 67 K • s −1 gave the optimum graphite nodularity and solid particle globularity. The results also show that the sloped plate more easily prevents inoculant fading since the total time processing is rather short. In addition the semi-solid ductile cast iron prepared using the inclined plate method was reheated to examine the effect of reheating conditions on the micro-structure and coarsening kinetics of the alloy. The solid fractions at different reheating temperatures and holding times were used to find the optimum reheating temperature range."
"""AI""","Semi-solid metal casting and forming present a promising avenue for producing various metal alloys. While there have been numerous applications of semi-solid processing for lightweight alloys, the focus on iron and steel remains limited. This research endeavors to transform the dendritic structure of iron into a globular form using an inclined plate. The study investigates the impact of plate length and slope on the casting structure. The findings reveal that this process effectively alters the dendritic structure to a globular morphology. An optimal configuration is achieved with a sloped plate angle of 7.5° and a length of 560 mm, coupled with a cooling rate of 67 K • s −1, resulting in optimal graphite nodularity and solid particle globularity. Moreover, the sloped plate design demonstrates greater effectiveness in curbing inoculant fading due to its relatively short processing time. Subsequently, the research subjects semi-solid ductile cast iron prepared via the inclined plate method to a reheating process. This examination delves into the influence of reheating conditions on the alloy's microstructure and coarsening kinetics. Through analysis of solid fractions at varying reheating temperatures and durations, an optimal temperature range for reheating is established."
"""Human""","Currently, the IT-support for energy performance rating of buildings is insufficient. So-called ITplatforms often “built” of an ad-hoc, inconsistent combination of off-the-shelf building management components, distributed data metering equipment and several monitoring software tools. A promising approach to achieve consistent, holistic performance data management is the implementation of an integrated, modular wireless sensor platform. This paper presents an approach of how wireless sensors can be seamlessly integrated into existing and future intelligent building management systems supporting improved building performance and diagnostics with an emphasis on energy management."
"""AI""","The existing IT support for assessing the energy performance of buildings is inadequate, characterized by ad-hoc combinations of off-the-shelf building management components, distributed data metering equipment, and various monitoring software tools within IT platforms. To establish a more cohesive and comprehensive approach to managing performance data, the integration of an integrated, modular wireless sensor platform is proposed. This paper outlines a strategy for seamlessly incorporating wireless sensors into both current and upcoming intelligent building management systems. The focus lies on enhancing building performance and diagnostics, particularly in the realm of energy management."
"""Human""","The growing size of the multiprocessor systems increases their vulnerability to component failures. It is crucial to local and to replace the fault processors to maintain system's high reliability. The fault diagnosis is the process of identifying faulty processors in a system through testing. This paper establishes the diagnosabilities of the incomplete star graph S n (n> 4) with missing links under the PMC model and its variant, the BGM model, and shows that the diagnosabilities of incomplete star graph S n under these two diagnostic models can be determined by the minimum degree of its topology structure. This method can also be applied to the other existing multiprocessor systems"
"""AI""","As multiprocessor systems grow in size, their susceptibility to component failures increases, necessitating the timely identification and replacement of faulty processors to maintain system reliability. Fault diagnosis, the process of identifying malfunctioning processors through testing, is essential in this context. This paper examines the diagnosabilities of incomplete star graphs, denoted as S n (where n > 4), that possess missing links. The study is conducted under the PMC (Parallel Multiple Comparison) model and its variant, the BGM (Between-Group and within-Group Misconnection) model. The paper establishes that the diagnosabilities of incomplete star graphs S n , governed by these two diagnostic models, can be determined by evaluating the minimum degree of their topology structure. This approach is not limited to incomplete star graphs but can be extended to other existing multiprocessor systems as well."
"""Human""","This paper presents a dynamic model of a planar flexible inverted pendulum system under the frame of multi-body dynamics by floating frame of reference formulation (FFRF). By proper simplification and linearization, the state space equation of the system was established for linear analysis and control design. The simulation method for such a coupled system by multi-body dynamics program was also provided. The designed controller with a simple low-pass filter for the flexible inverted pendulum was validated by the simulation of a simple flexible pendulum sample. The result demonstrates a new method of designing and verifying a feedback controller of a flexible multi-body system."
"""AI""","In this paper, a dynamic model of a planar flexible inverted pendulum system is introduced within the context of multi-body dynamics, utilizing the floating frame of reference formulation (FFRF). Through appropriate simplification and linearization techniques, the state space equation governing the system's behavior is derived, enabling subsequent linear analysis and control design. Additionally, the paper outlines a simulation approach to handle the interactions within such a coupled system using a multi-body dynamics program. To validate the effectiveness of the proposed method, a controller was designed for the flexible inverted pendulum, featuring a straightforward low-pass filter. The designed controller's performance was assessed through simulation using a basic flexible pendulum model, showcasing a novel approach for developing and validating feedback controllers for flexible multi-body systems."
"""Human""","We present an overview of unique properties of metamaterials, especially negative index materials. These have allowed novel applications, concepts and devices to be developed in the past decade. A review of the progress made in this field is presented with a focus on microwave devices and applications in wireless communications. Since a metamaterial can be regarded as a continuous medium with effective dielectric permittivity and effective magnetic permeability, we present the procedure for the extraction of effective electromagnetic parameters for a guided wave structure with split-ring resonators. As examples, our own designs of bandpass and triple-band filters, which are constructed using metamaterial-inspired resonator elements, are presented and discussed."
"""AI""","This paper provides an overview of the distinctive characteristics of metamaterials, particularly negative index materials, and highlights their role in fostering novel applications, concepts, and devices over the past decade. The review centers on advancements made in this field, with a specific emphasis on microwave devices and their applications in wireless communications. Given that a metamaterial can be treated as a continuous medium characterized by effective dielectric permittivity and magnetic permeability, the paper outlines the procedure for deducing effective electromagnetic parameters for a guided wave structure utilizing split-ring resonators. The article also showcases self-developed designs of bandpass and triple-band filters that utilize resonator elements inspired by metamaterials, delving into their construction and discussing their significance."
"""Human""","Optimization of adaptive traffic signal timing is one of the most complex problems in traffic control systems. This paper presents an adaptive transit signal priority (TSP) strategy that applies the parallel genetic algorithm (PGA) to optimize adaptive traffic signal control in the presence of TSP. The method can optimize the phase plan, cycle length, and green splits at isolated intersections with consideration for the performance of both the transit and the general vehicles. A VISSIM (VISual SIMulation) simulation testbed was developed to evaluate the performance of the proposed PGA-based adaptive traffic signal control with TSP. The simulation results show that the PGA-based optimizer for adaptive TSP outperformed the fully actuated NEMA control in all test cases. The results also show that the PGA-based optimizer can produce TSP timing plans that benefit the transit vehicles while minimizing the impact of TSP on the general vehicles."
"""AI""","""This paper introduces an adaptive transit signal priority (TSP) strategy that employs a parallel genetic algorithm (PGA) to optimize the adaptive control of traffic signals while incorporating TSP functionality. The approach focuses on optimizing phase plans, cycle lengths, and green splits at isolated intersections, taking into account the performance of both transit and general vehicles. A simulation testbed using VISSIM (VISual SIMulation) was developed to assess the effectiveness of the proposed PGA-based adaptive traffic signal control with TSP. The simulation outcomes demonstrate that the PGA-based optimizer for adaptive TSP surpasses fully actuated NEMA control across all test scenarios. Moreover, the results reveal that the PGA-based optimizer can generate TSP timing plans that improve transit vehicle flow while minimizing the impact on general vehicle traffic."""
"""Human""","""The United States real estate market is currently facing its worst hit in two decades due to the slowdown of housing sales. The most affected by this decline are real estate investors and home developers who are currently struggling to break-even financially on their investments. For these investors, it is of utmost importance to evaluate the current status of the market and predict its performance over the shortterm in order to make appropriate financial decisions. This paper presents the development of artificial neural network based models to support real estate investors and home developers in this critical task. The paper describes the decision variables, design methodology, and the implementation of these models. The models utilize historical market performance data sets to train the artificial neural networks in order to predict unforeseen future performances. An application example is analyzed to demonstrate the model capabilities in analyzing and predicting the market performance. The model testing and validation showed that the error in prediction is in the range between −2% and +2%."""
"""AI""","""Amid the current slowdown in housing sales, the US real estate market is grappling with its most significant downturn in twenty years, resulting in challenges for real estate investors and home developers to break even on their investments. In this context, accurately assessing the market's present status and predicting its near-term trajectory are paramount for informed financial decision-making. This study introduces artificial neural network-based models tailored to aid real estate investors and home developers in this crucial task. The paper outlines the decision variables, design process, and implementation of these models. Leveraging historical market data sets, the models train artificial neural networks to forecast future performance trends. An illustrative case study showcases the models' proficiency in analyzing and predicting market dynamics. Rigorous testing and validation demonstrate prediction errors within the range of -2% to +2%."""
"""Human""","""A test strategy for analog filters was proposed. The output voltage and supply current of the circuit were monitored when using the oscillation test technique. The frequency, average value, maximum value and amplitude of both output voltage and supply current were taken as test parameters. Tolerance bands of test parameters were analyzed. Fault detectabilities of test parameters were compared and combined, and optimal parameter sets were derived. Experimental results show that both the output voltage and supply current give significant contribution to fault detection. Considering catastrophic, single and double parametric faults, the fault coverage in testing the benchmark circuit can be raised from 90.60% for traditional voltageonly oscillation test strategy to 97.2% by monitoring both output voltage and current parameters."""
"""AI""","""The paper presents a novel test strategy designed specifically for analog filters. This strategy involves utilizing the oscillation test technique to monitor the circuit's output voltage and supply current. Various test parameters such as frequency, average value, maximum value, and amplitude of both output voltage and supply current are measured. The paper then discusses the analysis of tolerance bands for these test parameters. To enhance fault detection, the fault detectabilities of different test parameters are compared and combined, leading to the identification of optimal parameter sets. Experimental results indicate that both output voltage and supply current measurements play a significant role in fault detection. When considering various fault types, including catastrophic, single, and double parametric faults, the fault coverage achieved by monitoring both output voltage and current parameters increases from 90.60% (using the traditional voltage-only oscillation test strategy) to 97.2%."""
"""Human""","""Closed-form solutions were developed to optimize kinematics design of a 2-degree-of-freedom (2-DOF) planar parallel manipulator. The optimum design based on the workspace was presented. Meanwhile, a global, comprehensive conditioning index was introduced to evaluate the kinematic designs. The optimal parallel manipulator is incorporated into a 5-DOF hybrid machine tool which includes a 2-D0F rotational milling head and a long movement worktable. The results show that the planar parallel manipulator-based machine tool can be successfully used to machine blades and guide vanes for a hydraulic turbine."""
"""AI""","""The paper introduces closed-form solutions for optimizing the kinematic design of a 2-degree-of-freedom (2-DOF) planar parallel manipulator. The optimization process is based on considerations of the workspace, ensuring that the manipulator performs optimally within its range of motion. Additionally, a comprehensive conditioning index is introduced to globally evaluate the kinematic designs, allowing for a thorough assessment of their performance.The optimized parallel manipulator is then integrated into a larger 5-degree-of-freedom (5-DOF) hybrid machine tool. This hybrid machine tool comprises a 2-DOF rotational milling head and a long-range movement worktable. The results of this integration demonstrate that the planar parallel manipulator-based machine tool proves effective in machining blades and guide vanes for hydraulic turbines. Overall, the study showcases the successful application of optimized parallel manipulators in complex machining tasks, particularly in the context of the hydraulic turbine industry."""
"""Human""","""This paper presents a model matching control (MMC) method based on the sliding mode control (SMC) method for longitudinal acceleration tracking control in a vehicular stop-and-go cruise control system. The nonlinearity of the vehicle acceleration response at low speeds was analyzed to develop a transfer function model of the vehicle longitudinal dynamics using the least-mean-square system identification technique. This transfer function was then used to design the MMC controller, including an SMC feedback compensator. The system combines the advantages of the two control methods with robust control and rapid response. Simulations show that the controller enhances the rapid trackability to the vehicle acceleration and improves the system's robustness at low speeds compared with conventional PID MMC controllers."""
"""AI""","""The paper introduces a new approach to control vehicular stop-and-go cruise systems by combining model matching control (MMC) with sliding mode control (SMC). It addresses the challenge of nonlinearity in vehicle acceleration at low speeds by developing a transfer function model through system identification. The MMC controller incorporates SMC feedback to enhance both rapid acceleration tracking and system robustness, resulting in improved performance compared to conventional PID-based MMC controllers. Simulation results demonstrate the effectiveness of the proposed method in achieving accurate and robust acceleration control in such systems."""
"""Human""","""The paper analyzes the equity of road resources distribution in urban areas by modeling the competitive relationship among different road users. A logistic model is used to describe the development of different traffic modes in the transportation network. The system is similar to the species competition model, so a two-species model is used to analyze the relationship between users based on the stability of the equilibrium points. The Lotka-Volterra model is then used to describe the multi-species cases with numerical examples, showing that this model can describe the effects of the road space distribution on the competitive user relationships. Policy makers must ensure the equity of road resources distribution so that each urban transportation mode is properly developed for sustainable social development."""
"""AI""","""The study focuses on analyzing the fairness of distributing road resources within urban areas by examining the competitive interactions between various road users. Using a logistic model to represent the growth of different transportation modes, the paper applies principles from species competition models to understand the dynamics among users. A two-species model investigates equilibrium stability, while a Lotka-Volterra model extends the analysis to multiple species scenarios. Numerical examples demonstrate the model's ability to depict the impacts of road space allocation on user competition dynamics. The findings emphasize the importance of equitable resource distribution for sustainable urban transportation development."""
"""Human""","""This paper describes the design of a unified framework for a multilingual text-to-speech (TTS) synthesis engine — Crystal. The unified framework defines the common TTS modules for different languages and/or dialects. The interfaces between consecutive modules conform to the speech synthesis markup language (SSML) specification for standardization, interoperability, multilinguality, and extensibility. Detailed module divisions and implementation technologies for the unified framework are introduced, together with possible extensions for the algorithm research and evaluation of the TTS synthesis. Implementation of a mixed-language TTS system for Chinese Putonghua, Chinese Cantonese, and English demonstrates the feasibility of the proposed unified framework."""
"""AI""","""The paper presents the development of a unified framework named Crystal for multilingual text-to-speech (TTS) synthesis. This framework aims to provide a common set of TTS modules applicable to various languages and dialects, ensuring interoperability, standardization, and extensibility through adherence to the speech synthesis markup language (SSML) specification. The study outlines the module divisions, implementation technologies, and potential extensions of the framework for algorithm research and evaluation purposes. To illustrate its effectiveness, a mixed-language TTS system encompassing Chinese Putonghua, Chinese Cantonese, and English is implemented within the proposed unified framework."""
"""Human""","""An image and video quality assessment method was developed using neural network and support vector machines (SVM) with the peak signal to noise ratio (PSNR) and the structure similarity indexes used to describe image quality. The neural network was used to obtain the mapping functions between the objective quality assessment indexes and subjective quality assessment. The SVM was used to classify the images into different types which were accessed using different mapping functions. Video quality was assessed based on the quality of each frame in the video sequence with various weights to describe motion and scene changes in the video. The number of isolated points in the correlations of the image and video subjective and objective quality assessments was reduced by this method. Simulation results show that the method accurately accesses image quality. The monotonicity of the method for images is 6.94% higher than with the PSNR method, and the root mean square error is at least 35.90% higher than with the PSNR."""
"""AI""","""The paper introduces a novel method for assessing image and video quality using a combination of neural networks and support vector machines (SVM). The method employs peak signal-to-noise ratio (PSNR) and structural similarity indexes as descriptors of image quality. Neural networks are utilized to establish relationships between objective and subjective quality assessment measures, while SVM is employed to classify images based on different mapping functions. For video quality assessment, the method considers individual frame quality with varying weights to account for motion and scene changes. By using this approach, the method reduces the number of isolated points in subjective and objective quality assessments for images and videos. Simulation results demonstrate the method's accuracy in assessing image quality, with higher monotonicity and lower root mean square error compared to the PSNR method."""
"""Human""","""Two classical theories, the technology acceptance model (TAM) and the innovation diffusion theory (lOT), were integrated into a model for analyzing individual information technology (IT) adoption behavior. In this IDT-TAM framework, the perceived characteristics of innovation are categorized as subjective evaluation, objective conditions, and interaction factors. The cognitive mechanisms that drive user IT acceptance are analyzed based on the patterns through which these characteristics influence individual user IT adoption behavior. The model was tested using an empirical survey regarding the adoption and use of e-mail in China. Results from a structural equation model analysis illustrate that the model provides meaningful insights for understanding, explaining, and predicting the IT adoption behavior of Chinese users."""
"""AI""","""The paper presents a model that combines two well-known theories, the Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT), to analyze individual adoption behavior of information technology (IT). Within this framework, the perceived characteristics of innovation are divided into subjective evaluation, objective conditions, and interaction factors. The model examines how these characteristics impact users' IT adoption behavior through cognitive mechanisms. The model's effectiveness is demonstrated through an empirical survey focused on e-mail adoption in China. The results, analyzed using a structural equation model, reveal that the model offers valuable insights into understanding, explaining, and predicting IT adoption behavior among Chinese users."""
"""Human""","""Automatic defect detection in X-ray images is currently a focus of much research at home and abroad. The technology requires computerized image processing, image analysis, and pattern recognition. This paper describes an image processing method for automatic defect detection using image data fusion which synthesizes several methods including edge extraction, wave profile analyses, segmentation with dynamic threshold, and weld district extraction. Test results show that defects that induce an abrupt change over a predefined extent of the image intensity can be segmented regardless of the number, location, shape, or size. Thus, the method is more robust and practical than the current methods using only one method."""
"""AI""","""The paper addresses the ongoing research on automatic defect detection in X-ray images. The method employs computerized image processing, analysis, and pattern recognition techniques. It presents an approach based on image data fusion, combining edge extraction, wave profile analysis, dynamic threshold segmentation, and weld district extraction. This method proves effective in detecting defects causing sudden intensity changes in the image, irrespective of their number, location, shape, or size. Compared to single-method approaches, this fusion-based method demonstrates greater robustness and practicality in automatic defect detection."""
"""Human""","""One key function of intelligent transportation systems is to automatically detect abnormal traffic phenomena and to help further investigations of the cause of the abnormality. This paper describes a robust principal components analysis (RPCA)-based abnormal traffic flow pattern isolation and loop detector fault detection method. The results show that RPCA is a useful tool to distinguish regular traffic flow from abnormal traffic flow patterns caused by accidents and loop detector faults. This approach gives an effective traffic flow data pre-processing method to reduce the human effort in finding potential loop detector faults. The method can also be used to further investigate the causes of the abnormality."""
"""AI""","""Intelligent transportation systems play a crucial role in identifying unusual traffic occurrences and aiding in the analysis of their origins. This paper introduces a method for detecting abnormal traffic patterns and loop detector faults using robust principal components analysis (RPCA). The approach effectively separates normal traffic flows from anomalies caused by accidents and detector faults. By employing RPCA, this technique streamlines traffic data preprocessing, lessening the need for human intervention in identifying possible loop detector issues. Furthermore, it can serve as a tool for delving deeper into the reasons behind such abnormalities."""
"""Human""","""One of the difficulties that goal-oriented requirements analyses encounters is that the efficiency of the goal refinement is based on the analysts' subjective knowledge and experience. To improve the efficiency of the requirements elicitation process, engineers need approaches with more systemized analysis techniques. This paper integrates the goal-oriented requirements language i ∗ with concepts from a structured problem analysis notation, problem frames (PF). The PF approach analyzes software design as a contextualized problem which has to respond to constraints imposed by the environment. The proposed approach is illustrated using the meeting scheduler exemplar. Results show that integration of the goal and the problem analysis enables simultaneous consideration of the designer's subjective intentions and the physical environmental constraints."""
"""AI""","""Efficiency in goal-oriented requirements analysis often faces challenges due to the reliance on analysts' subjective insights. To enhance the effectiveness of requirements elicitation, systematic methods are sought to improve the process. This paper merges the i* goal-oriented requirements language with concepts from problem frames (PF), a structured problem analysis notation. PF examines software design within its context, accounting for constraints imposed by the environment. The approach is demonstrated using a meeting scheduler example, illustrating how integrating goal and problem analysis allows for a simultaneous consideration of designer intentions and environmental constraints, yielding more comprehensive insights."""
"""Human""","""Balanced wrapper scan chains are desirable for system-an-chip (SoC) testing because they minimize the time required to transport the test data. A new heuristic algorithm is proposed based on meanvalue approximation and implement fast re-optimization as a subsequence of an earlier best-fit-decrease (BFO) method. The mean length of each scan chain was introduced as an approximation target to balance different scan chains and hence saved testing time. Experimental results present both for assumed arbitrary cores and cores from ITC'02 benchmark and show the effectiveness of the algorithm. The proposed algorithm can provide more balanced wrapper design efficiently for the test scheduling stage"""
"""AI""","""Balanced wrapper scan chains play a crucial role in efficient system-on-chip (SoC) testing by minimizing data transportation time. This paper introduces a novel heuristic algorithm that leverages mean-value approximation and rapid re-optimization as part of a best-fit-decrease (BFO) method. The algorithm aims to balance various scan chains by targeting the mean length of each chain, resulting in time savings during testing. Experimental evaluations using arbitrary cores and ITC'02 benchmark cores highlight the algorithm's effectiveness. The proposed approach efficiently enhances wrapper design for the test scheduling phase, promoting more balanced and streamlined testing procedures."""
"""Human""","""Electronic commerce is becoming increasingly important in business, but lack of intention to purchase has become a main barrier in the development of electronic commerce. Thus, effective measures are needed to promote consumers' intentions to purchase in online consumer to consumer (C2C) stores. This paper postulates that five factors, the perceived ease of use of the website, perceived usefulness of the website, vendor competence, introduction and recommendations of third parties, and vendors' attitude toward customers, influence consumers' intentions to purchase in online C2C stores and this intention directly leads to their action to purchase from online C2C stores. The structural equation modeling (SEM) method was used to analyze empirical data, supporting these hypotheses except for the effect of vendor competence."""
"""AI""","""Electronic commerce has gained immense significance in business, but the obstacle of lacking purchase intent hinders its growth. To overcome this, effective strategies are necessary to enhance consumers' willingness to buy from online consumer-to-consumer (C2C) stores. This study proposes that five factors—perceived ease of website use, perceived website usefulness, vendor competence, endorsements from third parties, and vendors' customer-oriented behavior—impact consumers' intent to purchase from online C2C stores. This intention directly translates into their actual purchases. Empirical data were analyzed using structural equation modeling (SEM), confirming these hypotheses, with the exception of the impact of vendor competence."""
"""Human""","""The distributed antenna system (DAS) is considered as a promising architecture for future wireless access. This paper describes the uplink of a power-controlled circular-layout DAS (CL-DAS) with minimum mean-square error (MMSE) receivers. Results from random matrix theory are used to show that for such a DAS, the per-user sum rate and the total transmit power both converge as the number of users and antennas goes to infinity with a constant ratio of antennas to users. The relationship between the asymptotic per-user sum rate and the asymptotic total transmit power is given for all possible values of the radius of the circle on which antennas are placed. This rate-power relationship is then used to find the optimal radius. With this optimal radius, the CL-DAS is proved to offer a significant gain compared with a traditional co-located antenna system. Simulation results demonstrate the validity of the analysis and the superiority of the DAS."""
"""AI""","""The distributed antenna system (DAS) emerges as a promising architecture for future wireless access. This study focuses on the uplink of a power-controlled circular-layout DAS (CL-DAS) with minimum mean-square error (MMSE) receivers. Leveraging insights from random matrix theory, it's demonstrated that the per-user sum rate and total transmit power converge as the number of users and antennas increases infinitely with a consistent antennas-to-users ratio. The relationship between asymptotic per-user sum rate and total transmit power is established for various circle radii where antennas are positioned. This insight aids in finding the optimal radius, showing that the CL-DAS offers substantial gains compared to traditional co-located antenna systems. Simulation results validate these findings and confirm the advantages of the DAS approach."""
"""Human""","""Based on the theory of task-technology fit and literature review of mobile technology acceptance, a research model is presented based on the features of mobile technologies and the actual usage of mobile applications in business environments, which identifies the characteristics of mobile applications and fitting tasks. An exploratory case is studied to understand how ABC Company, Hong Kong, utilizes mobile selling applications in their sales management process. This study gives some managerial suggestions for enterprises to successfully use mobile applications."""
"""AI""","""Drawing upon task-technology fit theory and a literature review of mobile technology acceptance, this paper introduces a research model that aligns mobile technology features with actual usage of mobile applications in business settings. By identifying key characteristics of mobile applications and their compatibility with specific tasks, the study presents insights into the adoption of mobile selling applications within ABC Company, Hong Kong, and their integration into the sales management process. Through an exploratory case study, the research offers managerial recommendations for enterprises seeking effective utilization of mobile applications."""
"""Human""","""Dynamic airspace management plans and assigns airspace resources to airspace users on demand to increase airspace capacity. Although many studies of air traffic flow management (ATFM) have sought to optimally allocate air traffic to get the best use of given airspace resources, few studies have focused on how to build an efficient air traffic network or how to adjust the current network in real time. This paper presents an integer program model named the dynamic air route open-close problem (DROP). DROP has a cost-based objective function which takes into account constraints such as the shortest occupancy time of routes, which are not considered in ATFM models. The aim of DROP is to determine which routes will be opened to a certain user during a given time period. Simulation results show that DROP can facilitate utilization of air routes. DROP, a simplified version of an air traffic network constructing problem, is the first step towards realizing dynamic airspace management. The combination of ATFM and DROP can facilitate decisions toward more reasonable, efficient use of limited airspace resources."""
"""AI""","""This paper addresses the concept of dynamic airspace management by introducing an integer programming model called the dynamic air route open-close problem (DROP). Unlike traditional air traffic flow management (ATFM) studies that focus on optimizing air traffic allocation within existing airspace, DROP aims to efficiently construct and adjust the air traffic network itself in real-time. The model incorporates a cost-based objective function considering factors such as the shortest route occupancy time, which are often neglected in ATFM models. DROP's objective is to determine which routes should be open for specific users within a given time frame. Simulation results demonstrate that DROP enhances the utilization of air routes and provides a foundation for more effective utilization of limited airspace resources, aligning with the goals of dynamic airspace management."""
"""Human""","""This paper presents analytical expressions for the multiple-input multiple-output (MIMO) channel capacity in frequency-flat Rayleigh fading environments. An exact analytical expression is given for the ergodic capacity for single-input multiple-output (SIMO) channels. The analysis shows that the SIMO channel capacity can be approximated by a Gaussian random variable and that the MIMO channel capacity can be approximated as the sum of multiple SIMO capacities. The SIMO channel results are used to derive approximate closed-form expressions for the MIMO channel ergodic capacity and the complementary cumulative distribution function (CCDF) of the MIMO channel capacity (outage capacity). Simulations show that these theoretical results are good approximations for M IMO systems with an arbitrary number of transmit or receive antennas. Moreover, these analytical expressions are relatively simple which makes them very useful for practical computations."""
"""AI""","""This paper focuses on the analysis of the multiple-input multiple-output (MIMO) channel capacity in frequency-flat Rayleigh fading environments. It provides analytical expressions for the ergodic capacity of single-input multiple-output (SIMO) channels. The analysis reveals that the SIMO channel capacity can be approximated using a Gaussian random variable, and the MIMO channel capacity can be approximated as the sum of multiple SIMO capacities. Based on the SIMO channel results, the paper derives closed-form expressions to approximate the MIMO channel's ergodic capacity and the complementary cumulative distribution function (CCDF) of the MIMO channel capacity (outage capacity). Simulation results demonstrate that these theoretical approximations are accurate for MIMO systems with various numbers of transmit or receive antennas. The presented analytical expressions are straightforward and practical for computations."""
"""Human""","""High spectral efficiency distributed antenna systems (DAS) require vertical Bell-Labs layered space-time (V-BLAST) like spatial multiplexing schemes. However, unlike normal point-to-point multiple input multiple output (MIMO) channels, DAS channels have different large-scale fadings from different transmit antennas, thus making equal power and rate transmission that is feasible in MIMO channels unrealistic in DAS channels. This paper proposes a novel transmit antenna selection scheme with power and rate allocation. The scheme is based on large-scale fading (shadow fading and path loss) and is suitable for VBLAST structures with zero-forcing and successive interference cancellation (ZF-SIC) receivers, ensuring balanced average symbol error rate (SER) performance in each layer. On the receiver side, a fixed detection order is used, which is obtained in the transmit antenna selection process. Simulation results show that the proposed scheme gives good performance gains over equal power and rate transmission systems without antenna selection."""
"""AI""","""This paper addresses the challenge of achieving high spectral efficiency in distributed antenna systems (DAS) through spatial multiplexing schemes like vertical Bell-Labs layered space-time (V-BLAST). Unlike traditional point-to-point multiple input multiple output (MIMO) channels, DAS channels exhibit varying large-scale fadings from different transmit antennas. This makes the equal power and rate transmission strategy that works in MIMO channels unsuitable for DAS channels. To address this, the paper introduces a unique transmit antenna selection approach that incorporates power and rate allocation. This scheme takes into account factors such as shadow fading and path loss to optimize the performance of V-BLAST structures with zero-forcing and successive interference cancellation (ZF-SIC) receivers, ensuring balanced average symbol error rate (SER) across each layer. The receiver side employs a fixed detection order determined during the transmit antenna selection process. Simulation results demonstrate that the proposed approach offers performance improvements compared to equal power and rate transmission systems without antenna selection."""
"""Human""","""This paper characterizes quality, budget, and demand as fuzzy variables in a fuzzy vendor selection expected value model and a fuzzy vendor selection chance-constrained programming model, to maximize the total quality level. The two models have distinct advantages over existing methods for selecting vendors in fuzzy environments. A genetic algorithm based on fuzzy simulations is designed to solve these two models. Numerical examples show the effectiveness of the algorithm."""
"""AI""","""In this study, a vendor selection process is approached by considering quality, budget, and demand as fuzzy variables within a framework of fuzzy vendor selection expected value model and a fuzzy vendor selection chance-constrained programming model. The goal is to maximize the overall quality level while accounting for uncertainties. These models offer unique advantages in handling vendor selection within fuzzy environments compared to existing methods. To solve these models, a genetic algorithm is devised based on fuzzy simulations. Numerical examples demonstrate the effectiveness of the proposed algorithm in tackling these vendor selection challenges."""
"""Human""","""The distributed system with high performance and stability is commonly adopted in large scale scientific and engineering computing. In this paper, we discuss a fault-tolerant mechanism under Linux circumstance to improve the fault-tolerant ability of the system, namely a scheme and frame to form the stable computing platform. In terms of the structure and function of the distributed system, active list and file invocation strategies are employed in the task management. System multilevel fault-tolerance can be achieved by repeated processes in a single node and task migration on multi-nodes. Manager node agent introduced in this paper administrates the nodes using the list, disposes of the tasks according to the nodes' performance, and hence, to be able to make full use of the cluster resources. An evaluation method is proposed to appraise the performance. The analyzed results show the usefulness of the scheme proposed except for some additional overhead of memory consumption."""
"""AI""","""This paper discusses the implementation of a fault-tolerant mechanism within a distributed system operating under a Linux environment, aimed at enhancing system stability and performance. The proposed mechanism focuses on creating a reliable computing platform by employing active lists and file invocation strategies for task management. The approach involves both single-node processes and multi-node task migration to achieve multilevel fault tolerance. A manager node agent oversees node administration, task allocation, and resource optimization. An evaluation method is introduced to assess system performance, and the results indicate the effectiveness of the proposed scheme, despite some added memory overhead."""
"""Human""","""A neuro-sliding-mode control (NSMC) strategy was developed to handle the complex nonlinear dynamics and model uncertainties of flexible-link manipulators. A composite controller was designed based on a singularly perturbed model of flexible-link manipulators when the rigid motion and flexible motion are decoupled. The NSMC is employed to control the slow subsystem to track a desired trajectory with a traditional sliding mode controller to stabilize the fast subsystem which represents the link vibrations. A stability analysis of the flexible modes is also given. Simulations confirm that the NSMC performs better than the traditional sliding-mode control for controlling flexible-link manipulators. The control strategy not only gives good tracking performance for the joint angle, but also effectively suppresses endpoint vibrations. The simulations also show that the control strategy has a strong self-adaptive ability for controlling manipulators with different parameters."""
"""AI""","""This study introduces a neuro-sliding-mode control (NSMC) approach to address the intricate nonlinear dynamics and uncertainties inherent in flexible-link manipulators. The approach employs a composite controller based on a singularly perturbed model that decouples rigid and flexible motions. NSMC is utilized to regulate the slow subsystem for desired trajectory tracking, while a conventional sliding mode controller stabilizes the fast subsystem dealing with link vibrations. The stability of flexible modes is analyzed, and simulations reveal that the NSMC outperforms traditional sliding-mode control for flexible-link manipulator control. The proposed strategy not only achieves accurate joint angle tracking but also effectively dampens endpoint vibrations. Furthermore, the approach demonstrates adaptability across manipulators with varying parameters."""
"""Human""","""Token protocol provides a new coherence framework for shared-memory multiprocessor systems. It avoids indirections of directory protocols for common cache-to-cache transfer misses, and achieves higher interconnect bandwidth and lower interconnect latency compared with snooping protocols. However, the broadcasting increases network traffic, limiting the scalability of token protocol. This paper describes an efficient technique to reduce the token protocol network traffic, called sharing relation cache. This cache provides destination set information for cache-to-cache miss requests by caching directory information for recent shared data. This paper introduces how to implement the technique in a token protocol. Simulations using SPLASH-2 benchmarks show that in a 16-core chip multiprocessor system, the cache reduced the network traffic by 15% on average."""
"""AI""","""The token protocol introduces a novel coherence framework for shared-memory multiprocessor systems, aiming to enhance interconnect bandwidth and minimize latency compared to snooping protocols. However, the inherent broadcasting in token protocols can limit scalability due to increased network traffic. This study presents an effective approach called sharing relation cache to mitigate token protocol network congestion. This cache stores directory information of recently shared data, providing destination set details for cache-to-cache miss requests. The paper outlines the integration of this technique into a token protocol and validates its efficiency using SPLASH-2 benchmarks. Simulation results demonstrate an average reduction of 15% in network traffic for a 16-core chip multiprocessor system."""
"""Human""","""An ultra-thin flexible eddy current proximity sensor array was developed for online measurements of tiny gaps between large smooth metallic and nonmetallic surfaces of arbitrary shapes. The probe of the flexible eddy current sensor array, which includes a set of sensor coils, is fabricated on a thin flexible substrate using the flexible printed circuit board process which allows the probe to be very thin and flexible so that it can conform to the surface geometry of the measured objects. The sensor coils are connected to an inductance-capacitance oscillator, which converts the distance between the sensor coil and the metallic target to a frequency output. Experimental results show that the measurement accuracy of the sensor system can reach ±0.5% for a 2-mm gap and the sensor system is suitable for online gap measurements."""
"""AI""","""This study introduces an innovative approach to online measurements of small gaps between smooth metallic and nonmetallic surfaces using an ultra-thin flexible eddy current proximity sensor array. The sensor array consists of coils placed on a thin, flexible substrate manufactured using the flexible printed circuit board process. This design enables the sensor to conform to various surface shapes. The sensor coils are connected to an inductance-capacitance oscillator that converts the distance between the coil and the metallic target into a frequency output. Experimental findings indicate high measurement accuracy with ±0.5% for a 2-mm gap, making this sensor system suitable for real-time gap measurements."""
"""Human""","""A method was developed to solve the combined system of the current field and the circuit. The “super-node” was used to transform the matrix for conventional nodal analyses of a circuit system from non-positive definite to positive definite. Then, a positive definite matrix for the overall system was obtained by combining the matrix from the circuit nodal analysis method and the matrix resulted from finite element method (FEM) formulation to solve the FEM fields. This approach has been successfully applied to simulate the electrical potential and current distributions on each metal layer of printed circuit boards (PCBs) and integrated circuit (IC) packages for a given power supply. The simulation results can then be used to analyze the properties of the PCBs and IC packages such as the port resistances and IR drops. The results can also be used to optimize PCB and IC package designs, such as by adjusting the power/ground distribution networks."""
"""AI""","""This paper presents a method to efficiently solve a combined system involving both the current field and the circuit. The method involves using a ""super-node"" transformation to convert the matrix for conventional nodal analysis of the circuit system from non-positive definite to positive definite. By combining the matrices from the circuit nodal analysis method and the matrices resulting from the finite element method (FEM) formulation used to solve the FEM fields, a positive definite matrix for the overall system is obtained. This approach is successfully applied to simulate the distribution of electrical potential and current across various metal layers of printed circuit boards (PCBs) and integrated circuit (IC) packages, considering a specific power supply configuration. The simulation results enable the analysis of properties like port resistances and IR drops in the PCBs and IC packages. Additionally, these results can aid in optimizing the design of PCBs and IC packages, such as by adjusting power/ground distribution networks."""
"""Human""","""The uncertainties of grid sites security are main hurdle to make the job scheduling secure, reliable and fault-tolerant. Most existing scheduling algorithms use fixed-number job replications to provide fault tolerant ability and high scheduling success rate, which consume excessive resources or can not provide sufficient fault tolerant functions when grid security conditions change. In this paper a fuzzy-logic-based selfadaptive replication scheduling (FSARS) algorithm is proposed to handle the fuzziness or uncertainties of job replication number which is highly related to trust factors behind grid sites and user jobs. Remote sensing-based soil moisture extraction (RSBSME) workload experiments in real grid environment are performed to evaluate the proposed approach and the results show that high scheduling success rate of up to 950/0 and less grid resource utilization can be achieved through FSARS. Extensive experiments show that FSARS scales well when user jobs and grid sites increase."""
"""AI""","""This paper addresses the challenge of ensuring secure, reliable, and fault-tolerant job scheduling in grid environments where uncertainties in grid site security are prevalent. Existing scheduling algorithms often use a fixed number of job replications to enhance fault tolerance, which may lead to resource inefficiencies or inadequate fault tolerance when security conditions change. To address this, the paper introduces a fuzzy-logic-based self-adaptive replication scheduling (FSARS) algorithm. FSARS adjusts the number of job replications based on the uncertainties in trust factors associated with grid sites and user jobs. Real-world experiments involving remote sensing-based soil moisture extraction (RSBSME) workload are conducted to evaluate the proposed FSARS algorithm. The results indicate that FSARS achieves a high scheduling success rate of up to 95%, while minimizing grid resource utilization. The algorithm's scalability is demonstrated through extensive experiments, highlighting its effectiveness in handling increasing numbers of user jobs and grid sites."""
"""Human""","""Engineers tend to use different software to perform tasks such as geometry modeling, database management, numerical analysis, and visualization. This may cause decrease of productivity and loss of information during the conversion process between different data file formats. This paper presents a computer aided design (CAD) and computer aided engineering (CAE) system integration using scientific visualization tools and techniques. It deals with the development of a 3D CAD add-in for lighting analysis which uses the CAD model as 3D interface for creating a lighting scheme, processing, and visualizing 2D or 3D illuminance fields. Visualization features as color and contour mapping were developed using the visualization toolkit (VTK) toolkit. The application integrates all functionalities of the 3D CAD with tools for light sources database management, pre-processing, processing, and post-processing of illuminance fields in a single environment. This approach increases productivity and eliminates the need for different software."""
"""AI""","""This paper addresses the common issue of engineers using various software tools for different tasks, resulting in reduced productivity and information loss during data format conversions. The paper proposes a solution by integrating computer aided design (CAD) and computer aided engineering (CAE) systems through scientific visualization techniques. Specifically, the paper focuses on the development of a 3D CAD add-in for lighting analysis, leveraging the CAD model as a 3D interface for creating lighting schemes and visualizing illuminance fields. Visualization features such as color and contour mapping are implemented using the visualization toolkit (VTK). This integrated system combines 3D CAD functionalities with tools for light source management, pre-processing, processing, and post-processing of illuminance fields within a unified environment. This integration approach aims to enhance productivity and eliminate the need for switching between different software tools."""
"""Human""","""In this paper, we describe an image enhancement and interpretation methodology to enhance and recognize surface defects and critical patterns from remote imagery of sewer pipeline inspection. The objective is to provide inspectors and professionals with better tools to allow them to examine the imagery for condition assessment. We present initial results of a collaboration with a robotic company through a case study on computer-assisted processing and interpretation of sewer pipeline inspection imagery. In the mean time, the described enhancement and interpretation methodology can also be applied to sewer pipeline condition assessment in an offline mode, where this methodology can support professionals' examination of acquired sewer condition imagery"""
"""AI""","""This paper presents a methodology for enhancing and identifying surface defects and important patterns in remote imagery captured during sewer pipeline inspections. The aim is to offer inspectors and experts improved tools to analyze the imagery for assessing the condition of sewer pipelines. The study involves a collaboration with a robotic company and showcases the computer-assisted processing and interpretation of inspection imagery. The methodology introduced for enhancement and interpretation can also be employed offline, aiding professionals in reviewing sewer condition imagery for assessment purposes."""
"""Human""","""Recently, direct acquisition of GPS P-code has received considerable attention to enhance the anti-jamming and anti-spoofing capabilities of GPS receivers. This paper describes a P-code acquisition method that uses block searches with large-scale FFT to search code phases and carrier frequency offsets in parallel. To limit memory use, especially when implemented in hardware, only the largest correlation result with its position information was preserved after searching a block of resolution cells in both the time and frequency domains. A second search was used to solve the code phase slip problem induced by the code frequency offset. Simulation results demonstrate that the probability of detection is above 0.99 for carrier-to-noise density ratios in excess of 40 dB· Hz when the predetection integration time is 0.8 ms and 6 non-coherent integrations are used in the analysis."""
"""AI""","""This paper presents a method for acquiring GPS P-code directly to improve the resistance against jamming and spoofing in GPS receivers. The approach involves block searches using a large-scale Fast Fourier Transform (FFT) to simultaneously search code phases and carrier frequency offsets. To manage memory usage, only the highest correlation result along with its position information is retained after searching a block of resolution cells in both time and frequency domains. A subsequent search addresses the issue of code phase slip caused by code frequency offset. Simulation outcomes indicate that with a predetection integration time of 0.8 ms and 6 non-coherent integrations, the probability of detection surpasses 0.99 for carrier-to-noise density ratios greater than 40 dB·Hz."""
"""Human""","""A simple, efficient multiple description coding (MDC) algorithm was developed based on weighted signal combinations. The scheme uses the standard video encoder with a pre-processing stage to generate multiple descriptions. The decoder then uses a post-processing algorithm to combine the descriptions to provide better image quality. A scalable codec in the MDC system allows the system to provide multiple descriptions and scalability at the same time. In addition, since the different scalable descriptions may have different qualities, a simple averaging process is not optimal. An optimal weighted combination of the two descriptions was developed based on the signal to noise ratios. Compared with the simple average combination, the algorithm significantly improved the video quality, especially with large quality differences between the two descriptions, with gains of up to 3.56 dB."""
"""AI""","""An efficient multiple description coding (MDC) algorithm is introduced, leveraging weighted signal combinations to enhance its performance. The approach involves using a standard video encoder to create multiple descriptions, and a post-processing algorithm is applied at the decoder to combine these descriptions for improved image quality. The MDC system includes scalability and multiple descriptions simultaneously, enabling greater flexibility. Given that scalable descriptions can have varying qualities, a straightforward averaging method is suboptimal. Instead, an optimal weighted combination is developed using signal-to-noise ratios. Compared to simple averaging, this algorithm notably enhances video quality, especially when there are significant quality differences between descriptions, achieving gains of up to 3.56 dB."""
"""Human""","""In this paper a new low noise amplifier configuration is proposed to achieve wideband operation. This configuration consists of an LC-ladder filter and a common-emitter stage employing shunt-shunt capacitive feedback to realize wideband matching. Design equations for this configuration are derived, as well as equations for the important performance measures namely noise figure, gain and IIP3. The results of a design for achieving typical low noise amplifier specifications in the ultra-wideband are calculated from these equations and plotted. Without any optimization S11 of less than −10 dB over the entire frequency band and a minimum noise figure of 2.7 dB are predicted when achieving S21 of 20 dB. These results indicate that very good performance can be attained through the use of this technique. Simulations were also done to verify the calculated results."""
"""AI""","""This paper introduces a novel configuration for a low noise amplifier (LNA) that aims to provide wideband operation. The proposed design combines an LC-ladder filter and a common-emitter stage using capacitive feedback to achieve broad matching. The paper derives design equations for this configuration, including calculations for key performance metrics such as noise figure, gain, and third-order intercept point (IIP3). Utilizing these equations, the authors calculate and plot results for a design that meets typical LNA specifications in the ultra-wideband range. The predictions show that without optimization, the proposed design could achieve an S11 of less than -10 dB across the entire frequency range, a minimum noise figure of 2.7 dB, and an S21 of 20 dB. Simulation results also confirm the accuracy of the calculated outcomes, indicating the potential for excellent performance using this technique."""
"""Human""","""The use of communication networks in control loops has gained increasing attention in recent years due to its advantages and flexible applications. The network quality-of-service (QoS) in those so-called networked control systems always fluctuates due to changes of the traffic load and available network resources. This paper presents an intelligent scheduling controller design approach for a class of NCSs to handle network QoS variations. The sampling period and control parameters in the controller are simultaneously scheduled to compensate for the network QoS variations. The estimation of distribution algorithm is used to optimize the sampling period and control parameters for better performance. Compared with existing networked control methods, the controller has better ability to compensate for the network QoS variations and to balance network loads. Simulation results show that the plant setting time with the intelligent scheduling controller is reduced by about 64.0% for the medium network load and 49.1% for high network load and demonstrate the effectiveness of the proposed approaches."""
"""AI""","""This paper addresses the utilization of communication networks in control loops, particularly in networked control systems (NCSs), which have gained attention due to their flexibility and advantages. The variable quality-of-service (QoS) of these networks, influenced by traffic load and available resources, poses challenges. The paper proposes an intelligent scheduling controller design for NCSs to manage fluctuations in network QoS. The controller adjusts both sampling intervals and control parameters to compensate for changes in network conditions. An estimation of distribution algorithm optimizes these aspects for improved performance. Compared to existing methods, the proposed controller better handles network QoS variations and balances network loads. Simulations show that the intelligent scheduling controller significantly reduces plant setting time, offering potential benefits for both medium and high network loads. This research demonstrates the effectiveness of the presented approaches."""
"""Human""","""In order to overcome the drawbacks of 3RRR non-redundant parallel manipulators, a redundantly actuated planar parallel manipulator, the 4RRR manipulator, was examined. In the current study, three types of workspace were analyzed. In the analysis of the reachable workspace, the shape of the workspace of 4RRR PMs was illustrated, and the relationship between the parameters of parallel mechanisms (PMs) and this kind of workspace was discussed. In the analysis of the m-orientation workspace, a procedure for calculating this type of workspace was presented, and the relationship between this type of workspace and the requirement of rotational displacement was revealed. In the analysis of the nonsingular workspace, the singularity of 4RRR PMs was discussed, the boundary of the singularity was illustrated, and a scheme to maximize the nonsingular workspace was presented. Depicting the properties of 4RRR PMs from different perspectives, the analyses of these three kinds of workspace can serve as helpful references for the structure design and mechanism control of 4RRR PMs."""
"""AI""","""This study focuses on addressing the limitations of non-redundant 3RRR parallel manipulators by examining a redundantly actuated planar parallel manipulator known as the 4RRR manipulator. Three types of workspaces associated with this manipulator were analyzed. The reachable workspace's shape for 4RRR manipulators was depicted, and its relationship with parallel mechanism parameters was discussed. The m-orientation workspace was evaluated by proposing a calculation method and revealing its connection to rotational displacement requirements. The nonsingular workspace was investigated in terms of singularity, its boundary was illustrated, and a strategy to maximize this workspace was introduced. By exploring the characteristics of 4RRR manipulators across these workspace perspectives, these analyses can offer valuable insights for the design and control of such mechanisms."""
"""Human""","""To investigate quantum confinement effects on silicon (Si) light source electroluminescence (EL), nanometre-scale Si finger junctions were manufactured in a fully customized silicon-on-insulator (SOI) production technology. The wafers were manufactured in the cleanroom using an electron-beam pattern generator (EPG). The SOI light source with the highest irradiance emitted about 9 times more optical power around λ = 850 nm than a 0.35 μm bulk-CMOS avalanche light-source operating at the same current. It is shown that the buried oxide (BOX) layer in a SOI process could be used to reflect about 25 % of otherwise lost downward-radiated light back up to increase the external power efficiency of SOI light sources."""
"""AI""","""This research explores the impact of quantum confinement on electroluminescence (EL) in silicon (Si) light sources. Nanoscale Si finger junctions were created using a specialized silicon-on-insulator (SOI) production technique. These SOI wafers were fabricated in a cleanroom using electron-beam pattern generation. The highest-performing SOI light source emitted approximately 9 times more optical power around a wavelength (λ) of 850 nm compared to a 0.35 μm bulk-CMOS avalanche light-source operating at the same current. The study demonstrates that the buried oxide (BOX) layer within the SOI process can act as a reflector, redirecting around 25% of downward-radiated light back upwards. This reflective effect contributes to enhancing the external power efficiency of SOI light sources."""
"""Human""","""A synchronous grammar based on the formalism of context-free grammar was developed by generalizing the first component of production that models the source text. Unlike other synchronous grammars, the grammar allows multiple target productions to be associated to a single production rule which can be used to guide a parser to infer different possible translational equivalences for a recognized input string according to the feature constraints of symbols in the pattern. An extended generalized LR algorithm was adapted to the parsing of the proposed formalism to analyze the syntactic structure of a language. The grammar was used as the basis for building a machine translation system for Portuguese to Chinese translation. The empirical results show that the grammar is more expressive when modeling the translational equivalences of parallel texts for machine translation and grammar rewriting applications."""
"""AI""","""A novel synchronous grammar was developed using the framework of context-free grammar, extending the first component of production responsible for modeling the source text. This unique grammar allows multiple target productions to be associated with a single production rule. This feature guides a parser in determining various potential translational equivalences for a recognized input string, considering the constraints of symbols in the pattern. The extended generalized LR algorithm was modified for parsing this formalism to analyze the syntactic structure of a language. The grammar formed the foundation of a machine translation system from Portuguese to Chinese, and empirical results demonstrated its enhanced expressiveness in modeling translational equivalences, benefiting applications like machine translation and grammar rewriting."""
"""Human""","""In wireless sensor networks (WSNs), a faulty sensor may produce incorrect data and transmit them to the other sensors. This would consume the limited energy and bandwidth of WSNs. Furthermore, the base station may make inappropriate decisions when it receives the incorrect data sent by the faulty sensors. To solve these problems, this paper develops an online distributed algorithm to detect such faults by exploring the weighted majority vote scheme. Considering the spatial correlations in WSNs, a faulty sensor can diagnose itself through utilizing the spatial and time information provided by its neighbor sensors. Simulation results show that even when as many as 30% of the sensors are faulty, over 95% of faults can be correctly detected with our algorithm. These results indicate that the proposed algorithm has excellent performance in detecting fault of sensor measurements in WSNs."""
"""AI""","""In wireless sensor networks (WSNs), the presence of faulty sensors transmitting incorrect data can lead to energy and bandwidth wastage and erroneous decisions at the base station. To address this issue, this paper presents an online distributed algorithm that utilizes a weighted majority vote approach to identify such faults. By accounting for spatial correlations within the WSN, a faulty sensor can self-diagnose using spatial and temporal information from neighboring sensors. Simulation outcomes demonstrate that even when up to 30% of sensors are faulty, the algorithm achieves over 95% accuracy in detecting faults. These results underline the algorithm's effectiveness in identifying faulty sensor measurements within WSNs."""
"""Human""","""This paper presents a digital readout system to detect small capacitive signals of a micromachined angular rate sensor. The flexible parameter adjustment ability and the computation speed of the digital signal processor were used to develop a new calibration procedure to determine the scale factor of a gyroscope without a turntable. The force of gravity was used to deflect the movable masses in the sensor, which resulted in a corresponding angular rate input. The gyroscope scale factor was then measured without a turntable. Test results show a maximum deviation of about 1.20/0 with respect to the scale factor determined on a turntable with the accuracy independent of the manufacturing process and property variations. The calibration method in combination with the improved readout electronics can minimize the calibration procedure and, thus, reduce the manufacturing costs."""
"""AI""","""This paper introduces a digital readout system designed for detecting small capacitive signals in a micromachined angular rate sensor. By leveraging the digital signal processor's flexible parameter adjustment and fast computation capabilities, a novel calibration procedure is developed to determine the gyroscope's scale factor without relying on a turntable. The sensor's movable masses are deflected by gravity, producing a corresponding angular rate input for scale factor measurement. Test results indicate a maximum deviation of approximately 1.20% from the scale factor determined using a turntable, with consistent accuracy across different manufacturing processes and property variations. This combined calibration method and enhanced readout electronics offer the advantage of reducing manufacturing costs through a streamlined calibration procedure."""
"""Human""","""The objective of this paper is to introduce three semi-automated approaches for ontology mapping using relatedness analysis techniques. In the architecture, engineering, and construction (AEC) industry, there exist a number of ontological standards to describe the semantics of building models. Although the standards share similar scopes of interest, the task of comparing and mapping concepts among standards is challenging due to their differences in terminologies and perspectives. Ontology mapping is therefore necessary to achieve information interoperability, which allows two or more information sources to exchange data and to re-use the data for further purposes. The attribute-based approach, corpus-based approach, and name-based approach presented in this paper adopt the statistical relatedness analysis techniques to discover related concepts from heterogeneous ontologies. A pilot study is conducted on IFC and CIS/2 ontologies to evaluate the approaches. Preliminary results show that the attribute-based approach outperforms the other two approaches in terms of precision and F-measure."""
"""AI""","""The objective of this paper is to introduce three semi-automated approaches for ontology mapping using relatedness analysis techniques. In the architecture, engineering, and construction (AEC) industry, there exist a number of ontological standards to describe the semantics of building models. Although the standards share similar scopes of interest, the task of comparing and mapping concepts among standards is challenging due to their differences in terminologies and perspectives. Ontology mapping is therefore necessary to achieve information interoperability, which allows two or more information sources to exchange data and to re-use the data for further purposes. The attribute-based approach, corpus-based approach, and name-based approach presented in this paper adopt the statistical relatedness analysis techniques to discover related concepts from heterogeneous ontologies. A pilot study is conducted on IFC and CIS/2 ontologies to evaluate the approaches. Preliminary results show that the attribute-based approach outperforms the other two approaches in terms of precision and F-measure."""
"""Human""","""For text-independent speaker verification, the Gaussian mixture model (GMM) using a universal background model strategy and the GMM using support vector machines are the two most commonly used methodologies. Recently, a new SVM-based speaker verification method using GMM super vectors has been proposed. This paper describes the construction of a new speaker verification system and investigates the use of nuisance attribute projection and test normalization to further enhance performance. Experiments were conducted on the core test of the 2006 NIST speaker recognition evaluation corpus. The experimental results indicate that an SVM-based speaker verification system using GMM super vectors can achieve appealing performance. With the use of nuisance attribute projection and test normalization, the system performance can be significantly improved, with improvements in the equal error rate from 7.780/0 to 4.920/0 and detection cost function from 0.0376 to 0.0251."""
"""AI""","""This paper discusses text-independent speaker verification techniques, focusing on Gaussian mixture model (GMM)-based approaches and support vector machines (SVM). A recent method using SVM and GMM super vectors is introduced, and the study aims to enhance the system's performance by implementing nuisance attribute projection and test normalization. The experiments are conducted on the 2006 NIST speaker recognition evaluation corpus, revealing that the SVM-based speaker verification system employing GMM super vectors can achieve favorable performance. The inclusion of nuisance attribute projection and test normalization leads to significant improvements, with the equal error rate reducing from 7.78% to 4.92% and the detection cost function improving from 0.0376 to 0.0251."""
"""Human""","""The session initiation protocol (SIP) is a signaling protocol for Internet telephony, multimedia conferencing, presence, event notification, and instant messaging. With the gaining popularity, more and more SIP implementations have been developed and deployed. How to guarantee the conformance of those SIP implementations is the key point of interconnection and interoperation among them. This paper proposes the test method and architecture for the SIP protocol based on the IPv6 tester system. Tree and tabular combined notation (TTCN-2) is adopted to describe the test suite. With an enhanced reference implementation, the data-processing ability to the original test system has been greatly improved. In the following test practices, some errors have been found in the SIP entities under test. It is proved that the proposed test method and architecture are effective to verify the conformance of the SIP entities, and the result of the conformance test may provide helpful reference to the development of SIP products."""
"""AI""","""The session initiation protocol (SIP) serves as a signaling protocol for various communication tasks such as Internet telephony, multimedia conferencing, and instant messaging. As SIP implementations become more widespread, ensuring their adherence to standards becomes crucial for effective interconnection and interoperation. This paper presents a method and architecture for testing the SIP protocol using an IPv6 tester system. The test suite is described using the tree and tabular combined notation (TTCN-2). An enhanced reference implementation has improved data-processing capabilities in the test system. Through testing, errors were detected in the SIP entities being examined. This indicates the effectiveness of the proposed method and architecture in confirming the conformity of SIP entities, with the test results aiding in the development of SIP products."""
"""Human""","""Traditionally, timing and the failure rate are the only two factors considered in software reliability formula, which is actually incomplete. Reliability should be redefined as a function of software complexity, test effectiveness, and operating environment. This paper focuses on software complexity with its relation to the software reliability. Today, many software complexity measurements have been proposed, but most of them treat the reliability model incompletely. This paper proposes a new method which considers a relatively complete view of software reliability including its complexity and test effectiveness of the software being tested."""
"""AI""","""Traditionally, software reliability has been evaluated based solely on timing and failure rate, which presents an incomplete picture. A more comprehensive approach to defining software reliability should take into account factors like software complexity, test effectiveness, and the operating environment. This paper shifts its focus to software complexity and its connection to software reliability. Although numerous software complexity metrics have been introduced, most of them do not provide a holistic reliability model. This paper introduces a novel approach that considers a more comprehensive perspective of software reliability, encompassing its complexity as well as the effectiveness of testing applied to the software under examination."""
"""Human""","""Based on building footprints (building polygons) on digital maps, we are proposing the GIS and CG integrated system that automatically generates 3D building models with multiple roofs. Most building polygons' edges meet at right angles (orthogonal polygon). The integrated system partitions orthogonal building polygons into a set of rectangles and places rectangular roofs and box-shaped building bodies on these rectangles. In order to partition an orthogonal polygon, we proposed a useful polygon expression in deciding from which vertex a dividing line is drawn. In this paper, we propose a new scheme for partitioning building polygons and show the process of creating 3D roof models."""
"""AI""","""This paper introduces a novel approach for creating 3D building models with multiple roofs by utilizing building footprints from digital maps. The proposed system integrates Geographic Information Systems (GIS) and Computer Graphics (CG) techniques to automate the generation of these models. The system specifically targets buildings with orthogonal polygons, where edges meet at right angles. It achieves this by partitioning these polygons into sets of rectangles and then placing rectangular roofs and box-shaped building bodies on these partitioned rectangles. The key aspect of partitioning orthogonal polygons involves determining the appropriate vertex from which a dividing line should be drawn. The paper presents a method for partitioning building polygons and outlines the entire process of generating 3D roof models using this approach."""
"""Human""","""As the core algorithm and the most time consuming part of almost every modern network intrusion management system (NIMS), string matching is essential for the inspection of network flows at the line speed. This paper presents a memory and time efficient string matching algorithm specifically designed for NIMS on commodity processors. Modifications of the Aho-Corasick (AC) algorithm based on the distribution characteristics of NIMS patterns drastically reduce the memory usage without sacrificing speed in software implementations. In tests on the Snort pattern set and traces that represent typical NIMS workloads, the Snort performance was enhanced 1.480/0–200/0 compared to other well-known alternatives with an automaton size reduction of 4.86–6.11 compared to the standard AC implementation. The results show that special characteristics of the NIMS can be used into a very effective method to optimize the algorithm design."""
"""AI""","""This paper addresses the critical role of string matching in network intrusion management systems (NIMS) and presents an efficient algorithm tailored for NIMS on commodity processors. String matching is a time-intensive process in NIMS, required for inspecting network flows at high speeds. The paper introduces an optimized version of the Aho-Corasick (AC) algorithm, customized to the distribution characteristics of NIMS patterns. By making these modifications, the algorithm significantly reduces memory usage while maintaining high processing speed. The proposed algorithm was tested using the Snort pattern set and typical NIMS workloads, showing a remarkable performance enhancement of 148-200% compared to well-known alternatives. Additionally, the algorithm achieves a 4.86-6.11 reduction in automaton size when compared to the standard AC implementation. This research demonstrates how tailoring algorithms to the unique attributes of NIMS can lead to highly effective optimizations."""
"""Human""","""This paper undertakes the performance evaluation and comparison of different phase shift keying (PSK) modulation schemes in aeronautical channels. Specifically, bit error rate (BER) curves and outage probabilities of various modulation schemes in high dynamic environments are investigated by means of numerical analysis and computer simulation. Simulation results show that the performance of all modulation schemes degrades with the increase of the power ratio or dynamic factor, and that differential quadrature phase shift keying (DQPSK), differential 8 phase shift keying (D8PSK), 16 and 64 differential amplitude and phase shift keying (DAPSK) perform better than other same order modulation schemes, respectively. As the conclusion, the differential modulation schemes are more suitable for the aeronautical channels in order to withstand the Doppler shift and the changing rate of the Doppler."""
"""AI""","""This paper conducts a comprehensive performance evaluation and comparison of various phase shift keying (PSK) modulation schemes within aeronautical channels. The study focuses on assessing the bit error rate (BER) curves and outage probabilities of different modulation schemes, particularly in environments with high dynamics. The analysis is carried out using a combination of numerical calculations and computer simulations. The simulation outcomes highlight several important findings: the performance of all modulation schemes deteriorates as the power ratio or dynamic factor increases, and among the same-order modulation schemes, differential quadrature phase shift keying (DQPSK), differential 8 phase shift keying (D8PSK), as well as 16 and 64 differential amplitude and phase shift keying (DAPSK) exhibit superior performance. Consequently, the differential modulation schemes, which include the aforementioned ones, are found to be more suitable for aeronautical channels due to their ability to withstand the effects of Doppler shift and rapid changes in Doppler rate."""
"""Human""","""Based on the new concept of thermal mass which refers to the equivalent mass of the thermal energy in an object according to Einstein's mass-energy relation, the kinetic energy and potential energy of thermal mass as well as its dissipation were introduced to establish the Lagrange equations in heat conduction. The results show that Fourier's law is equivalent to the Lagrange equations with negligible inertia forces. The application of Lagrange equations in heat conduction makes it possible to unify the treatment of heat transfer and mechanics as well as electrics through the method of analytical mechanics."""
"""AI""","""This study introduces a novel concept called ""thermal mass,"" which quantifies the equivalent mass of thermal energy within an object based on Einstein's mass-energy equivalence principle. The concept of thermal mass leads to the introduction of kinetic energy and potential energy associated with thermal mass, along with considerations of energy dissipation. By utilizing these concepts, the Lagrange equations are established for the domain of heat conduction. The analysis reveals that Fourier's law, a fundamental principle in heat conduction, can be represented as the Lagrange equations when the influence of inertia forces is negligible. This application of Lagrange equations in the realm of heat conduction presents an opportunity to unify the treatment of heat transfer, mechanics, and even electrical phenomena through the analytical mechanics approach."""
"""Human""","""To exert strong actuating power of piezoelectric stack and facilitate the incorporation of piezoelectric stack and host structure for vibration control application, a Π shape piezoelectric stack actuator (PISA) was designed. One piezoelectric stack and a Π shape metal pedestal were assembled together by a screw to build a PISA. Formula for calculating the actuating moment output of PISA was derived. Then PISA was applied in a vibration control system of a cantilever beam example. Thermal-elasticity analogy theory was applied in the modeling of the piezoelectric active vibration control system. Positive position feedback (PPF) control, linear quadric regulator (LQR) control, and neural network predictive (NNP) control strategies were adopted to perform the first bending mode vibration control of the beam. Simulation results indicate that with this new actuator and the PPF or LQR control strategy, the first bending mode amplitude of the cantilever beam can be reduced by about 60% or 79%while using the NNP control strategy the amplitude can be suppressed by more than 95%These results indicate that PISA can sever as a high efficiency actuator in structural vibration control application."""
"""AI""","""To enhance the actuating power of piezoelectric stacks and facilitate their integration into host structures for vibration control purposes, a novel Π-shaped piezoelectric stack actuator (PISA) was developed. The PISA involves the assembly of a single piezoelectric stack and a Π-shaped metal pedestal using screws. A formula was derived to calculate the actuating moment output of the PISA. This new actuator design was then applied in a vibration control system for a cantilever beam example. The modeling of the piezoelectric active vibration control system utilized the thermal-elasticity analogy theory. Three control strategies—positive position feedback (PPF), linear quadric regulator (LQR), and neural network predictive (NNP)—were employed to control the first bending mode vibration of the beam. Simulation results demonstrate that the introduction of the PISA, along with PPF or LQR control, can reduce the amplitude of the first bending mode by around 60% or 79%, respectively. Furthermore, the NNP control strategy achieved an impressive reduction of over 95% in amplitude. These findings highlight the potential of the PISA as an efficient actuator for structural vibration control applications."""
"""Human""","""The program slicing technique is employed to calculate the current values of the variables at some interest points in software test data generation. This paper introduces the concept of statement domination to represent the multiple nests, and presents a dynamic program slice algorithm based on forward analysis to generate dynamic slices. In the approach, more attention is given to the statement itself or its domination node, so computing program slices is more easy and accurate, especially for those programs with multiple nests. In addition, a case study is discussed to illustrate our algorithm. Experimental results show that the slicing technique can be used in software test data generation to enhance the effectiveness."""
"""AI""","""This paper explores the application of program slicing technique for calculating variable values at specific points of interest during software test data generation. It introduces the notion of ""statement domination"" to handle complex nested structures and presents a dynamic program slice algorithm that employs forward analysis to generate dynamic slices. This approach focuses on the statement itself or its domination node, which makes computing program slices easier and more accurate, particularly for programs with intricate nesting. The paper also includes a case study to demonstrate the algorithm's effectiveness. Experimental results indicate that using the slicing technique in software test data generation can enhance its efficacy."""
"""Human""","""In this paper, a series of fatigue tests were performed to study the effect of drilling process on fatigue behavior of open holes in dog-bone specimens drilled by the traditional drilling process and advanced Winslow drilling process. The evaluations of initial fatigue qualities of the two types of holes were carried out based on the equivalent initial flaw size method and coincidence criterion. Fatigue fracture surfaces were observed by stereomicroscope. The results show that the location where the fatigue crack initiates is unchanged when these two different drilling processes are used. A longer fatigue life shows a more severe dispersibility. When the Winslow drilling process is adopted, the equivalent initial flaw size is smaller than 0.125 mm. The Winslow drilling process meets the requirement of coincidence criterion."""
"""AI""","""This study focuses on investigating the impact of drilling processes on the fatigue behavior of open holes in dog-bone specimens. The traditional drilling process and the advanced Winslow drilling process were compared through a series of fatigue tests. To assess the initial fatigue qualities of the two types of holes, the equivalent initial flaw size method and coincidence criterion were employed for evaluation. Additionally, fatigue fracture surfaces were analyzed using a stereomicroscope. The findings indicate that the location of fatigue crack initiation remains consistent between the two drilling processes. A longer fatigue life corresponds to a more pronounced variability. The use of the Winslow drilling process results in an equivalent initial flaw size below 0.125 mm, satisfying the coincidence criterion."""
"""Human""","""This paper describes a parallel computing platform using the existing facilities for the digital watershed model. In this paper, distributed multi-layered structure is applied to the computer cluster system, and the MPI-2 is adopted as a mature parallel programming standard. An agent is introduced which makes it possible to be multi-level fault-tolerant in software development. The communication protocol based on checkpointing and rollback recovery mechanism can realize the transaction reprocessing. Compared with conventional platform, the new system is able to make better use of the computing resource. Experimental results show the speedup ratio of the platform is almost 4 times as that of the conventional one, which demonstrates the high efficiency and good performance of the new approach."""
"""AI""","""This paper presents a novel parallel computing platform designed for the digital watershed model by leveraging existing digital facilities. The platform employs a distributed multi-layered structure within a computer cluster system, utilizing the mature MPI-2 parallel programming standard. To enhance fault tolerance in software development, an agent is introduced, enabling multi-level fault tolerance. A communication protocol utilizing checkpointing and rollback recovery mechanisms allows for transaction reprocessing. When compared to conventional platforms, the new system effectively utilizes computing resources, leading to significantly improved performance. Experimental results demonstrate that the platform achieves a speedup ratio approximately four times greater than that of the conventional approach, showcasing the efficiency and effectiveness of the proposed methodology."""
"""Human""","""With hard turning, which is an attractive alternative to existing grinding processes, surface quality is of great importance. Signal processing techniques were used to relate workpiece surface topography to the dynamic behavior of the machine tool. Spatial domain frequency analyses based on fast Fourier transform were used to analyze the tool behavior. Wavelet reconstruction was used for profile filtering. The results show that machine vibration remarkably affects the surface topography at small feed rates, but has negligible effect at high feed rates. The analyses also show how to control the surface quality during hard turning."""
"""AI""","""Hard turning, emerging as a promising alternative to traditional grinding processes, places significant emphasis on achieving high-quality surface finishes. In this context, signal processing methods were employed to establish a connection between the surface topography of the workpiece and the dynamic characteristics of the machine tool. To gain insights into the behavior of the tool, spatial domain frequency analyses were conducted using fast Fourier transform techniques. Wavelet reconstruction was subsequently applied to filter profile data. The findings highlight that the vibration of the machine tool noticeably impacts the resulting surface topography at lower feed rates, while its influence becomes negligible at higher feed rates. Moreover, these analyses provide insights into strategies for maintaining and controlling surface quality in the context of hard turning operations."""
"""Human""","""The influence of different information sharing scenarios in a single supplier-single retailer supply chain is analyzed. The five information sharing scenarios are centralized information sharing, full information sharing, supplier-dominated information sharing, retailer-dominated information sharing, and noninformation sharing. Iterative procedures are developed to obtain the inventory policies and the system costs at equilibrium points. Numerical examples show that the cost of a centralized inventory system is about 20%–40% lower than that of a decentralized system with non-information sharing. Furthermore, a higher information sharing level does not always lead to a lower system cost in a decentralized supply chain due to inventory competition."""
"""AI""","""This study investigates the impact of various information sharing scenarios in a supply chain consisting of a single supplier and a single retailer. Five distinct information sharing approaches are examined: centralized information sharing, full information sharing, supplier-dominated information sharing, retailer-dominated information sharing, and non-information sharing. Through the use of iterative procedures, equilibrium points are identified to determine inventory policies and associated system costs. Numerical examples are presented to demonstrate the findings. Specifically, the research indicates that adopting a centralized inventory system can yield cost reductions of approximately 20% to 40% compared to a decentralized system lacking information sharing. Notably, it's observed that a higher level of information sharing doesn't always translate to lower system costs within a decentralized supply chain, due to the presence of inventory competition dynamics."""
"""Human""","""The demand for communications anywhere, anytime, and on any device without recording the date is growing with the development of modern information technologies. Current wireless communications enable people to freely exchange information, while web services provide loosely-coupled, language-neutral, and platform-independent ways of linking applications within organizations, across enterprises, and across the Internet. This paper describes a mobile information system for police service, which integrates wireless communication technologies and web service technologies. The system satisfies the special demands of police information systems, such as security and interoperability. Policemen in the system can use various kinds of mobile terminals, such as mobile phones, tablet PCs, personal digital assistants, and laptops equipped with wireless LAN or GPRS to access data in the central database. The system is based on web services with a secure web service architecture and simple object access protocol message passing."""
"""AI""","""The rise of modern information technologies has led to an increased demand for communication that is accessible anywhere, at any time, and through any device. Wireless communication capabilities have enabled seamless information exchange, while web services have facilitated the linkage of applications across different platforms, organizations, and the Internet. This paper introduces a mobile information system tailored for police services, which effectively combines wireless communication technologies with web service technologies. This system is designed to meet the specific requirements of police information systems, including security and interoperability. Within this system, police officers can utilize a range of mobile devices such as mobile phones, tablet PCs, personal digital assistants, and laptops equipped with wireless LAN or GPRS to access data stored in a centralized database. The foundation of the system relies on secure web services architecture and the use of the simple object access protocol for message transmission."""
"""Human""","""This study presents evidence from analyses of the acoustic parameters of fluent continuous speech to show that within-paragraph prosodic phrase boundaries are related more to contrasts of neighborhood prosodic states rather than between-phrase pause durations; prosodic states receive more constraints from higher level discourse information. by revising a modular acoustic model by Tseng's hierarchical prosodic phrase grouping framework and examining the much varied prosodic phrase (PPh) boundary B3 within speech paragraph, we show that statistical accounts of layered contributions reveal distinct contrasts between boundary immediate duration and intensity patterns irrespective of pause duration. Contrasts of FO contour patterns were also observed in these locations. Evidence was also obtained to illustrate how PPh boundary states are specified more by higher level discourse information than by lower level prosodic word construction. These combined results suggest that contrastive neighboring prosodic states are more significant cues to PPh boundaries than boundary pause duration. The results also help explain why in fluent speech between-phrase pause durations vary greatly, and can be applied to automatic speech segmentation."""
"""AI""","""This study presents evidence through analyses of acoustic parameters in fluent continuous speech, demonstrating that prosodic phrase boundaries within paragraphs are more influenced by contrasts in neighboring prosodic states rather than the duration of pauses between phrases. The study also reveals that prosodic states are shaped more significantly by higher-level discourse information, indicating a hierarchical relationship. To substantiate these findings, the study revises an acoustic model using Tseng's hierarchical prosodic phrase grouping framework. By examining the variations in prosodic phrase boundary B3 within speech paragraphs, the study shows that distinct contrasts emerge in boundary immediate duration and intensity patterns, regardless of pause duration. The study also observes contrasts in fundamental frequency (FO) contour patterns at these boundary locations. Additionally, the study provides evidence that higher-level discourse information has a more influential role in defining prosodic phrase boundary states than lower-level prosodic word construction. The combined results suggest that the cues for prosodic phrase boundaries are primarily derived from the contrasts between neighboring prosodic states rather than the duration of boundary pauses. These findings offer insights into the considerable variation in between-phrase pause durations observed in fluent speech and can be applied to the field of automatic speech segmentation."""
"""Human""","""The alignment between information systems (IS) and business strategy along with its implications for perceived IS effectiveness and business performance is an important question, which is rarely studied in China. Based on an empirical study, this paper summarized the significance of IS strategic alignment and its impact on business performance. This study also measured business strategy, information system strategy, and information system strategic alignment, built a conceptual model to describe the relationship between these factors, and investigated their implications for information system performance and business performance. A structural equation model was employed to test the conceptual model. Analyses of data gathered in a survey indicate that information system strategic alignment is a better predictor of business performance than business strategy or information system strategy alone, although business strategy can significantly influence business performance."""
"""AI""","""This paper addresses the often overlooked question of the alignment between information systems (IS) and business strategy and its implications for perceived IS effectiveness and business performance, particularly within the context of China. Through empirical research, the paper highlights the significance of IS strategic alignment and its influence on overall business performance. The study examines business strategy, information system strategy, and information system strategic alignment, and constructs a conceptual model to illustrate the relationships among these factors. Furthermore, the paper explores the impacts of these factors on information system performance and business performance. Using a structural equation model, the conceptual model is tested, and data from a survey is analyzed. The findings from the survey suggest that information system strategic alignment is a more effective predictor of business performance than business strategy or information system strategy alone, although business strategy still has a significant influence on business performance."""
"""Human""","""Modulations and diversities, including the Costas-ordered stepped-frequency and nonlinear stepped-frequency waveforms are widely used in linear frequency modulation (LFM) pulse trains to reduce the relatively high autocorrelation function (ACF) sidelobes. An efficient method was developed to optimize the interpulse frequency modulation to remove most of the ACF sidelobes about the mainlobe peak, with only a small increase in the mainlobe width. The genetic algorithm is used to solve the nonlinear optimization problem to find the interpulse frequency modulation sequence. The effects on the ACF sidelobes suppression and mainlobe widening are studied. The results show that the new design is superior to the corresponding stepped-frequency LFM signal and weighted stepped-frequency LFM signal in the terms of the ACF sidelobes reduction and mainlobe spread."""
"""AI""","""This paper delves into the utilization of modulations and diversities within linear frequency modulation (LFM) pulse trains, with a focus on mitigating the relatively high sidelobes in the autocorrelation function (ACF). Specifically, it explores the application of various techniques such as Costas-ordered stepped-frequency and nonlinear stepped-frequency waveforms. The paper introduces an efficient method for optimizing the interpulse frequency modulation to minimize the ACF sidelobes around the mainlobe peak, while only slightly increasing the mainlobe width. To achieve this, a genetic algorithm is employed to solve the nonlinear optimization problem and determine the interpulse frequency modulation sequence. The study examines the impacts on sidelobe suppression and mainlobe broadening. The outcomes indicate that this novel design outperforms traditional stepped-frequency LFM signals and weighted stepped-frequency LFM signals in terms of reducing ACF sidelobes and widening the mainlobe."""
"""Human""","""A representation schema called translation corresponding tree (TCT) has been applied to a Portuguese to Chinese example-based machine translation system. The translation examples are annotated by the representation of the TCT structure. Each TCT describes not only the syntactic structure of the source sentence (i.e., Portuguese in our system) but also the translation correspondences (i.e., Chinese translation). In addition, the TCT nodes describe the corresponding linguistic relationships between the source and target languages. The translation examples can be effectively represented with this annotation schema and organized in the bilingual knowledge database or example base. In the real machine translation process, the target language is synthesized with higher quality by referring to the TCT translation information."""
"""AI""","""In the context of a Portuguese to Chinese example-based machine translation system, a representation schema called ""translation corresponding tree"" (TCT) has been employed. This schema is used to annotate translation examples in the system. Each TCT not only captures the syntactic structure of the source sentence (Portuguese) but also includes information about the corresponding translation (Chinese). Additionally, the TCT nodes encode the linguistic relationships between the source and target languages. This approach effectively represents translation examples and organizes them in a bilingual knowledge database or example base. During the actual machine translation process, the TCT translation information is utilized to synthesize the target language (Chinese) with higher quality."""
"""Human""","“The alignment between information systems (IS) and business strategy along with its implications for perceived IS effectiveness and business performance is an important question, which is rarely studied in China. Based on an empirical study, this paper summarized the significance of IS strategic alignment and its impact on business performance. This study also measured business strategy, information system strategy, and information system strategic alignment, built a conceptual model to describe the relationship between these factors, and investigated their implications for information system performance and business performance. A structural equation model was employed to test the conceptual model. Analyses of data gathered in a survey indicate that information system strategic alignment is a better predictor of business performance than business strategy or information system strategy alone, although business strategy can significantly influence business performance”"
"""AI""","“The alignment between information systems (IS) and business strategy, and its potential impact on perceived IS effectiveness and business performance, represents a significant and relatively unexplored area of inquiry, particularly within the context of China. This paper draws upon an empirical study to highlight the importance of achieving IS strategic alignment and its potential consequences for business performance. The research effort involves the measurement of business strategy, information system strategy, and the degree of alignment between the two. A conceptual model is then formulated to illustrate the interconnectedness of these factors, and their potential effects on both information system performance and business performance. To empirically assess the model, a structural equation model is employed. The findings derived from data collected through a survey suggest that the degree of information system strategic alignment serves as a more robust predictor of business performance when compared to either business strategy or information system strategy in isolation. Nevertheless, it's important to note that business strategy does have a significant impact on business performance, albeit to a somewhat lesser extent than information system strategic alignment.”"
"""Human""","“Modulations and diversities, including the Costas-ordered stepped-frequency and nonlinear stepped-frequency waveforms are widely used in linear frequency modulation (LFM) pulse trains to reduce the relatively high autocorrelation function (ACF) sidelobes. An efficient method was developed to optimize the interpulse frequency modulation to remove most of the ACF sidelobes about the mainlobe peak, with only a small increase in the mainlobe width. The genetic algorithm is used to solve the nonlinear optimization problem to find the interpulse frequency modulation sequence. The effects on the ACF sidelobes suppression and mainlobe widening are studied. The results show that the new design is superior to the corresponding stepped-frequency LFM signal and weighted stepped-frequency LFM signal in the terms of the ACF sidelobes reduction and mainlobe spread.”"
"""AI""","“This study investigates the application of modulations and diversities, such as the Costas-ordered stepped-frequency and nonlinear stepped-frequency waveforms, in linear frequency modulation (LFM) pulse trains to mitigate the relatively high autocorrelation function (ACF) sidelobes. An innovative approach was devised to optimize interpulse frequency modulation, effectively mitigating most ACF sidelobes around the mainlobe peak while only slightly increasing the mainlobe width. Employing a genetic algorithm, the nonlinear optimization problem was solved to determine the interpulse frequency modulation sequence. The study delves into the impact on ACF sidelobe suppression and mainlobe broadening, revealing that the proposed design outperforms both the corresponding stepped-frequency LFM signal and the weighted stepped-frequency LFM signal in terms of reducing ACF sidelobes and extending the mainlobe spread. These results underscore the effectiveness of the new design approach for enhancing the performance of LFM pulse trains in various applications.”"
"""Human""","“A representation schema called translation corresponding tree (TCT) has been applied to a Portuguese to Chinese example-based machine translation system. The translation examples are annotated by the representation of the TCT structure. Each TCT describes not only the syntactic structure of the source sentence (i.e., Portuguese in our system) but also the translation correspondences (i.e., Chinese translation). In addition, the TCT nodes describe the corresponding linguistic relationships between the source and target languages. The translation examples can be effectively represented with this annotation schema and organized in the bilingual knowledge database or example base. In the real machine translation process, the target language is synthesized with higher quality by referring to the TCT translation information.”"
"""AI""","“This paper introduces the application of a novel representation schema, known as the translation corresponding tree (TCT), within a Portuguese to Chinese example-based machine translation system. The TCT structure is used to annotate translation examples, encompassing both the syntactic arrangement of the source sentence (Portuguese) and the associated translation correspondences (Chinese). The TCT nodes further capture the linguistic relationships between the source and target languages. Leveraging this annotation schema, translation examples are efficiently organized within a bilingual knowledge database or example base. In practical machine translation processes, the TCT translation information enhances the synthesis of the target language, resulting in higher translation quality and accuracy.”"
"""Human""","“This paper represents a design and development of a mobile sensing unit as well as its prototype implementation for railway track monitoring. The unit consists of an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer and an ADC (Analog/Digital Converter) so that the unit can trace the route while capturing an acceleration response of a passenger vehicle. The unit enables more frequent and qualitative data acquisition compared with traditional and the state of the practice railway track inspection equipments. Locating disorder is the key of our unit, which has a reasonable accuracy of positioning with GPS data, existing facilities landmarks, and car acceleration responses. The proposed unit is a promising device for efficient properties management of railway agencies. The prototype implementation shows a result that car acceleration responses are related with the track displacements in low frequencies. The results also imply that sensor settlement on a vehicle floor, not axes or bogies, is effective for capturing track vertical displacements.”"
"""AI""","“This paper presents the design, development, and prototype implementation of a mobile sensing unit for railway track monitoring. The unit integrates an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer, and an Analog/Digital Converter (ADC) to track the route and capture acceleration responses of passenger vehicles. This approach enables more frequent and higher-quality data acquisition compared to traditional railway track inspection equipment. The unit's key feature is precise location determination, achieved through a combination of GPS data, landmarks, and car acceleration responses. This innovative unit holds promise for efficient railway property management. Prototype implementation results indicate a correlation between car acceleration responses and track displacements in low frequencies, suggesting that sensor placement on the vehicle floor is effective for capturing vertical track displacements, particularly in comparison to axes or bogies.”"
"""Human""","“The theory of complex networks was used to classify public transport networks into public transportation route networks, public transportation transfer networks, and bus station networks. The practical significance of the network parameters was then analyzed. The public transport networks in Langfang, Jining, and Dalian were then chosen as specific research cases. The results show that the public transportation networks have the characteristics of complex networks. In addition, the urban transportation network parameters all significantly affect the accessibility, convenience, and terrorist security capability of the urban public transportation network. The results link the findings with the actual situations to explore means to solve transportation system problems.”"
"""AI""","“This study applies the theory of complex networks to categorize public transport systems into three distinct types: public transportation route networks, public transportation transfer networks, and bus station networks. The research further examines the practical implications of network parameters. By focusing on the public transport networks of Langfang, Jining, and Dalian, the analysis reveals the inherent complex network characteristics of these systems. Additionally, the study establishes a significant relationship between urban transportation network parameters and factors such as accessibility, convenience, and the potential for security concerns within the urban public transportation network. These findings bridge theory with real-world scenarios, offering insights into potential solutions for transportation system challenges.”"
"""Human""","“Model order reduction of interconnect circuits is an important technique to reduce the circuit complexity and improve the efficiency of post-layout verification process in the nanometer VLSI design. Existing works using the Krylov subspace method are very efficient, but the resulting models are less compact and lack global accuracy. Also, existing methods cannot handle interconnect circuits with large input and output ports. Recent advances in reduction techniques using non-Krylov subspace techniques such as truncated balanced realization (TBR) hold some promise to solve these problems. In this paper, we first review the classic TBR-based reduction methods and then present the recent developments in fast TBR-based reduction and techniques such as PMTBR, SBPOR, and ETBR methods. These newly proposed methods try to avoid the expensive computing steps in traditional TBR methods at some cost to accuracy to boost efficiency and scalability, which is critical to reduce large interconnect parasitics modeled as RLCK circuits. The ETBR method can also reduce circuits with massive ports by considering the input signals. We show the pros and cons of each method and compare them on a set of large interconnect circuits, and finally point to some new research directions for this area”"
"""AI""","“This paper addresses the essential task of model order reduction for interconnect circuits in nanometer VLSI design, aiming to enhance efficiency in post-layout verification and reduce circuit complexity. While existing Krylov subspace methods are efficient, they often yield less compact models lacking global accuracy, particularly for circuits with extensive input and output ports. Recent advancements in non-Krylov subspace techniques, such as truncated balanced realization (TBR), offer potential solutions. This study reviews classic TBR-based reduction methods and introduces recent developments, including fast TBR-based reduction and techniques like PMTBR, SBPOR, and ETBR. These newer approaches seek to maintain efficiency and scalability by mitigating the computational demands of traditional TBR methods, albeit with some trade-offs in accuracy. This is particularly valuable for handling large interconnect parasitics modeled as RLCK circuits. Notably, the ETBR method accommodates circuits with numerous ports by considering input signals. Comparative analysis on sizeable interconnect circuits highlights the strengths and limitations of each approach. This paper concludes by pointing towards emerging research directions in this field.”"
"""Human""","“Circular self test path (CSTP) is an attractive technique for testing digital integrated circuits(IC) in the nanometer era, because it can easily provide at-speed test with small test data volume and short test application time. However, CSTP cannot reliably attain high fault coverage because of difficulty of testing random-pattern-resistant faults. This paper presents a deterministic CSTP (DCSTP) structure that consists of a DCSTP chain and jumping logic, to attain high fault coverage with low area overhead. Experimental results on ISCAS'89 benchmarks show that 100% fault coverage can be obtained with low area overhead and CPU time, especially for large circuits.”"
"""AI""","“This paper introduces a deterministic Circular Self Test Path (DCSTP) structure as an enhanced technique for testing nanometer-era digital integrated circuits (ICs). The DCSTP employs a combination of a DCSTP chain and jumping logic to address the challenge of achieving high fault coverage, particularly for random-pattern-resistant faults. This approach offers advantages in terms of fault coverage, area overhead, and CPU time. Experimental results conducted on ISCAS'89 benchmarks demonstrate the effectiveness of the DCSTP structure, achieving 100% fault coverage with minimal area overhead and CPU time, particularly for larger circuits. This advancement holds promise for enhancing the testing of digital ICs in the context of nanometer technology.”"
"""Human""","“In the era of Internet economics, e-business has become one of the most important strategic factors for enterprise development, so theoretical systems are needed to help enterprises develop e-business transformation strategies. A review of enterprise transformation theory identified five critical organization dimensions of e-business transformation, corporate strategy and vision transformation, organizational structure, product and market transformation, business process transformation, and corporate culture transformation. An e-business transformation process model was developed based on the five dimensions. This model can help enterprises to more effectively implement e-business transformation strategies.”"
"""AI""","“In the context of Internet economics, e-business has emerged as a crucial strategic element for enterprise growth, prompting the need for theoretical frameworks to guide effective e-business transformation strategies. Drawing from a comprehensive assessment of enterprise transformation theory, this study identifies five pivotal dimensions that underpin e-business transformation: corporate strategy and vision transformation, organizational structure, product and market transformation, business process transformation, and corporate culture transformation. Building upon these dimensions, an e-business transformation process model is formulated, offering enterprises a structured approach to implement e-business transformation strategies with greater efficacy. This model holds significant potential to guide enterprises through the intricate landscape of e-business adaptation and growth.”"
"""Human""","“An analytical expression is presented for the susceptibility of liquid cast iron to solidify according to the Fe-C-X metastable system (also known as the chilling tendency of cast iron, CT). The analysis incorporates the nucleation and growth processes associated with the eutectic transformation. The CT is related to the physicochemical state of the liquid, the eutectic cells in the flake graphite, and the number of nodules in nodular cast iron. In particular, the CT can be related to the critical wall thickness, s cr , or the chill width, W cr , in wedge shaped castings. Finally, this work serves as a guide for understanding the effect of technical factors such as the melt chemistry, the spheroidizing and inoculation practice, and the holding time and temperature on the resultant CT and chill of the cast iron. Theoretical calculations of s cr and W cr compare well with experimental data for flake graphite and nodular cast iron”"
"""AI""","“This paper presents an analytical expression to characterize the susceptibility of liquid cast iron to solidify based on the Fe-C-X metastable system, commonly referred to as chilling tendency (CT) of cast iron. The analysis incorporates the nucleation and growth processes associated with the eutectic transformation. The CT is closely tied to the physical and chemical state of the liquid, as well as the characteristics of eutectic cells in flake graphite and nodules in nodular cast iron. Notably, the CT can be linked to critical wall thickness (s cr ) or chill width (W cr ) in wedge-shaped castings. This work provides insights into how technical factors, such as melt chemistry, spheroidizing and inoculation practices, and holding time and temperature, impact the resulting CT and chill of the cast iron. Theoretical calculations for s cr and W cr demonstrate good agreement with experimental data for both flake graphite and nodular cast iron, offering valuable guidance for the optimization of cast iron properties.”"
"""Human""","“Problem-solving processes in value management (VM) workshops in the construction industry are experience-based, and the quality of these workshops depends very much on the experience of the team members. The efficiency and effectiveness of VM workshops can be improved by better reusing the experience of previous VM cases and field knowledge. This paper describes a new approach to facilitate VM workshops in the construction industry using data mining (OM) techniques. The feasibility of integrating OM techniques with VM workshops in the construction industry is demonstrated in case studies. Examples are presented to illustrate different methods of applying OM tools in VM workshops. The results show that OM techniques can help team members in VM workshops to understand their problems more clearly and to generate more ideas for current problems.”"
"""AI""","“This paper addresses the experience-based nature of problem-solving in value management (VM) workshops within the construction industry, highlighting the pivotal role of team members' expertise in workshop quality. Enhancing the efficiency and effectiveness of VM workshops is a key goal, achievable through improved utilization of past VM cases and on-site knowledge. The paper introduces an innovative approach that leverages data mining (OM) techniques to facilitate VM workshops in the construction sector. Through case studies, the feasibility of integrating OM methods within VM workshops is demonstrated. Real-world examples are provided to showcase diverse applications of OM tools in VM workshops. The findings underscore the ability of OM techniques to enhance problem comprehension and idea generation, thereby assisting VM workshop participants in attaining improved outcomes.”"
"""Human""","“This paper introduces a mesh less method based on Daubechies (DB) wavelets for 2-D elastic problems. The scaling and wavelet functions of the DB wavelet are used as basis functions to approximate the unknown field functions, so there is no need to construct costly shape functions as in the finite element method (FEM) and other meshless methods. In addition, the properties of the DB wavelets facilitate implementation of the method. The new method is used to analyze the elastic problem of a plain plate with a circle hole, and the numerical results agree well with the FEM. This method is effective and can be extended to solve complicated two or three dimensional problems.”"
"""AI""","“In this paper, a novel meshless approach based on Daubechies (DB) wavelets is introduced for addressing 2-D elastic problems. The DB wavelet's scaling and wavelet functions are harnessed as basis functions to approximate the unknown field functions. This eliminates the need for constructing elaborate shape functions as seen in finite element methods (FEM) and other meshless techniques. Moreover, the inherent properties of the DB wavelets simplify the method's implementation. The proposed approach is applied to analyze the elastic behavior of a flat plate with a circular hole, with the numerical outcomes exhibiting favorable agreement with FEM results. The method is proven effective and possesses potential for extension to tackle intricate two or three-dimensional problems.”"
"""Human""",“The accessibility of a destination-based transportation system is defined to quantify the performance of transportation systems which access a distinct destination. The access cost is used to reflect the utility of the transportation system including the fatigue and inconvenience in the total cost. The cost is quantified by two coefficients which represent the different characteristics of various people. The average cost and the income-relative accessibility are used to describe various aspects of the accessibility and to evaluate the accessibility of a destination-based system. A case study uses data from the Kunming transportation system to evaluate the accessibility of the present city airport. The calibrated coefficients are then used to evaluate the transportation system to the new Kunming international airport. The results show that this transportation accessibility evaluation can be combined with transportation planning to study transportation sub-systems.”
"""AI""","“The concept of accessibility within a destination-oriented transportation system is introduced to gauge its performance. This assessment quantifies the system's utility, factoring in elements such as fatigue and inconvenience, through an access cost metric. This cost is determined using coefficients that account for varying characteristics among individuals. The evaluation encompasses average cost and income-relative accessibility to comprehensively depict the accessibility spectrum and appraise the destination-oriented system. A practical example employs Kunming's transportation data to assess the accessibility of the existing city airport. The calibrated coefficients are subsequently employed to gauge the system's performance in connection with the new Kunming international airport. The findings underscore the utility of this accessibility evaluation methodology in tandem with transportation planning, offering insights into specific transportation sub-systems.”"
"""Human""","“An algorithm is presented for estimating the direction and strength of point light with the strength of ambient illumination. Existing approaches evaluate these illumination parameters directly in the high dimensional image space, while we estimate the parameters in two steps: first by projecting the image to an orthogonal linear subspace based on spherical harmonic basis functions and then by calculating the parameters in the low dimensional subspace. The test results using the CMU PIE database and Yale Database B show the stability and effectiveness of the method. The resulting illumination information can be used to synthesize more realistic relighting images and to recognize objects under variable illumination”"
"""AI""","“This paper introduces an algorithm for estimating the direction and intensity of point light along with ambient illumination strength. Unlike existing methods that assess these illumination parameters directly in high-dimensional image space, our approach involves a two-step process. Initially, the image is projected onto an orthogonal linear subspace using spherical harmonic basis functions, followed by parameter calculation in the low-dimensional subspace. Experimental results utilizing the CMU PIE database and Yale Database B demonstrate the method's stability and effectiveness. The obtained illumination data is useful for generating more realistic relit images and recognizing objects under varying illumination conditions.”"
"""Human""","“For a disaster whose scale includes an urban area, it is difficult to be studied with physical experiments. Numerical simulation is found a very efficient tool for such problem. This paper aims at developing an integrated urban earthquake simulation system (UESS) that uses GIS as the model source, CAD as the model generating tools, FEA as damage prediction, and virtual reality (VR) as the post-process platform. An automatic procedure was developed to buildup the 3D structural model of buildings in an urban area, as well as to simulate their earthquake performances, from the digital map of GIS. And the simulation results were presented in an integrated interface with a GIS view-port for position finding, a CAD view-port for 3D structural damage identification, and a VR view-port for 3D dynamic structural vibration display. An urban example with more than 7000 buildings was select to demonstrate the feasibility of proposed system.”"
"""AI""","“This paper introduces the development of an integrated urban earthquake simulation system (UESS) to address disasters on an urban scale, which are difficult to study through physical experiments. The UESS employs Geographic Information Systems (GIS) as the model source, Computer-Aided Design (CAD) as the model generation tool, Finite Element Analysis (FEA) for damage prediction, and Virtual Reality (VR) as the post-processing platform. An automated procedure is established to construct a 3D structural model of buildings within an urban area and simulate their earthquake responses using GIS data. Simulation outcomes are presented in an integrated interface that includes a GIS viewport for location identification, a CAD viewport for 3D structural damage assessment, and a VR viewport for displaying dynamic 3D structural vibrations. The viability of the proposed system is demonstrated through an urban example involving over 7000 buildings.”"
"""Human""","“This paper describes the problem of doing description logic (OL) reasoning with partially closed world. The issue was addressed by extending the syntax of OL SROIQ with an NBox, which specifies the predicates to close, extending the semantics with the idea of negation as failure, reducing the closed world reasoning to incremental reasoning on classical OL ontologies, and applying the syntactic approximation technology to improve the reasoning performance. Compared with the existing OBox approach, which corresponds to the relation database, the NBox approach supports deduction on closed concepts and roles. Also, the approximate reasoning can reduce reasoning complexity from N2EXPTIME-complete to PTIME-complete while preserving the correctness of reasoning for ontologies with certain properties.”"
"""AI""","“This paper addresses the challenge of performing description logic (DL) reasoning with partially closed worlds. The approach involves extending the syntax of DL SROIQ with an NBox that specifies the predicates to be closed, enhancing the semantics with the concept of negation as failure, transforming closed world reasoning into incremental reasoning on standard DL ontologies, and utilizing syntactic approximation techniques to enhance reasoning efficiency. In comparison to the existing OBox approach, which corresponds to a relational database, the proposed NBox method facilitates deduction on closed concepts and roles. Additionally, the adoption of approximate reasoning reduces reasoning complexity from being N2EXPTIME-complete to PTIME-complete, while maintaining the accuracy of reasoning for ontologies with specific characteristics.”"
"""Human""","“Quality of service (QoS) support is a key attribute for multimedia traffic including video, voice, and data in wireless local area networks (LANs) but is limited in 802.11-based wireless LANs. A polling-based scheme called the point coordination function (PCF) was developed for 802.11 LANs to support the transmission of multimedia traffic. However, the PCF is not able to meet the desired practical traffic differentiation requirements for real-time data. This paper describes a QoS support polling scheme based on the IEEE 802.11 medium access control (MAC) protocol. The scheme uses a two-level polling mechanism with the QoS classes differentiated by two different access policies. Stations with higher priority traffic such as key or real-time data form the first level and can access the common channel through an exhaustive access policy. Other stations with lower priority traffic form the second level and can access the channel through a gated access policy. A system model based on imbedded Markov chain theory and a generation function were setup to explicitly analyze the mean information packet waiting time of the two-level polling scheme. Theoretical and simulation results show that the new scheme efficiently differentiates services to guarantee better QoS and system stability”"
"""AI""","“This paper addresses the challenge of ensuring Quality of Service (QoS) for multimedia traffic like video, voice, and data within wireless local area networks (LANs), particularly in the context of 802.11-based wireless LANs. While a polling-based approach called the point coordination function (PCF) was developed for 802.11 LANs to accommodate multimedia traffic transmission, it falls short in delivering the required traffic differentiation for real-time data. This paper presents a QoS-supportive polling scheme founded on the IEEE 802.11 medium access control (MAC) protocol. The proposed approach employs a two-tier polling mechanism, with QoS classes differentiated by distinct access policies. The first tier is composed of stations with high-priority traffic, like real-time data, and follows an exhaustive access policy. Meanwhile, the second tier consists of stations with lower-priority traffic and utilizes a gated access policy. The study establishes a system model based on embedded Markov chain theory and a generation function to explicitly analyze the mean waiting time for information packets in the two-level polling scheme. Theoretical analysis and simulation results demonstrate the effectiveness of the proposed scheme in differentiating services, ensuring improved QoS, and maintaining system stability.”"
"""Human""","“A general review of the socio-economic impact of the intelligent transport system (ITS) is presented with a case study to demonstrate the data envelopment analysis method. Cost-benefit analyses are still the dominant method for evaluating ITS and other transport engineering projects, while cost effective analyses and multi-criteria appraisals are widely used to define and prioritize objectives by providing useful information for the most promising policy decisions. Both cost-benefit analyses and a data envelopment analysis method are applied to analyze the socio-economic impact of convoy driving systems. The main findings are that a convoy provides a worthwhile benefit-cost ratio when more than 30% of the traffics in the convoys and the traffic load exceeds 5500 vehicles/h for a three-lane motorway. The results also show that for a fixed percentage of convoys, increased demand will increase the data envelopment analysis method relative efficiency and that the neglect of certain output indicators of an ITS may result in underestimation of the system effects”"
"""AI""","“This paper offers a comprehensive review of the socio-economic implications of Intelligent Transport Systems (ITS) and employs a case study to illustrate the application of the Data Envelopment Analysis (DEA) method. While cost-benefit analyses remain prevalent for evaluating ITS and transportation projects, cost-effective analyses and multi-criteria appraisals are gaining traction for objective definition and prioritization. These methods aid in informing impactful policy decisions. This study employs both cost-benefit analyses and the DEA method to assess the socio-economic impact of convoy driving systems. The key findings highlight that convoy systems yield favorable benefit-cost ratios when over 30% of traffic participates in convoys and the traffic load surpasses 5500 vehicles/h on a three-lane motorway. The outcomes further demonstrate that, with a constant percentage of convoys, increased demand elevates DEA method relative efficiency. Additionally, neglecting specific output indicators of an ITS may lead to underestimations of the system's effects.”"
"""Human""","“The authors argue that travel forecasting models should be dynamic and disaggregate in their representation of demand, supply, and supply-demand interactions, and propose a framework for such models. The proposed framework consists of disaggregate activity-based representation of travel choices of individual motorists on the demand side integrated with disaggregate dynamic modeling of network performance, through vehicle-based traffic simulation models on the supply side. The demand model generates individual members of the population and assigns to them socioeconomic characteristics. The generated motorists maintain these characteristics when they are loaded on the network by the supply model. In an equilibrium setting, the framework lends itself to a fixed-point formulation to represent and resolve demand-supply interactions. The paper discusses some of the remaining development challenges and presents an example of an existing travel forecasting model system that incorporates many of the proposed elements.”"
"""AI""","“The authors advocate for the adoption of dynamic and disaggregated travel forecasting models that encompass demand, supply, and their interactions. To this end, they introduce a comprehensive framework for such models. This framework integrates a detailed activity-based portrayal of individual motorists' travel choices on the demand side with a granular, dynamic representation of network performance via vehicle-based traffic simulation models on the supply side. The demand model generates individual motorists based on socioeconomic attributes, which are preserved when they interact with the network in the supply model. In equilibrium scenarios, this framework offers a fixed-point approach to capture and address demand-supply interactions. The paper outlines existing challenges in development and offers an illustrative example of a travel forecasting model system that incorporates many components of the proposed framework.”"
"""Human""","“Investigators seek ways to effectively raise the response rate of surveys. Incentives have a great influence on response rate, retention rate, and response quality in traditional surveys. The purpose of this paper is to investigate what kinds of incentives are effective in web-based surveys. This study summarizes the research framework of previous studies on the effect of incentives in web-based surveys to show that material incentives have a significant effect on web-based surveys with prepaid incentives having a moderate influence on raising the response rate. Contingent incentives significantly reduce the response rate. These findings provide guidance for web-based investigations”"
"""AI""","“Researchers aim to enhance survey response rates and quality, particularly in web-based surveys, by exploring effective incentive strategies. Incentives have been known to impact response rates, retention rates, and response quality in conventional surveys. This paper delves into the efficacy of various incentives in the context of web-based surveys. A comprehensive review of previous studies' frameworks reveals that material incentives hold substantial sway over web-based surveys, particularly prepaid incentives, which exhibit a moderate influence in boosting response rates. In contrast, contingent incentives show a significant reduction in response rates. These insights contribute valuable guidance for optimizing web-based investigations.”"
"""Human""","“Old towns like Lijiang have enormous historic, artistic, and architectural value. The buildings in such old towns are usually made of highly combustible materials, such as wood and grass. If a fire breaks out, it will spread to multiple buildings, so fire spreading and controlling in old towns need to be studied. This paper presents a fire spread model for old towns based on cellular automaton. The cellular automaton rules were set according to historical fire data in empirical formulas. The model also considered the effects of climate. The simulation results were visualized in a geography information system. An example of a fire spread in Lijiang was investigated with the results showing that this model provides a realistic tool for predicting fire spread in old towns. Fire brigades can use this tool to predict when and how a fire spreads to minimize the losses”"
"""AI""","“Old towns like Lijiang possess significant historical, artistic, and architectural value, often constructed with highly combustible materials such as wood and thatch. Due to the potential for rapid fire propagation across multiple buildings, it is imperative to comprehensively investigate fire spread and control in these areas. This paper introduces a cellular automaton-based fire spread model tailored to old towns. The model's rules are derived from empirical formulas based on historical fire data, with considerations for climatic factors. The simulation outcomes are visualized using a geographical information system. By applying the model to a fire scenario in Lijiang, it is demonstrated that the model is an effective predictive tool for anticipating fire propagation within old towns. This model can be utilized by fire brigades to anticipate fire spread patterns and strategize mitigation measures to minimize potential losses.”"
"""Human""","“M. Hewitt: Regarding Mr Weaving's reply to Mr G. D. Walker's remarks on transformers with their windings screened by metallized paper, it is my impression that this reply is based on some misconception formed as a result of Mr Walker's wording, from which Mr Weaving apparently inferred that the individual turns had metallized paper applied to them, which is not the case, at any rate as far as multilayer cross-over type coils are concerned. For such coils “the unit” as far as shielding is concerned is a whole layer or even groups of layers and not individual turns.”"
"""AI""","“M. Hewitt responds to Mr. Weaving's reply to Mr. G. D. Walker's comments on transformers with windings screened by metallized paper. Hewitt suggests that Weaving's response may stem from a misunderstanding arising from Walker's wording. Hewitt clarifies that the application of metallized paper to individual turns is not the case, particularly in the context of multilayer cross-over type coils. In such cases, the shielding pertains to an entire layer or groups of layers rather than individual turns.”"
"""Human""","“Personalized service systems are an effective way to help users obtain recommendations for unseen items, within the enormous volume of information available based on their preferences. The most commonly used personalized service system methods are collaborative filtering, content-based filtering, and hybrid filtering. Unfortunately, each method has its drawbacks. This paper proposes a new method which unified partition-based collaborative filtering and meta-information filtering. In partition-based collaborative filtering the user-item rating matrix can be partitioned into low-dimensional dense matrices using a matrix clustering algorithm. Recommendations are generated based on these low-dimensional matrices. Additionally, the very low ratings problem can be solved using meta-information filtering. The unified method is applied to a digital resource management system. The experimental results show the high efficiency and good performance of the new approach”"
"""AI""","“This paper introduces a novel approach to personalized service systems, aimed at assisting users in finding recommendations for unseen items from the vast pool of available information based on their preferences. The conventional methods of personalized service systems—collaborative filtering, content-based filtering, and hybrid filtering—each have limitations. The proposed method combines partition-based collaborative filtering with meta-information filtering. The partition-based collaborative filtering involves dividing the user-item rating matrix into smaller, low-dimensional dense matrices using a matrix clustering algorithm, upon which recommendations are derived. Furthermore, the challenge posed by very low ratings is addressed through the integration of meta-information filtering. The effectiveness of this unified method is demonstrated through its application to a digital resource management system. The experimental results underscore the approach's efficiency and strong performance.”"
"""Human""","“Engineers tend to use different software to perform tasks such as geometry modeling, database management, numerical analysis, and visualization. This may cause decrease of productivity and loss of information during the conversion process between different data file formats. This paper presents a computer aided design (CAD) and computer aided engineering (CAE) system integration using scientific visualization tools and techniques. It deals with the development of a 3D CAD add-in for lighting analysis which uses the CAD model as 3D interface for creating a lighting scheme, processing, and visualizing 2D or 3D illuminance fields. Visualization features as color and contour mapping were developed using the visualization toolkit (VTK) toolkit. The application integrates all functionalities of the 3D CAD with tools for light sources database management, pre-processing, processing, and post-processing of illuminance fields in a single environment. This approach increases productivity and eliminates the need for different software”"
"""AI""","“This paper introduces a solution to the common problem of engineers using various software tools for different tasks, leading to reduced productivity and data loss during the conversion between different file formats. The proposed approach involves integrating computer-aided design (CAD) and computer-aided engineering (CAE) systems using scientific visualization tools and techniques. Specifically, a 3D CAD add-in for lighting analysis is developed, leveraging the CAD model as a 3D interface for creating lighting schemes and visualizing 2D or 3D illuminance fields. The visualization capabilities, including color and contour mapping, are implemented using the Visualization Toolkit (VTK). This integrated application encompasses all aspects of 3D CAD alongside tools for managing light sources, pre-processing, processing, and post-processing of illuminance fields within a unified environment. As a result, this approach enhances productivity and eliminates the need for multiple software applications.”"
"""Human""","“For text-independent speaker verification, the Gaussian mixture model (GMM) using a universal background model strategy and the GMM using support vector machines are the two most commonly used methodologies. Recently, a new SVM-based speaker verification method using GMM super vectors has been proposed. This paper describes the construction of a new speaker verification system and investigates the use of nuisance attribute projection and test normalization to further enhance performance. Experiments were conducted on the core test of the 2006 NIST speaker recognition evaluation corpus. The experimental results indicate that an SVM-based speaker verification system using GMM super vectors can achieve appealing performance. With the use of nuisance attribute projection and test normalization, the system performance can be significantly improved, with improvements in the equal error rate from 7.780/0 to 4.920/0 and detection cost function from 0.0376 to 0.0251.”"
"""AI""","“This paper focuses on text-independent speaker verification techniques, particularly exploring Gaussian mixture models (GMM) using both a universal background model approach and support vector machines (SVM) as prevalent methodologies. A novel approach involving SVM-based speaker verification with GMM super vectors is introduced. The paper outlines the creation of this new verification system and delves into the potential enhancements achieved through nuisance attribute projection and test normalization. Experiments were conducted on the 2006 NIST speaker recognition evaluation corpus, highlighting that the SVM-based speaker verification system using GMM super vectors exhibits promising performance. Furthermore, the incorporation of nuisance attribute projection and test normalization significantly enhances the system's efficacy, leading to substantial improvements in the equal error rate and the detection cost function.”"
"""Human""","“This paper represents a design and development of a mobile sensing unit as well as its prototype implementation for railway track monitoring. The unit consists of an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer and an ADC (Analog/Digital Converter) so that the unit can trace the route while capturing an acceleration response of a passenger vehicle. The unit enables more frequent and qualitative data acquisition compared with traditional and the state of the practice railway track inspection equipments. Locating disorder is the key of our unit, which has a reasonable accuracy of positioning with GPS data, existing facilities landmarks, and car acceleration responses. The proposed unit is a promising device for efficient properties management of railway agencies. The prototype implementation shows a result that car acceleration responses are related with the track displacements in low frequencies. The results also imply that sensor settlement on a vehicle floor, not axes or bogies, is effective for capturing track vertical displacements.”"
"""AI""","“This paper outlines the design and development of a mobile sensing unit along with its prototype implementation for monitoring railway tracks. Comprising components such as an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer, and an Analog/Digital Converter (ADC), this unit facilitates route tracing while capturing acceleration responses from a passenger vehicle. In comparison to conventional railway track inspection equipment, the unit offers more frequent and high-quality data acquisition. A pivotal feature of the unit is its capability to detect track anomalies, benefiting from accurate positioning via GPS data, landmarks, and car acceleration responses. The proposed unit holds promise for effective railway property management. The prototype's results demonstrate a correlation between car acceleration responses and track displacements, particularly in low frequencies. Notably, sensor placement on a vehicle's floor, rather than axes or bogies, effectively captures vertical track displacements.”"
"""Human""","“Topology design of artificial neural networks (ANNs) is an important problem for large scale applications. This paper describes a new efficient pruning method, the multi-weight optimal brain surgeon (MWOBS) method, to optimize neural network topologies. The advantages and disadvantages of the OBS and unit-OBS were analyzed to develop the method. Actually, optimized topologies are difficult to get within reasonable times for complex problems. Motivating by the mechanism of natural neurons, the MW-OBS method balances the accuracy and the time complexity to achieve better neural network performance. The method will delete multiple connections among neurons according to the second derivative of the error function, so the arithmetic converges rapidly while the accuracy of the neural network remains high. The stability and generalization ability of the method are illustrated in a Java program. The results show that the MWOBS method has the same accuracy as OBS, but time is similar to that of unit-OBS. Therefore, the MWOBS method can be used to efficiently optimize structures of neural networks for large scale applications”"
"""AI""","“This paper introduces a novel and efficient method, the multi-weight optimal brain surgeon (MWOBS) technique, for the topology design optimization of artificial neural networks (ANNs). Addressing the challenges posed by complex problems, the paper builds on the OBS and unit-OBS methods, taking into account their pros and cons. The MWOBS method draws inspiration from the behavior of natural neurons, aiming to strike a balance between accuracy and time complexity for enhanced neural network performance. By utilizing the second derivative of the error function, the MWOBS method prunes multiple connections between neurons, resulting in rapid arithmetic convergence without compromising neural network accuracy. The stability and generalization ability of the approach are validated through a Java program. The results demonstrate that MWOBS offers accuracy comparable to OBS while achieving a time efficiency similar to unit-OBS. This makes the MWOBS method a promising solution for efficiently optimizing neural network structures, particularly in large-scale applications.”"
"""Human""","“In this paper, we study the problem of rule extraction from data sets using the rough set method. For inconsistent rules due to improper selection of split-points during discretization, and/or to lack of information, we propose two methods to remove their inconsistency based on irregular decision tables. By using these methods, inconsistent rules are eliminated as far as possible, without affecting the remaining consistent rules. Experimental test indicates that use of the new method leads to an improvement in the mean accuracy of the extracted rules.”"
"""AI""","“This paper delves into the process of rule extraction from datasets utilizing the rough set approach. Addressing the challenge of inconsistent rules stemming from improper split-point selection during discretization or insufficient information, we present two techniques to rectify this inconsistency using irregular decision tables. These methods aim to effectively eliminate inconsistent rules without compromising the consistency of the remaining ones. Experimental testing demonstrates that the implementation of these novel methods contributes to enhancing the mean accuracy of the extracted rules.”"
"""Human""","“With the development of information technology, the fire safety assessment of whole structure or region based on the computer simulation has become a hot topic. However, traditionally, the concerned studies are performed separately for different objectives and difficult to perform an overall evaluation. A new multi-dimensional integration model and methodology for fire safety assessment were presented and two newly developed integrated systems were introduced to demonstrate the function of integration simulation technology in this paper. The first one is the analysis on the fire-resistant behaviors of whole structure under real fire loads. The second one is the study on fire evaluation and emergency rescue of campus based on geography information technology (GIS). Some practical examples are presented to illuminate the advantages of computer integration technology on fire safety assessment and emphasize some problems in the simulation. The results show that the multi-dimensional integration model offers a new way and platform for the integrating fire safety assessment of whole structure or region, and the integrated software developed is the useful engineering tools for cost-saving and safe design.”"
"""AI""","“As information technology continues to advance, the use of computer simulation for comprehensive fire safety assessments of entire structures or regions has gained significant attention. However, traditional approaches often tackle different objectives separately, leading to challenges in achieving an overarching evaluation. This paper introduces a novel multi-dimensional integration model and methodology for fire safety assessments, showcasing two newly developed integrated systems to highlight the capabilities of integration simulation technology. The first system addresses the analysis of fire-resistant behaviors of entire structures under realistic fire loads, while the second system focuses on fire evaluation and emergency response planning for campuses using geographic information technology (GIS). Real-world examples are provided to underscore the benefits of computer integration technology in fire safety assessment, while also shedding light on certain challenges in simulation. The results underscore the potential of the multi-dimensional integration model as a platform for comprehensive fire safety assessments of entire structures or regions, with the developed integrated software serving as valuable engineering tools for efficient and secure design practices.”"
"""Human""","“Topology design of artificial neural networks (ANNs) is an important problem for large scale applications. This paper describes a new efficient pruning method, the multi-weight optimal brain surgeon (MWOBS) method, to optimize neural network topologies. The advantages and disadvantages of the OBS and unit-OBS were analyzed to develop the method. Actually, optimized topologies are difficult to get within reasonable times for complex problems. Motivating by the mechanism of natural neurons, the MW-OBS method balances the accuracy and the time complexity to achieve better neural network performance. The method will delete multiple connections among neurons according to the second derivative of the error function, so the arithmetic converges rapidly while the accuracy of the neural network remains high. The stability and generalization ability of the method are illustrated in a Java program. The results show that the MWOBS method has the same accuracy as OBS, but time is similar to that of unit-OBS. Therefore, the MWOBS method can be used to efficiently optimize structures of neural networks for large scale applications.”"
"""AI""","“In the context of large-scale applications, the topology design of artificial neural networks (ANNs) holds significant importance. This paper introduces a novel and efficient pruning technique, known as the multi-weight optimal brain surgeon (MWOBS) method, aimed at enhancing the optimization of neural network topologies. The paper begins by analyzing the strengths and weaknesses of existing OBS and unit-OBS methods, which serve as a foundation for the development of the MWOBS approach. The optimization of complex topologies within reasonable time frames is a notable challenge, and to address this, the MWOBS method draws inspiration from the mechanisms of natural neurons. By striking a balance between accuracy and time complexity, the MW-OBS approach achieves improved neural network performance. The method involves the removal of multiple connections among neurons based on the second derivative of the error function, leading to rapid arithmetic convergence while maintaining high neural network accuracy. The effectiveness of the method's stability and generalization capabilities is demonstrated through a Java program. The results indicate that the MWOBS method offers comparable accuracy to OBS while exhibiting time efficiency similar to unit-OBS. Consequently, the MWOBS technique emerges as a valuable tool for efficiently optimizing the structures of neural networks in large-scale applications.”"
"""Human""","“This paper gives a brief description of present-day designs of Planté and pasted-plate cells. Reference is made to the floating and trickle charging of Planté batteries and the modern applications of this type of battery are examined. These applications include batteries for switch-operating purposes in power stations and substations, batteries for operating telephone exchanges and for emergency lighting purposes”"
"""AI""","“This paper provides a concise overview of the current designs of Planté and pasted-plate cells. It discusses the floating and trickle charging methods employed with Planté batteries and delves into contemporary applications for this battery type. Notably, the paper explores the utilization of Planté batteries in switch-operating functions within power stations and substations, their use in telephone exchange operations, and their role in emergency lighting applications.”"
"""Human""","“The paper describes a first attempt at using reactively loaded, pwm converters as fast dynamic filters in a power network. The principles used are described, the current fed pwm converter used discussed and experimental results given as obtained on a variable speed drive as a non — linear load. The simulated and experimental results compare very well, and the technique appears promising for further investigation.”"
"""AI""","“The paper presents an initial exploration into the utilization of reactively loaded pulse-width modulation (PWM) converters as rapid dynamic filters within a power network. The underlying principles are elucidated, focusing on the employed current-fed PWM converter. The study delves into the details of the experimental setup, with results obtained from testing on a variable speed drive as a representative nonlinear load. The comparison between simulated and experimental outcomes demonstrates strong alignment, indicating a promising avenue for further research and investigation.”"
"""Human""","“This paper attempts to explain the Discrete Fourier Transform in terms of its action on real pulse and sinusoidal data. Even real data is seen to give rise to even real spectra, and odd real data produces odd imaginary spectra. The idea of aliases, leakage and windows are examined. The Fast Fourier Transform is a method of performing the DFT and it is approached firstly by decimation in time and then by decimation in frequency. The terms twiddle factor, bit reversal, shuffling, butterflies and radix are Introduced. Application of the FFT in correlation, convolution, band limited interpolation and coherence functions is discussed. The Chirp Z Transform performed by convolution in a transversal filter is examined, as it seems to be the cheapest and fastest method of performing the DFT.”"
"""AI""","“This paper aims to elucidate the Discrete Fourier Transform (DFT) by describing its impact on real pulse and sinusoidal data. It reveals that even real data generates even real spectra, while odd real data yields odd imaginary spectra. Concepts such as aliases, leakage, and windows are explored. The Fast Fourier Transform (FFT) is introduced as a technique to execute the DFT, discussed initially through decimation in time and subsequently via decimation in frequency. Terms like twiddle factor, bit reversal, shuffling, butterflies, and radix are introduced in this context. The paper also delves into the application of the FFT in correlation, convolution, band-limited interpolation, and coherence functions. Additionally, the Chirp Z Transform, accomplished through convolution in a transversal filter, is examined as an economical and swift approach for performing the DFT.”"
"""Human""","“This paper describes a novel balanced transmission line antenna where the maximum separation between the two conductors is only a fraction of a wavelength at the lowest operating frequency. The balanced transmission line is formed by exponentially extending the ridges of a double-ridged waveguide. The poor VSWR and patterns caused by reflections from the tips of the ridge extensions are improved by a combination of aperture matching and the use of microwave absorber. Measured VSWR, radiation pattern and gain data are presented for this compact, broadband antenna over the 2 to 11 GHz frequency range.”"
"""AI""","“This paper introduces an innovative balanced transmission line antenna design that features closely spaced conductors, with their maximum separation being a fraction of a wavelength at the lowest operating frequency. This balanced transmission line is realized by extending the ridges of a double-ridged waveguide in an exponential manner. To mitigate issues such as unfavorable voltage standing wave ratio (VSWR) and disrupted radiation patterns arising from reflections at the ridge extensions' tips, a combination of aperture matching and microwave absorber is employed. The study presents measured data for VSWR, radiation patterns, and gain across the frequency range of 2 to 11 GHz for this compact and broadband antenna design.”"
"""Human""","“This paper describes a computer program for the prediction of radio path loss over irregular terrain in the frequency range 20 to 10 000 MHz. Propagation modes modelled are line of sight, diffraction and troposcatter. The operation of the program it described briefly. The prediction accuracy has been analysed and the deviation between the active and predicted loss is shown to be of the same order of magnitude as the natural signal level fluctuations. Typical uses of the program and an example of system analysis are presented.”"
"""AI""","“This paper outlines a computer program designed to forecast radio path loss across uneven terrain within the frequency range of 20 to 10,000 MHz. The model incorporates various propagation modes, encompassing line of sight, diffraction, and troposcatter. The program's functionality is briefly explained. The study delves into the assessment of prediction accuracy, revealing that the variance between actual and anticipated loss is on par with the inherent fluctuations in signal strength. The paper illustrates common applications of the program and provides an instance of system analysis.”"
"""Human""",“This paper covers the experimental validation of FEM calculations of losses in a planar inductor under high frequency operating conditions. Thermally controlled experiments are performed at different frequencies in the operating circuit to validate the losses calculated by the FEM.”
"""AI""","“This paper focuses on experimentally validating finite element method (FEM) calculations pertaining to losses in a planar inductor operating under high-frequency conditions. Thermo-controlled experiments are conducted across various frequencies within the operational circuit, serving to authenticate the losses computed through FEM analysis.”"
"""Human""","“This paper investigated new properties of the ac (alternative current) Stark effect of three-level atom system in two-zone Raman-Ramsey interactions. It derived the closed-form expressions of ac Stark shift and shift phase. Then with the help of numerical computation, it demonstrated the effect of various combinations of laser intensities, initial atomic populations and laser detuning on Stark shift and shift phase. Based on analytical expressions and results of numerical computation the paper provided useful information for determination of the best conditions in reduction of the ac Stark shift”"
"""AI""","“This paper explores novel characteristics of the ac (alternative current) Stark effect in a three-level atom system subjected to two-zone Raman-Ramsey interactions. The study derives closed-form expressions for the ac Stark shift and shift phase. Utilizing numerical computations, it examines the impact of different combinations of laser intensities, initial atomic populations, and laser detuning on both the Stark shift and shift phase. Through analytical expressions and numerical findings, the paper offers valuable insights for optimizing conditions to mitigate the ac Stark shift.”"
"""Human""","“This paper is concerned with the data association problem of distributed long baseline 2-D passive sensors (or jammed 3-D active sensors). Two data association methods, minimum miss distance method and hinge angle method, are discussed when the earth curvature is considered. According to the pioneering work, when the earth curvature effect was not considered, the results obtained by using the hinge angle based statistic were essentially the same as those obtained by using the miss distance based statistic in two sensor case. In long baseline case, the earth curvature must be considered. In this paper, the detailed expressions of the above two methods are derived for long baseline cases. In order to analyze the performance of the two methods, the correct association probability of the true targets and the incorrect association probability of the ghost targets are defined. The Monte Carlo simulations in two and three sensor cases are made, respectively. Simulation results show that the minimum miss distance method is superior to the hinge angle method both in the case of three sensors and in the case of two sensors with targets being located near the baseline, and that the performance of the minimum miss distance method is equivalent to that of the hinge angle method when the targets are located far from the baseline.”"
"""AI""","“This paper addresses the data association problem in distributed long baseline 2-D passive sensors (or jammed 3-D active sensors), taking into account the curvature of the Earth. It discusses two data association methods: the minimum miss distance method and the hinge angle method. While previous work without considering Earth curvature showed similar results for both methods in a two-sensor case, this changes in long baseline scenarios where Earth curvature becomes significant. The paper derives detailed expressions for these methods in long baseline situations and defines correct and incorrect association probabilities for true and ghost targets, respectively, to analyze their performance. Monte Carlo simulations are conducted for two and three sensor cases. The simulation outcomes demonstrate that the minimum miss distance method outperforms the hinge angle method, especially when targets are located near the baseline or when three sensors are involved. However, their performance becomes similar when targets are farther from the baseline.”"
"""Human""","“This paper presents a secure communication protocol model-EABM, by which network security communication can be realized easily and efficiently. First, the paper gives a thorough analysis of the protocol system, systematic construction and state transition of EABM. Then, it describes the channels and the process of state transition of EABM in terms of ESTELLE. At last, it offers a verification of the accuracy of the EABM model”"
"""AI""","“This paper introduces the Enhanced Authentication and Bit Masking (EABM) secure communication protocol model, providing a convenient and effective solution for achieving network security in communication. The protocol system is comprehensively analyzed, including its systematic construction and state transition. The paper also details the channels and the state transition process of EABM using the ESTELLE formal description technique. Finally, the accuracy of the EABM model is verified, confirming its effectiveness in ensuring secure communication.”"
"""Human""","“Various types of cutting tools are known and are in use for machining parts. The dimensional parameters associated with cutting tools need to be estimated and compared to the desired values for determining their cutting performance. In this paper, a data analysis methodology for extracting parameters from a measured point set corresponding to the surface of a cutting tool is provided. We propose that the 3-D data can be simplified into 2-D data or regular data by virtually slicing it at a predetermined section or by projecting it onto a same axial plane after a simple fixed-axis rotation. A plurality of curves can be generated and optimized based on the obtained 2-D points on a cross section for calculating the section parameters, including radial (axial) rake angle, relief angle, and land width. Other dimensional parameters can also be extracted from the contour of the presented rotary axial projection data. The experimental results have shown that the approaches elaborated in this paper are effective and robust, which can be potentially extended to other applications such as the inspection of similar parts and their parameters extraction”"
"""AI""","“This paper introduces a data analysis methodology for extracting dimensional parameters from measured point sets representing cutting tool surfaces. The proposed approach involves simplifying 3-D data into 2-D or regular data by virtually slicing or projecting it onto a common axial plane after rotation. This simplification allows for the generation and optimization of curves based on 2-D points from cross sections, enabling the calculation of section parameters like radial (axial) rake angle, relief angle, and land width. Additionally, other dimensional parameters can be extracted from the contour of rotary axial projection data. Experimental results demonstrate the effectiveness and robustness of the methods presented, suggesting their potential applicability in various similar applications, including part inspection and parameter extraction.”"
"""Human""","“Nowadays, people are in need for continuous learning in order to keep up to date or to be upgraded in their job. An infrastructure for life-long learning requires continuous adaptation to learners' needs and must also provide flexible ways for students to use and personalize them. Controlling who can access a document, specifying when a student may be contacted for interactive instant messaging or periodical reminders in order to increase motivation for collaboration are just some examples of typical statements that may be specified by e.g., learners and learning management system administrators. This paper investigates how existing work in the area of policy representation and reasoning can be used in order to express these statements while at the same time obtaining the extra benefits policies provide (e.g., flexibility, dynamicity and interoperability). The paper analyzes existing policy languages and integrates one of them as part of a demonstration of its feasibility in providing more advanced and flexible eLearning environments.”"
"""AI""","“In the contemporary landscape, individuals require continuous learning to stay current and advance in their professions. Establishing a framework for lifelong learning necessitates adapting to learners' evolving needs while offering adaptable methods for personalization. This involves determining who can access documents, setting conditions for interactive instant messaging, or periodic reminders to enhance collaborative motivation, all of which can be articulated by learners and learning management system administrators. This study explores the integration of policy representation and reasoning principles to express these statements, simultaneously harnessing the supplementary advantages that policies offer, including flexibility, dynamism, and interoperability. By analyzing existing policy languages and incorporating one into the framework, the study demonstrates its viability in delivering advanced and versatile eLearning environments.”"
"""Human""","“NAND flash memory becomes one of the most popular storage devices in embedded system and mobile computers. Efficient flash file system designs are important for system designers and users. In this paper, we study the design issues and performance of flash-specific file systems and define four performance metrics to evaluate flash file systems. Then, detailed comparisons of three mainstream flash file systems are conducted, including JFFS2, YAFFS2 and UBIFS. Taking techniques used in the three file systems and evaluation results into account, flash file system design space is discussed. This paper can provide users a comprehensive understanding of NAND flash file system design, present guidelines for measurement of flash file systems and help users determine which file system best meets their need”"
"""AI""","“NAND flash memory has gained significant popularity as a storage device for embedded systems and mobile computers. Effective designs for flash file systems are crucial for both system designers and users. This paper delves into the design considerations and performance aspects of file systems specifically tailored for NAND flash memory. Four performance metrics are introduced to assess the efficiency of flash file systems. The study then proceeds with a detailed comparative analysis of three prominent flash file systems: JFFS2, YAFFS2, and UBIFS. By taking into account the techniques employed in these systems and the evaluation outcomes, the paper explores the design landscape of flash file systems. This work provides users with an encompassing comprehension of NAND flash file system design, offers guidelines for evaluating flash file systems, and assists users in selecting the most suitable file system for their requirements.”"
"""Human""","“This paper examines system combination issue for Syllable-confusion-network (SCN) -based Chinese Spoken term detection (STD). System combination for STD usually leads to improved accuracy but suffers from increased index size or complicated index structure. But in the scenarios where the index size and search speed are critical, a single compact index is highly desirable. Therefore we explore methods for efficient combination of a word-based system and a syllable-based system while keeping the compactness of the indices. First, a composite SCN is generated using two approaches: lattice combination and confusion network combination. Then a simple compact index is constructed from this composite SCN by merging cross-system redundant information. The experimental result on a 60-hour corpus shows that a relative accuracy improvement of 16.20% is achieved over the baseline syllable-based system. Meanwhile, it reduces the index size by 22.3% compared to the commonly adopted score combination method under comparable accuracy.”"
"""AI""","“This paper addresses the issue of system combination in the context of Chinese Spoken Term Detection (STD) using Syllable-confusion-networks (SCNs). System combination is known to enhance accuracy in STD, but it often results in larger index sizes or complex index structures. However, in scenarios where index size and search speed are critical, a compact index is preferable. This study explores methods for efficiently combining a word-based system and a syllable-based system while maintaining index compactness. First, a composite SCN is generated using two techniques: lattice combination and confusion network combination. Subsequently, a streamlined compact index is created from this composite SCN by eliminating cross-system redundant information. The experimental findings on a 60-hour corpus reveal a notable accuracy improvement of 16.20% compared to the baseline syllable-based system. Furthermore, it reduces the index size by 22.3% in comparison to the commonly employed score combination method while achieving similar accuracy levels.”"
"""Human""","“This paper investigates bandwidth allocation for services on multiple connections/paths in networks, and presents the mapping models from services to paths via connections based on Network utility maximization (NUM). Elastic services with concave utilities are firstly considered and the optimum of model can be obtained. Inelastic services with nonconcave (e.g., sigmoidal or discontinuous) utilities are also analyzed and the models with certain QoS guarantee for these services are presented. These models are significant to QoS guarantee for inelastic services transferred over multiple paths. Numerical examples verify optimization of the models with QoS guarantee for inelastic services.”"
"""AI""","“This paper delves into the allocation of bandwidth for services across multiple connections or paths within networks. It introduces mapping models that link services to paths through connections, employing the principles of Network Utility Maximization (NUM). The investigation begins by addressing elastic services with concave utilities, establishing an optimization model that yields the optimal solution. Subsequently, the study extends to inelastic services with nonconcave utilities, including sigmoidal or discontinuous functions. The paper puts forth models that ensure Quality of Service (QoS) guarantees for these inelastic services. These models are particularly relevant in providing QoS assurance for inelastic services transmitted across multiple paths. The validity of these models, including QoS guarantees for inelastic services, is demonstrated through numerical examples.”"
"""Human""","“This paper presents ND (Noise function detection) steganalysis algorithm to detect LSB steganography. The purpose of constructing the noise function is to quantify the smoothness or “regularity” of the images. ND method is based on the property that LSB embedding will increase the noise functional value of the image, and then the LSB embedding message ratio is estimated by constructing the simple line equation with the statistics of noise function in image. Experimental results show that this algorithm is more accurate and has a lower missing rate and false alarm rate than the conventional RS. Compared with RS method and some other powerful steganalysis approaches presented recently, ND method directly use the noise function to estimate the LSB embedding message ratio, neither fixed nor dynamic mask is needed. Thus, more running time is saved. The ND method is relatively faster, simpler and has good detection result”"
"""AI""","“This paper introduces the ND (Noise function detection) steganalysis algorithm, designed to detect steganography that involves Least Significant Bit (LSB) embedding. The algorithm focuses on constructing a noise function to quantify the regularity or smoothness of images. The ND method capitalizes on the fact that LSB embedding increases the noise function value of an image. It estimates the ratio of LSB embedding messages by forming a simple linear equation based on the image's noise function statistics. Through experimentation, the algorithm demonstrates improved accuracy compared to conventional RS (Rich Model Selection) methods. Notably, ND achieves lower rates of missing detection and false alarms. Unlike RS and other recent steganalysis approaches, ND directly employs the noise function to estimate the ratio of LSB embedding messages, eliminating the need for fixed or dynamic masks. This results in a more efficient and faster algorithm with strong detection capabilities.”"
"""Human""","“In this paper, the results of a 1 000 shot up-and-down test (duration 10 hours) in a 5 m rod-plane gap (relative air density RAD 0.83), stressed with positive standard switching impulses, are presented. The correlation between measured meteorological parameters and the flashover probability is investigated, as well as the effectiveness of atmospheric correction procedures. It is shown that the standard deviation of the flashover probability distribution appears to be independent of meteorological conditions and that solar radiation does not have a significant effect on the recorded flashover parameters. It is also shown that none of the examined atmospheric correction procedures are successful in removing all trends from the 50% disruptive discharge voltage of the test gap”"
"""AI""","“This paper discusses the outcomes of a 1,000-shot up-and-down test, which spanned over 10 hours, conducted in a 5-meter rod-plane gap with a relative air density (RAD) of 0.83. The test involved subjecting the gap to positive standard switching impulses. The study examines the relationship between measured meteorological variables and the likelihood of flashover, while also assessing the effectiveness of atmospheric correction techniques. Notably, the research reveals that the standard deviation of the flashover probability distribution remains unaffected by meteorological conditions. Additionally, solar radiation's impact on the recorded flashover parameters is deemed insignificant. Moreover, the investigation indicates that none of the analyzed atmospheric correction methods successfully eliminate all trends from the 50% disruptive discharge voltage observed in the test gap.”"
"""Human""","“Photonic integration technologies, which have been developed since the deployment of optical communications, are essential for increasing the network capacity at a low cost and with efficient power consumption. This paper reviews recent progress on photonic integrated devices for higher bit-rate and long-distance transmission technologies with advanced modulation formats”"
"""AI""","“This paper offers a comprehensive review of the recent advancements in photonic integration technologies, focusing on their crucial role in enhancing network capacity while maintaining cost-effectiveness and energy efficiency. The discussion specifically centers around photonic integrated devices that support higher bit-rate and long-distance transmission technologies, featuring advanced modulation formats.”"
"""Human""","“The effect of range-Dapper coupling caused by aircraft moving at very high speed makes trouble on selection of waveform parameters by using frequency-modulated interrupted continuous wave (FNHCW) or frequency-coded pulse (FCP). It also limits the increasing of coherent intergration time. In this paper, application of coherent phase-coded pluse train (CPCPT) solves range-Doppler coupling well. Relevant processing of CPCPT consists of three parts: Dopper preprocessing, pulse compression, and Doppler post-processing. The velocity information obtained by Doppler preprocessing is used for better pulse compression and range tracking. Doppler post-processing with range tracking could make longer coherent accumulation for better detection of target and higher velocity resolution. Finally, examples of data simulation are given to demonstrate the achievements mentioned above.”"
"""AI""","“This paper addresses the challenge of range-Doppler coupling caused by high-speed aircraft movement, which poses difficulties in selecting waveform parameters for frequency-modulated interrupted continuous wave (FNHCW) or frequency-coded pulse (FCP) systems. Additionally, this phenomenon limits the extension of coherent integration time. The paper introduces the coherent phase-coded pulse train (CPCPT) as a solution to effectively mitigate range-Doppler coupling. The processing of CPCPT involves three main steps: Doppler preprocessing, pulse compression, and Doppler post-processing. The velocity information acquired through Doppler preprocessing enhances pulse compression and range tracking. Doppler post-processing with range tracking enables longer coherent accumulation, thereby improving target detection and velocity resolution. The paper provides simulation examples to illustrate the benefits of CPCPT in addressing range-Doppler coupling challenges.”"
"""Human""","“To establish an efficient inter-satellite link (ISL) in an LEC) network, the effect of geometric characteristics of ISL on the ISLs and the devices on the LEO satellite should be examined. Because of the continuous movement of the LEO satellite, the time-varying behaviours of the ISL's geometric characrersistics continuously change with the change of the satellite's position on the orbit. These dynamic geometric characteristics of the ISLs are important for ISL's performance analyzing and the design of the devices on the LEO satellite. This paper describes dynamic geometric characteristics of ISL, analyzes the impact of these regulations on the tracking system of the satellite's antenna and the power adjusting system of the satellite'sTransmitter, with the Iridium system as an example.”"
"""AI""","“This research paper investigates the dynamic geometric characteristics of inter-satellite links (ISLs) within a Low Earth Orbit (LEO) satellite network, exploring their impact on both ISL performance and the design of satellite devices. The paper focuses on the continuous changes in ISL geometry due to the LEO satellite's orbital movement, examining their influence on the tracking system of the satellite's antenna and the power adjustment system of its transmitter. Using the Iridium system as a case study, the analysis highlights the importance of considering these dynamic properties in the design and operation of ISLs within LEO satellite networks.”"
"""Human""","“The multimedia conferencing system is an important tool for Computer supported Cooperative Work (CSCW), and also one of the basic techniques supporting team work in the Concurrent Engineering. In this paper the architecture and function modules of the reported multimedi a conferencing system are described. Then in order to reduce the network bandwidth required by the conferencing system, two methods for sending the conferencing data by using multicasting are discussed.”"
"""AI""","“This paper presents a multimedia conferencing system as a crucial tool for Computer Supported Cooperative Work (CSCW) and Concurrent Engineering's collaborative efforts. The architecture and functional modules of the system are outlined. To optimize network bandwidth usage, the paper explores two approaches for transmitting conferencing data through multicasting.”"
"""Human""","“Recent advances in virtualization technologies make the VM (Virtual machine) based server consolidation attractive for reducing cost and improving efficiency in enterprise computing. However, on a Xen-based platform, the I/O performance of a VM degrades significantly as the degree of consolidation increases, even if the VM is configured to have enough CPU share. This paper analyzes this phenomenon and identifies the reason as the mismatch between the CPU scheduler of Xen and its I/O architecture. We therefore present four enhancements for the CPU scheduler, including preempt-back, no-preemption of driver domain, block-bonus, and more fine-grained accounting. These enhancements are applied into our newly developed I/O-friendly CPU scheduler, namely Credit-HC (Credit-based high-consolidation-oriented) scheduler. The evaluation results show that with Credit-HC, one VM's I/O performance is nearly irrelevant to other domains' CPU workload, while the overhead is negligible. Moreover, the fairness of CPU share is kept as well.”"
"""AI""","“This paper addresses the issue of declining I/O performance in virtual machine (VM) based server consolidation on Xen-based platforms. Despite adequate CPU allocation, I/O performance degrades with increased consolidation. The paper attributes this to a disparity between Xen's CPU scheduler and its I/O architecture. To mitigate this, the authors propose enhancements for the CPU scheduler, introducing preempt-back, no-preemption for driver domain, block-bonus, and fine-grained accounting. These enhancements are incorporated into a new I/O-friendly CPU scheduler named Credit-HC (Credit-based high-consolidation-oriented). Evaluation results demonstrate that Credit-HC ensures individual VM's I/O performance is unaffected by other domains' CPU workloads, maintaining fairness and minimal overhead.”"
"""Human""","“Grey sequence generation can draw out and develop implied rules of the original data. Different kinds of generation methods were summarized and classified into two types: partial generation and whole generation. The average generation and stepwise ratio generation is disussed, the preference generation is regard as a special case of proportional division based on analysis geometric theory, propose an idea of using concave and convex status of discrete data to determine the generation coefficient. Based on the stepwise and smooth ratio generation, a tendency average generation is proposed and have a comparison using the data provided in papers listed in the references. The comparison proves that the new generation is better than the other two generations and errors are obviously reduced.”"
"""AI""","“This paper delves into the realm of grey sequence generation, a process that unveils latent rules within original data. Various generation methods are categorized as partial or whole generation. The study explores average generation and stepwise ratio generation, and identifies preference generation as a unique instance of proportional division rooted in geometric theory. A novel approach is introduced, utilizing the concave and convex states of discrete data to determine generation coefficients. Leveraging stepwise and smooth ratio generation, the concept of tendency average generation is introduced and substantiated through comparison with data from referenced papers. The findings reveal that this new generation method outperforms the other two, significantly reducing errors.”"
"""Human""",“A new aim-points-choosing model for attacking mobile missile systems in sector region is presented and realized in this paper. The model has the generality for attacking mobile target and is also useful for making an operational decision.”
"""AI""","""This paper introduces and implements a novel model for selecting target aim points when attacking mobile missile systems within a specified sector region. The model possesses versatility in targeting mobile objects and offers practical utility in operational decision-making processes.”"
"""Human""",“This paper describes the modeling and simulation of the protocol of CCSDS advanced orbiting systems (AOS). The network features modeled in the implementation of CCSDS AOS are to multiplex different kinds of sources into virtual channel data units (VCDUs) in the data processing module The emphasis of this work is placed on the algorithm for com mutating VCDUs into physical channels in the form of continuous data stream. The objectives of modeling CCSDS AOS protocol are to analyze the performance of this protocol when it is used to process various data”
"""AI""","“This paper presents a comprehensive modeling and simulation approach for the CCSDS advanced orbiting systems (AOS) protocol. The implementation focuses on the data processing module, where different sources are multiplexed into virtual channel data units (VCDUs). The key aspect of this research involves developing an algorithm for efficiently converting VCDUs into continuous data streams within physical channels. The primary goal of modeling the CCSDS AOS protocol is to analyze its performance across various data processing scenarios.”"
"""Human""","“Job planning (JP) systems shop-oriented provide a basis for job shop scheduling and control in organizing short-term production activities. This paper presents a method based on timed Petri net (TPN) method that is used to program optimal JP for assembly shop. It includes three parts further. Firstly, an architecture of solutions to JP problems for any kind of shop oriented is presented to define a particular JP for a designated JP problem. Secondly, Petri net model is specified for aircraft-part assembly processes. Finally, algorithms for optimizing generation of dynamic mechanism and a simulating case are then discussed. In comparison with traditional methods such as PERT or CPM, it is obviously convenient for planners or schedulers to schedule and manage assembly processes”"
"""AI""","“This paper introduces a method for job planning (JP) systems in a shop-oriented context, serving as a foundation for effective job shop scheduling and control for short-term production activities. The approach is based on timed Petri nets (TPN), applied to achieve optimal JP in assembly shops. The paper is organized into three main sections. Firstly, a comprehensive solution architecture for JP issues in different shop contexts is outlined, allowing for specific JP solutions to be tailored to designated problems. Secondly, a Petri net model is defined for the assembly processes of aircraft parts. Lastly, algorithms are discussed for dynamically optimizing the generation of mechanisms and a simulated case study is presented. In contrast to conventional methods like PERT or CPM, this approach offers notable advantages for planners and schedulers in terms of managing and scheduling assembly processes with enhanced convenience.”"
"""Human""","“Enterprise management information system (EMIS) in Manufacturing CIMS Integrating Platform (MACIP), refers to a computer system that manages the information for running an enterprise. A typical EMIS consists of a group of closely connected functions such as production planning, material management, accounting, quality management, etc. The EMIS exchanges information with the CAD/CAPP system in the design department, and the shop floor controller (SFC) in the manufacturing department, while the global information system (GIS) of MACIP supplies the mechanism for information sharing within the enterprise. This paper introduces the EMIS model for a typical manufacturing enterprise, then analyses the interface of the EMIS with the CAD/CAPP system and the SFC. A technical scheme for integrating the EMIS with the GIS is given. This scheme considers the integration of some MRPI systems in the market, and adopts advanced industrial standards to ensure its flexibility and reusability.”"
"""AI""","“An Enterprise Management Information System (EMIS) within the Manufacturing Computer-Integrated Manufacturing System (MACIP) framework denotes a computerized system responsible for overseeing enterprise operations. An EMIS encompasses interconnected functions, including production planning, material management, accounting, quality control, and more. It interfaces with the CAD/CAPP system within the design department and the shop floor controller (SFC) in manufacturing, while the global information system (GIS) of MACIP facilitates enterprise-wide information exchange. This paper outlines the EMIS model for a representative manufacturing enterprise, delves into its interface with the CAD/CAPP system and SFC, and proposes a technical approach for integrating EMIS with GIS. This approach accommodates integration with available MRPI systems in the market and adopts advanced industrial standards to ensure adaptability and reusability.”"
"""Human""",“In this paper a wavelet-based fuzzy neural network for interval estimation of processed data with its interval learning algorithm is proposed. It is also proved to be an efficient approach to calculate the wavelet coefficient”
"""AI""","“This paper introduces a novel approach utilizing a wavelet-based fuzzy neural network for interval estimation of processed data, coupled with its corresponding interval learning algorithm. The proposed method is demonstrated to be highly effective in calculating the wavelet coefficient, thus offering a promising avenue for accurate interval estimation in data processing.”"
"""Human""","“This paper proposes a fast and precise de-quantization algorithm based on Chebyshev approximation polynomial for MP3/AAC decoding. Due to the properties of Chebyshev approximation polynomial, compared with any other polynomial of the same degree, the infinity norm of proposed algorithm reaches minimum. In contrast to the conventional method, the average error is reduced from 0.03161 to 0.00051066 at the cost of two additional multiplications. The SNR (signal to noise ratio) for MP3/AAC files is enhanced about 8dB averagely”"
"""AI""","“In this paper, a novel de-quantization algorithm for efficient MP3/AAC decoding is introduced, leveraging the Chebyshev approximation polynomial. This algorithm capitalizes on the unique attributes of the Chebyshev polynomial, resulting in a minimized infinity norm compared to other polynomials of equal degree. In contrast to traditional methods, this approach significantly reduces the average error from 0.03161 to 0.00051066, requiring only two additional multiplications. Consequently, the signal-to-noise ratio (SNR) for MP3/AAC files is enhanced by an average of approximately 8dB.”"
"""Human""","“The intensive research work into Surface Acoustic Wave (SAW) technology over the last decade has introduced many new possibilities to the system designer. Currently most of the major electronic systems firms in the USA and a large number in Europe, have some research effort devoted to SAW. The aim of this paper is to indicate some of the application areas of SAW devices both in the military and civil fields with specific reference to radar and communication applications”"
"""AI""","“In the past decade, there has been significant research focused on Surface Acoustic Wave (SAW) technology, leading to a plethora of opportunities for system designers. Numerous prominent electronic systems companies in the USA and Europe have allocated research resources to explore SAW technology. This paper aims to highlight several potential application areas for SAW devices, encompassing both military and civilian domains, with a particular focus on their relevance to radar and communication applications.”"
"""Human""","“In this paper, two different paradigms to realize a scalable all-optical packet switch with label swapping will be reviewed. The two paradigms are based on wavelength routing switch and space routing switch. All the functions required for switching the packets, namely, the label processor, the label rewriter, and the optical switch, are based on all-optical signal processing with no electronic control. This allows for very fast processing time and potential photonic integration of the systems. We report, for both techniques, experimental results showing the routing operation of the 160-Gb/s packets and beyond. We will discuss and compare both techniques in terms of devices and bit-rate scalability, power consumption, power penalty performance, and cascadability as key parameters for the realization of an all-optical packet switch”"
"""AI""","“This paper presents an examination of two distinct paradigms aimed at achieving a scalable all-optical packet switch through the utilization of label swapping techniques. These two paradigms center around the concepts of a wavelength routing switch and a space routing switch. Importantly, all essential functions required for effective packet switching – including the label processor, label rewriter, and optical switch – are executed using all-optical signal processing, entirely eliminating the need for electronic control. This inherent attribute results in exceptionally fast processing times and holds the potential for seamless photonic integration within the systems. Experimental findings are presented for both paradigms, showcasing successful routing operations for packet speeds exceeding 160 Gb/s. A comprehensive discussion and comparison of both techniques follow, taking into account crucial parameters such as devices and bit-rate scalability, power consumption, power penalty performance, and cascadability – all of which play a pivotal role in the realization of a fully functional all-optical packet switch.”"
"""Human""","“Today's firewalls and security gateways are required to not only block unauthorized accesses by authenticating packet headers, but also inspect flow payloads against malicious intrusions. Deep inspection emerges as a seamless integration of packet classification for access control and pattern matching for intrusion prevention. The two function blocks are linked together via well-designed session lookup schemes. This paper presents an architecture-aware session lookup scheme for deep inspection on network processors (NPs). Test results show that the proposed session data structure and integration approach can achieve the OC-48 line rate (2.5 Gbps) with inline stateful content inspection on the Intel IXP2850 NP. This work provides an insight into application design and implementation on NPs and principles for performance tuning of NP-based programming such as data allocation, task partitioning, latency hiding, and thread synchronization.”"
"""AI""","“Contemporary firewalls and security gateways are faced with the challenge of not only preventing unauthorized access through packet header authentication but also scrutinizing flow payloads to detect malicious intrusions. Deep inspection, which combines packet classification for access control and pattern matching for intrusion prevention, has emerged as an effective approach. This paper introduces an architecture-aware session lookup scheme tailored for deep inspection on network processors (NPs). The proposed scheme tightly integrates packet classification and pattern matching through well-designed session lookup mechanisms. Test results demonstrate that this integrated approach, along with the session data structure, enables achieving OC-48 line rates (2.5 Gbps) with inline stateful content inspection on the Intel IXP2850 NP. This study not only sheds light on the design and implementation of applications on NPs but also provides fundamental principles for enhancing the performance of NP-based programming, encompassing aspects such as data allocation, task distribution, latency reduction, and thread synchronization.”"
"""Human""","“Due to the characteristics of P2P network, the correctness of P2P RFID code resolution service is not guaranteed. In this paper, a novel secure communication mechanism for the P2P RFID code resolution network is proposed based on double random numbers. In this mechanism, the nodes involved in the resolution process generate random numbers, and the random numbers are used for node authentication and message authentication. Based on Kademlia, the proposed secure communication mechanism in the P2P RFID code resolution network is implemented. In experiments, the P2P RFID code resolution network with the proposed secure communication mechanism is not only load balancing, extensible, and node failure tolerant, but also highly efficient in code resolution. At last, the future work is presented.”"
"""AI""","“The inherent characteristics of P2P networks introduce uncertainties in ensuring the accuracy of P2P RFID code resolution services. This paper addresses this challenge by introducing a novel secure communication mechanism for P2P RFID code resolution networks, leveraging the concept of double random numbers. In this approach, participating nodes generate random numbers that serve as a basis for both node authentication and message verification. Building on the Kademlia protocol, the proposed secure communication mechanism is integrated into the P2P RFID code resolution network. Experimental results reveal that the network, augmented with the secure communication mechanism, not only demonstrates attributes such as load balancing, extensibility, and resilience to node failures, but also excels in the efficiency of code resolution. The paper concludes by outlining potential avenues for future research in this domain.”"
"""Human""","“National Surgical Adjuvant Breast and Bowel Project (NSABP) Protocol B-18 was initiated in 1988 to determine whether four cycles of doxorubicin/cyclophosphamide given preoperatively improve survival and disease-free survival (DFS) when compared with the same chemotherapy given postoperatively. Secondary aims included the evaluation of preoperative chemotherapy in downstaging the primary breast tumor and involved axillary lymph nodes, the comparison of lumpectomy rates and rates of ipsilateral breast tumor recurrence (IBTR) in the two treatment groups, and the assessment of the correlation between primary tumor response and outcome. Initially published findings were based on a follow-up of 5 years; this report updates results through 9 years of follow-up. There continue to be no statistically significant overall differences in survival or DFS between the two treatment groups. Survival at 9 years is 70% in the postoperative group and 69% in the preoperative group (P = .80). DFS is 53% in postoperative patients and 55% in preoperative patients (P = .50). A statistically significant correlation persists between primary tumor response and outcome, and this correlation has become statistically stronger with longer follow-up. Patients assigned to preoperative chemotherapy received notably more lumpectomies than postoperative patients, especially among patients with tumors greater than 5 cm at study entry. Although the rate of IBTR was slightly higher in the preoperative group (10.7% versus 7.6%), this difference was not statistically significant. Marginally statistically significant treatment-by-age interactions appear to be emerging for survival and DFS, suggesting that younger patients may benefit from preoperative therapy, whereas the reverse may be true for older patients.”"
"""AI""","“The National Surgical Adjuvant Breast and Bowel Project (NSABP) Protocol B-18 was initiated in 1988 with the goal of determining the potential benefits of administering four cycles of doxorubicin/cyclophosphamide chemotherapy prior to surgery, compared to the same chemotherapy given after surgery. The study aimed to assess improvements in survival and disease-free survival (DFS) outcomes. Secondary objectives included evaluating the effectiveness of preoperative chemotherapy in reducing the size of primary breast tumors and associated axillary lymph nodes, comparing lumpectomy rates and rates of ipsilateral breast tumor recurrence (IBTR) between the two treatment groups, and exploring the relationship between primary tumor response and overall outcome. Initially, findings were reported based on 5 years of follow-up; this updated report extends the results to 9 years of follow-up. No statistically significant differences in survival or DFS were observed between the two treatment groups. After 9 years, survival rates were 70% in the postoperative group and 69% in the preoperative group, while DFS rates were 53% and 55%, respectively. The correlation between primary tumor response and outcome remained statistically significant and strengthened with prolonged follow-up. Patients undergoing preoperative chemotherapy showed a higher rate of lumpectomies compared to postoperative patients, particularly for tumors exceeding 5 cm at study enrollment. Although the rate of IBTR was slightly higher in the preoperative group (10.7% versus 7.6%), this difference was not statistically significant. Additionally, there were indications of statistically significant treatment-by-age interactions for survival and DFS, suggesting potential benefits of preoperative therapy for younger patients and possibly differing effects for older patients.”"
"""Human""","“Wireless sensor networks are expected to find wide ap- plicability and increasing deployment in the near future. In this paper, we propose a formal classification of sensor net- works, based on their mode of functioning, as proactive and reactive networks. Reactive networks, as opposed to passive data collecting proactive networks, respond immediately to changes in the relevant parameters of interest. We also in- troduce a new energy efficient protocol, TEEN (Threshold sensitive Energy Efficient sensor Network protocol) for re- active networks. We evaluate the performance of our proto- col for a simple temperature sensing application. In terms of energy efficiency, our protocol has been observed to out- perform existing conventional sensor network protocols.”"
"""AI""","“Wireless sensor networks are becoming increasingly relevant and widely deployed. This paper introduces a formal classification of sensor networks into two categories: proactive and reactive networks, based on their modes of operation. Proactive networks primarily collect data passively, while reactive networks respond immediately to changes in relevant parameters. A new energy-efficient protocol called TEEN (Threshold-sensitive Energy Efficient sensor Network protocol) is proposed for reactive networks. The protocol's performance is evaluated through a temperature sensing application. Notably, the TEEN protocol exhibits superior energy efficiency compared to existing conventional sensor network protocols.”"
"""Human""","“HERWIG is a general-purpose Monte Carlo event generator, which includes the simulation of hard lepton-lepton, lepton-hadron and hadron-hadron scattering and soft hadron-hadron collisions in one package. It uses the parton-shower approach for initial- and final-state QCD radiation, including colour coherence effects and azimuthal correlations both within and between jets. This article updates the description of HERWIG published in 1992, emphasising the new features incorporated since then. These include, in particular, the matching of first-order matrix elements with parton showers, a more correct treatment of heavy quark decays, and a wide range of new processes, including many predicted by the Minimal Supersymmetric Standard Model, with the option of R-parity violation. At the same time we offer a brief review of the physics underlying HERWIG, together with details of the input and control parameters and the output data, to provide a self-contained guide for prospective users of the program”"
"""AI""","“HERWIG is a versatile Monte Carlo event generator designed to simulate a range of particle scattering scenarios. It encompasses hard interactions involving lepton-lepton, lepton-hadron, and hadron-hadron collisions, as well as soft collisions between hadrons. One of its distinctive features is the application of the parton-shower technique to model both initial and final-state quantum chromodynamics (QCD) radiation, taking into account color coherence effects and azimuthal correlations within and between particle jets. This paper presents an updated description of HERWIG, building upon the original 1992 publication, with a focus on recent enhancements. Notable additions include the incorporation of first-order matrix element matching with parton showers, improved handling of heavy quark decays, and an expanded selection of processes, including those predicted by the Minimal Supersymmetric Standard Model and the possibility of R-parity violation. The article also provides an overview of the underlying physics, details on input and control parameters, and information on output data, offering a comprehensive guide for those looking to utilize the HERWIG program.”"
"""Human""","“Mono- and bimetallic colloidal particles have gained increasing attention in science and application throughout the last several years. In this contribution, we present a synopsis of the wet chemical syntheses of these materials and survey potential applications in catalysis and materials science. Methods for the characterization of these particles and their surfaces are not reviewed here.”"
"""AI""","“This paper offers a comprehensive overview of the growing interest in monometallic and bimetallic colloidal particles, both in scientific research and practical applications. The authors provide a summary of the wet chemical synthesis methods employed for creating these materials, while also highlighting their potential uses in catalysis and materials science. It's important to note that this paper does not delve into the characterization techniques for these particles and their surfaces, focusing instead on the synthesis and applications.”"
"""Human""","“The ways in which the accumulation of mutations might contribute to the process of aging in higher organisms or in individual clones of cells has been discussed at length. No corresponding treatment of the consequences of transcription errors in the translation of the DNA message into RNA and protein sequences seems to be available. Here I show that a consideration of the rate of accumulation of such errors leads to a paradox, the resolution of which may be relevant to the problem of aging, and that there are a number of simple experiments that should decide whether this is the case”"
"""AI""","“This paper addresses the extensive discussion surrounding the impact of mutation accumulation on the aging process in higher organisms or individual cell clones. However, it notes the absence of a corresponding exploration of the effects of transcription errors during the translation of DNA messages into RNA and protein sequences. The author presents findings that highlight a paradox arising from the rate at which these errors accumulate, suggesting potential relevance to the aging phenomenon. The paper proposes that simple experiments could offer insights into whether this connection holds true.”"
"""Human""","“Many of the effects of nitric oxide are mediated by the direct modification of cysteine residues resulting in an adduct called a nitrosothiol. Here, we describe a novel method for detecting proteins that contain nitrosothiols. In this three-step procedure, nitrosylated cysteines are converted to biotinylated cysteines. Biotinylated proteins can then be detected by immunoblotting or can be purified by avidin-affinity chromatography. We include examples of the detection of S-nitrosylated proteins in brain lysates after in vitro S-nitrosylation, as well as the detection of endogenous S-nitrosothiols in selected neuronal proteins”"
"""AI""","“This study introduces a novel technique for identifying proteins containing nitrosothiols, which are adducts formed by the direct modification of cysteine residues through the action of nitric oxide. The method involves a three-step process wherein nitrosylated cysteines are transformed into biotinylated cysteines. Biotinylated proteins can then be detected through immunoblotting or purified using avidin-affinity chromatography. The paper includes instances of detecting S-nitrosylated proteins in brain lysates following in vitro S-nitrosylation, as well as identifying endogenous S-nitrosothiols in specific neuronal proteins.”"
"""Human""",“In this paper we present two hybrid Particle Swarm Optimisers combining the idea of the par- ticle swarm with concepts from Evolutionary Al- gorithms. The hybrid PSOs combine the tradi- tional velocity and position update rules with the ideas of breeding and subpopulations. Both hy- brid models were tested and compared with the standard PSO and standard GA models. This is done to illustrate that PSOs with breeding strate- gies have the potential to achieve faster conver- gence and the potential to find a better solution. The objective of this paper is to describe how to make the hybrids benefit from genetic methods and to test their potential and competetiveness on function optimisation.”
"""AI""",“This paper introduces two hybrid Particle Swarm Optimizers (PSOs) that integrate concepts from Evolutionary Algorithms. These hybrid PSOs combine traditional particle swarm velocity and position update rules with breeding and subpopulation ideas from Evolutionary Algorithms. The study compares these hybrid models with both the standard PSO and standard Genetic Algorithm (GA) models. The purpose is to demonstrate that PSOs with breeding strategies can potentially achieve quicker convergence and discover better solutions. The paper aims to describe the incorporation of genetic methods into the hybrids and evaluate their potential and competitiveness in function optimization.”
"""Human""","“We examine various aspects of the conjectured duality between warped AdS5 geometries with boundary branes and strongly coupled (broken) conformal field theories coupled to dynamical gravity. We also examine compactifications with 5-d gauge fields, in which case the holographic dual is a broken CFT weakly coupled to dynamical gauge fields in addition to gravity. The holographic picture is used to clarify a number of important phenomenological issues in these and related models, including the questions of black hole production, radius stabilization, early universe cosmology, and gauge coupling unification.”"
"""AI""","“This study explores different facets of the proposed duality between warped AdS5 geometries featuring boundary branes and strongly coupled (broken) conformal field theories (CFTs) coupled to dynamic gravity. Additionally, it investigates compactifications involving 5-dimensional gauge fields, resulting in a holographic dual consisting of a broken CFT weakly linked to dynamic gauge fields and gravity. The holographic framework is leveraged to elucidate several crucial phenomenological considerations within these models, encompassing topics such as black hole creation, radius stabilization, early universe cosmology, and gauge coupling unification.”"
"""Human""","“This catalog (published in seven parts, all released on the same day) is based on only the published literature for the Staphylinidae. Of the 32 subfamilies, the following 28 are included herein: Apateticinae, Dasycerinae, Empelinae, Euaesthetinae, Glypholomatinae, Habrocerinae, Leptotyphlinae, Megalopsidiinae, Micropeplinae, Microsilphinae, Neophoninae, Olisthaerinae, Omaliinae, Osoriinae, Oxyporinae, Oxytelinae, Phloeocharinae, Piestinae, Protactinae†, Proteininae, Protopselaphinae, Pseudopsinae, Solieriinae, Staphylininae, Steninae, Tachyporinae, Trichophyinae, and Trigonurinae. The Aleocharinae, Paederinae, Pselphinae, and Scaphidiinae are excluded from this edition of the catalog. References to the original citation or description are given for available family-group, genus-group, and species-group names of both extant and extinct forms. The type genus is cited for each family-group name, the type species for each genus-group name, and the type locality for each species-group name. Where appropriate, all subgenera, subspecies, or synonyms are listed for each valid name. Annotated subsequent references are presented for all names. Distributional summaries are given for each valid taxon. Full bibliographic citations are in Part VII. A short historical review, coauthored with Aleš Smetana, follows the Introduction (Part I), with the main focus on biographical sketches that include many photographs. The goal of this catalog is to summarize the current state of knowledge of the family and to stimulate worldwide monographic studies.”"
"""AI""","“This comprehensive catalog, consisting of seven parts released simultaneously, is exclusively based on the published literature regarding the Staphylinidae family. Encompassing 28 out of the 32 subfamilies, including Apateticinae, Dasycerinae, Empelinae, Euaesthetinae, Glypholomatinae, Habrocerinae, Leptotyphlinae, Megalopsidiinae, Micropeplinae, Microsilphinae, Neophoninae, Olisthaerinae, Omaliinae, Osoriinae, Oxyporinae, Oxytelinae, Phloeocharinae, Piestinae, Protactinae†, Proteininae, Protopselaphinae, Pseudopsinae, Solieriinae, Staphylininae, Steninae, Tachyporinae, Trichophyinae, and Trigonurinae. Notable exceptions are Aleocharinae, Paederinae, Pselphinae, and Scaphidiinae, which are not included in this edition of the catalog. The catalog provides references to the original citations or descriptions for both extant and extinct family-group, genus-group, and species-group names. Each family-group name is paired with its type genus, genus-group names with their type species, and species-group names with their type locality. Additionally, it lists all valid names' subgenera, subspecies, or synonyms, as well as subsequent references and distributional summaries for each taxon. Aiming to capture the present understanding of the family's taxonomy and to stimulate comprehensive studies, this catalog also includes historical insights and biographical sketches coauthored with Aleš Smetana.”"
"""Human""","“Proxy signature is a signature scheme that an original signer delegates his/her signing capability to a proxy signer, and then the proxy signer creates a signature on behalf of the original signer. In this paper we show various attack scenarios against previous proxy signature schemes, which shows that proxy signature schemes should be designed very carefully. Based on these weaknesses, we provide new classifications of proxy signatures; strong vs. weak proxy signatures, designated vs. non-designated proxy signatures, and self-proxy signatures. We construct a simple and efficient strong non-designated proxy signature scheme and apply it to multi-proxy signature when plural delegations of multiple original signers exist. We also show that self-proxy signature can be applied to partially blind signatures.”"
"""AI""","“Proxy signature is a cryptographic scheme that enables an original signer to delegate their signing authority to a proxy signer, who can then sign messages on behalf of the original signer. This paper delves into the vulnerabilities of existing proxy signature schemes, highlighting potential attack scenarios that underscore the importance of careful scheme design. Drawing from these identified weaknesses, the paper introduces new classifications for proxy signatures, categorizing them into strong vs. weak proxy signatures, designated vs. non-designated proxy signatures, and self-proxy signatures. The paper presents a novel, efficient strong non-designated proxy signature scheme, which is extended to multi-proxy signatures for multiple original signers. Furthermore, it demonstrates how self-proxy signatures can be applied to partially blind signatures. The research underscores the need for robust proxy signature designs and contributes new insights to the field.”"
"""Human""","""Foot-and-mouth is one of the world's most economically important livestock diseases. We developed an individual farm–based stochastic model of the current UK epidemic. The fine grain of the epidemiological data reveals the infection dynamics at an unusually high spatiotemporal resolution. We show that the spatial distribution, size, and species composition of farms all influence the observed pattern and regional variability of outbreaks. The other key dynamical component is long-tailed stochastic dispersal of infection, combining frequent local movements with occasional long jumps. We assess the history and possible duration of the epidemic, the performance of control strategies, and general implications for disease dynamics in space and time.”"
"""AI""","“Foot-and-mouth disease ranks among the most economically significant livestock illnesses globally. This study introduces a stochastic model of the ongoing epidemic in the UK, focusing on individual farms. By employing detailed epidemiological data, the model offers exceptional spatiotemporal resolution, exposing infection dynamics. Findings reveal that the spatial arrangement, size, and animal composition of farms contribute to outbreak patterns and regional disparities. Additionally, the model highlights the significance of sporadic, long-distance infection spread, which combines regular local movements with occasional distant transmissions. The research evaluates the epidemic's history and potential duration, appraises the efficacy of control measures, and draws broader insights into the broader implications of disease dynamics across space and time.”"
"""Human""","“The resource-based view can be positioned relative to at least three theoretical traditions: SCP-based theories of industry determinants of firm performance, neo-classical microeconomics, and evolutionary economics. In the 1991 article, only the first of these ways of positioning the resourcebased view is explored. This article briefly discusses some of the implications of positioning the resource-based view relative to these other two literatures; it also discusses some of the empirical implications of each of these different resource-based theories”"
"""AI""","“The resource-based view (RBV) of firm performance can be situated in relation to three main theoretical frameworks: industry determinants of firm performance based on Structure-Conduct-Performance (SCP) theories, neo-classical microeconomics, and evolutionary economics. The 1991 article predominantly delved into the first way of positioning the RBV. However, this present article briefly delves into the implications of placing the RBV within these other two academic contexts. It also examines the empirical ramifications of each distinct RBV theory in relation to these theoretical perspectives.”"
"""Human""","“Osteoinduction is the process by which osteogenesis is induced. It is a phenomenon regularly seen in any type of bone healing process. Osteoinduction implies the recruitment of immature cells and the stimulation of these cells to develop into preosteoblasts. In a bone healing situation such as a fracture, the majority of bone healing is dependent on osteoinduction. Osteoconduction means that bone grows on a surface. This phenomenon is regularly seen in the case of bone implants. Implant materials of low biocompatibility such as copper, silver and bone cement shows little or no osteoconduction. Osseointegration is the stable anchorage of an implant achieved by direct bone-to-implant contact. In craniofacial implantology, this mode of anchorage is the only one for which high success rates have been reported. Osseointegration is possible in other parts of the body, but its importance for the anchorage of major arthroplasties is under debate. Ingrowth of bone in a porous-coated prosthesis may or may not represent osseointegration.”"
"""AI""","“Osteoinduction is the process where osteogenesis, or bone formation, is stimulated. This phenomenon is commonly observed during various bone healing processes. Osteoinduction involves attracting immature cells and encouraging their development into preosteoblasts. In situations like bone healing after fractures, osteoinduction plays a crucial role. On the other hand, osteoconduction refers to the growth of bone on a surface. This is often seen in bone implants, where materials like copper, silver, and bone cement with low biocompatibility exhibit limited or no osteoconduction. Osseointegration, another concept, involves the firm anchoring of an implant through direct contact with bone tissue. While this is particularly successful in craniofacial implantology, its applicability in other parts of the body, especially for major arthroplasties, remains debated. The incorporation of bone within a porous-coated prosthesis may or may not represent osseointegration.”"
"""Human""","“A comprehensive, but simple-to-use software package for executing a range of standard numerical analysis and operations used in quantitative paleontology has been developed. The program, called PAST (PAleontological STatistics), runs on stan- dard Windows computers and is available free of charge. PAST integrates spread- sheet-type data entry with univariate and multivariate statistics, curve fitting, time- series analysis, data plotting, and simple phylogenetic analysis. Many of the functions are specific to paleontology and ecology, and these functions are not found in stan- dard, more extensive, statistical packages. PAST also includes fourteen case studies (data files and exercises) illustrating use of the program for paleontological problems, making it a complete educational package for courses in quantitative methods.”"
"""AI""","“The development of an accessible and user-friendly software package named PAST (PAleontological STatistics) is introduced in this paper. Designed for Windows computers, PAST offers a wide array of standard numerical analysis and operations commonly employed in quantitative paleontology. The program seamlessly combines spreadsheet-style data entry with functionalities for univariate and multivariate statistics, curve fitting, time-series analysis, data visualization, and basic phylogenetic analysis. A notable feature is its inclusion of functions tailored specifically to the fields of paleontology and ecology, which are often absent in more comprehensive statistical software. PAST also includes fourteen case studies, complete with data files and exercises, providing hands-on examples for tackling paleontological problems and making it a comprehensive educational tool for quantitative methods courses.”"
"""Human""","“To assess whether there are universal rules that govern amino acid–base recognition, we investigate hydrogen bonds, van der Waals contacts and water-mediated bonds in 129 protein–DNA complex structures. DNA–backbone interactions are the most numerous, providing stability rather than specificity. For base interactions, there are significant base–amino acid type correlations, which can be rationalised by considering the stereochemistry of protein side chains and the base edges exposed in the DNA structure. Nearly two-thirds of the direct read-out of DNA sequences involves complex networks of hydrogen bonds, which enhance specificity. Two-thirds of all protein–DNA interactions comprise van der Waals contacts, compared to about one-sixth each of hydrogen and water-mediated bonds. This highlights the central importance of these contacts for complex formation, which have previously been relegated to a secondary role. Although common, water-mediated bonds are usually non-specific, acting as space-fillers at the protein–DNA interface. In conclusion, the majority of amino acid–base interactions observed follow general principles that apply across all protein–DNA complexes, although there are individual exceptions. Therefore, we distinguish between interactions whose specificities are ‘universal’ and ‘context-dependent’. An interactive Web-based atlas of side chain–base contacts provides access to the collected data, including analyses and visualisation of the three-dimensional geometry of the interactions.”"
"""AI""","“This study delves into the rules governing amino acid-base recognition within protein-DNA complexes by analyzing 129 such structures. Hydrogen bonds, van der Waals contacts, and water-mediated bonds were examined. DNA-backbone interactions were found to contribute stability, while base interactions displayed correlations with specific amino acid types. This is attributed to the stereochemistry of protein side chains and exposed base edges in DNA structures. Complex networks of hydrogen bonds were prominent in direct DNA sequence readout, enhancing specificity. Van der Waals contacts dominated interactions, followed by hydrogen and water-mediated bonds. Water-mediated bonds typically lack specificity, serving as space-fillers. Overall, general principles underlie most amino acid-base interactions in protein-DNA complexes, though exceptions exist. A web-based atlas provides data access, analyses, and visualization of interaction geometries.”"
"""Human""","“Research on the relation between diabetes mellitus and dementia has produced conflicting results, and the relation has not been investigated among Blacks and Hispanics. In this study, Cox proportional hazards models were used to analyze longitudinal data from 1,262 elderly subjects without dementia at baseline (1991–1996) who were followed for an average of 4.3 years between 1992 and 1997. Outcomes were incident Alzheimer's disease and dementia associated with stroke. The prevalence of diabetes was 20% at baseline. The adjusted relative risk of Alzheimer's disease among persons with diabetes as compared with those without diabetes was 1.3 (95% confidence interval (CI): 0.8, 1.9). The adjusted relative risk for the composite outcome of Alzheimer's disease and cognitive impairment without dementia (without stroke) in subjects with diabetes was 1.6 (95% CI: 1.2, 2.1). The adjusted relative risk of stroke-associated dementia in persons with diabetes was 3.4 (95% CI: 1.7, 6.9). Among Blacks and Hispanics, approximately one third of the risk of stroke-associated dementia was attributable to diabetes (33% (95% CI: 31, 36) and 36% (95% CI: 33, 37), respectively), as compared with 17% (95% CI: 13, 22) among Whites. The finding of an association between diabetes and the composite outcome of Alzheimer's disease and cognitive impairment without dementia (without stroke) is consistent with prior reports of a modest relation between diabetes and Alzheimer's disease.”"
"""AI""","“This study investigates the connection between diabetes mellitus and dementia, particularly among Black and Hispanic populations, where research has been lacking. Longitudinal data from 1,262 elderly subjects without dementia at baseline were analyzed, with an average follow-up of 4.3 years. The adjusted relative risk of Alzheimer's disease for individuals with diabetes was 1.3, while the risk of a composite outcome involving Alzheimer's disease and cognitive impairment without dementia was 1.6. The adjusted relative risk of stroke-associated dementia in those with diabetes was 3.4. Notably, the risk of stroke-associated dementia attributed to diabetes was 33% for Blacks, 36% for Hispanics, and 17% for Whites. This study highlights the potential link between diabetes and different forms of dementia, shedding light on previously unexplored populations.”"
"""Human""","“In a population-based study of 6,386 men and women aged 25–84 years in Tromsø, Norway, in 1994–1995, the authors assessed the age- and sex-specific distribution of the abdominal aortic diameter and the prevalence of and risk factors for abdominal aortic aneurysm. Renal and infrarenal aortic diameters were measured with ultrasound. The mean infrarenal aortic diameter increased with age. The increase was more pronounced in men than in women. The age-related increase in the median diameter was less than that in the mean diameter. An aneurysm was present in 263 (8.9%) men and 74 (2.2%) women (p < 0.001). The prevalence of abdominal aortic aneurysm increased with age. No person aged less than 48 years was found with an abdominal aortic aneurysm. Persons who had smoked for more than 40 years had an odds ratio of 8.0 for abdominal aortic aneurysm (95% confidence interval: 5.0, 12.6) compared with never smokers. Low serum high density lipoprotein cholesterol was associated with an increased risk for abdominal aortic aneurysm. Other factors associated with abdominal aortic aneurysm were a high level of plasma fibrinogen and a low blood platelet count. Antihypertensive medication (ever use) was significantly associated with abdominal aortic aneurysm, but high systolic blood pressure was a risk factor in women only. This study indicates that risk factors for atherosclerosis are also associated with increased risk for abdominal aortic aneurysm”"
"""AI""","“In a population-based study conducted in Tromsø, Norway, involving 6,386 individuals aged 25 to 84 years in 1994-1995, the authors investigated the distribution of abdominal aortic diameter across different age and sex groups, and examined the prevalence and risk factors associated with abdominal aortic aneurysm. Ultrasound measurements were used to assess renal and infrarenal aortic diameters. The study revealed that mean infrarenal aortic diameter increased with age, particularly in men. The prevalence of abdominal aortic aneurysm was higher in men (8.9%) compared to women (2.2%), and increased with age. Interestingly, no individuals below the age of 48 had abdominal aortic aneurysms. Long-term smoking (over 40 years) was linked to an 8-fold increase in the odds of abdominal aortic aneurysm compared to never smokers. Low levels of serum high-density lipoprotein cholesterol, high plasma fibrinogen levels, and low blood platelet counts were also associated with an elevated risk of abdominal aortic aneurysm. Antihypertensive medication use was connected with an increased risk, and high systolic blood pressure was a risk factor primarily among women. This study underscores the connection between risk factors for atherosclerosis and the heightened risk of abdominal aortic aneurysms.”"
"""Human""","“This paper studies the population dynamics of preference traits in a model of intergenerational cultural transmission. Parents socialize and transmit their preferences to their offspring, motivated by a form of paternalistic altruism (“imperfect empathy”). In such a setting we study the long run stationary state pattern of preferences in the population, according to various socialization mechanisms and institutions, and identify sufficient conditions for the global stability of an heterogenous stationary distribution of the preference traits.  We show that cultural transmission mechanisms have very different implications than evolutionary selection mechanisms with respect to the dynamics of the distribution of the traits in the population, and we study mechanisms which interact evolutionary selection and cultural transmission”"
"""AI""","“This study delves into the population dynamics of preference traits within a model of intergenerational cultural transmission. The model considers the transmission of preferences from parents to their offspring, driven by a type of paternalistic altruism known as ""imperfect empathy."" The primary focus lies in analyzing the steady-state distribution of preferences within the population over the long term, under diverse socialization mechanisms and institutions. The paper identifies conditions that ensure the global stability of a varied stationary distribution of preference traits. Importantly, it highlights the stark differences between cultural transmission mechanisms and evolutionary selection mechanisms in shaping the dynamics of trait distribution in the population. Additionally, the study investigates mechanisms that involve an interplay between evolutionary selection and cultural transmission.”"
"""Human""","“This paper focuses on the engineering and computer science aspects of developments, applications, and resources related to particle swarm optimization. Developments in the particle swarm algorithm since its origin in 1995 are reviewed. Included are brief discussions of constriction factors, inertia weights, and tracking dynamic systems. Applications, both those already developed, and promising future application areas, are reviewed. Finally, resources related to particle swarm optimization are listed, including books, Web sites, and software. A particle swarm optimization bibliography is at the end of the paper.”"
"""AI""","“This paper centers on the engineering and computer science dimensions of particle swarm optimization (PSO), encompassing its advancements, applications, and available resources. The evolution of the particle swarm algorithm from its inception in 1995 is examined, touching on topics such as constriction factors, inertia weights, and the tracking of dynamic systems. The study delves into existing applications of PSO and identifies potential areas for its future application. Furthermore, a compilation of resources related to PSO is provided, encompassing books, websites, and software tools. The paper concludes with a bibliography of particle swarm optimization research.”"
"""Human""","“Micromachining and micro-electromechanical system (MEMS) technologies can be used to produce complex structures, devices and systems on the scale of micrometers. Initially micromachining techniques were borrowed directly from the integrated circuit (IC) industry, but now many unique MEMS-specific micromachining processes are being developed. In MEMS, a wide variety of transduction mechanisms can be used to convert real-world signals from one form of energy to another, thereby enabling many different microsensors, microactuators and microsystems. Despite only partial standardization and a maturing MEMS CAD technology foundation, complex and sophisticated MEMS are being produced. The integration of ICs with MEMS can improve performance, but at the price of higher development costs, greater complexity and a longer development time. A growing appreciation for the potential impact of MEMS has prompted many efforts to commercialize a wide variety of novel MEMS products. In addition, MEMS are well suited for the needs of space exploration and thus will play an increasingly large role in future missions to the space station, Mars and beyond.”"
"""AI""","“Micromachining and micro-electromechanical system (MEMS) technologies have opened the door to creating intricate structures, devices, and systems at the micrometer scale. Initially, techniques from the integrated circuit industry were adopted, but unique MEMS-specific micromachining methods have emerged. MEMS offers diverse transduction mechanisms to convert real-world signals between different energy forms, enabling various microsensors, microactuators, and microsystems. Despite partial standardization and evolving MEMS computer-aided design (CAD) tools, intricate and advanced MEMS are being developed. Integrating integrated circuits (ICs) with MEMS can enhance performance, albeit at the cost of higher development expenses, greater complexity, and prolonged development cycles. Recognizing MEMS' potential impact, numerous endeavors are underway to commercialize innovative MEMS products. Additionally, MEMS align well with the needs of space exploration, positioning them to play a significant role in future missions to the space station, Mars, and beyond.”"
"""Human""",“This piper reports upon a programme for the measurement of lightning currents and striking distances on a 60 m research mast. The programme hat been in progress in South Africa for the past five years and the mast is situated in hilly terrain at an altitude of about 1 400 m.”
"""AI""","“This paper reports on a program focused on measuring lightning currents and striking distances using a 60 m research mast. The program has been active in South Africa for the last five years, with the mast located in hilly terrain at an altitude of approximately 1,400 m.”"
"""Human""","“Different meanings may be attached to the term high structure depending on the reference level from which the height is measured. To make this statement more explicit let us consider three specific examples of high structures for which lightning data are available. The first of these is the Empire State Building, New York, which has a height of 380 metres (1250 ft) above ground level, the ground being approximately at sea level. The building is not isolated but is surrounded by other tall buildings, so that in effect there is a grouped complex of high structures with the top of the Empire State Building forming its peak.”"
"""AI""","“The term ""high structure"" can have different meanings depending on the reference level from which the height is measured. To clarify this, let's examine three specific examples of high structures with available lightning data. The first example is the Empire State Building in New York, which has a height of 380 meters (1250 ft) above ground level. The ground level is roughly at sea level. It's important to note that the Empire State Building is not isolated; it is situated amidst other tall buildings. As a result, there exists a cluster of high structures, with the peak of the Empire State Building forming a part of this complex.”"
"""Human""","“The problems associated with high-voltage testing and research, would in most cases be viewed by manufacturers in South Africa, from the standpoints of scope and cost, and of equipment and manpower requirements. In turn these are related to following the needs of existing specifications with regard to routine and type testing of high-voltage equipment. Specifications must of necessity be related to South African conditions and attention is drawn to the need for continual investigation in this respect. As there is always a desire to improve upon methods of testing, research is done on this, and the outcome is various suggested tests, not currently the subject of specifications. Some of these, chiefly concerned with discharge testing, are discussed. Comment is made on existing testing facilities of manufacturers in South Africa. Finally the difficulties in determining requirements of the future are considered.”"
"""AI""","“When addressing the challenges linked to high-voltage testing and research, manufacturers in South Africa typically approach the issues in terms of scope, cost, equipment, and workforce demands. These aspects are closely tied to adhering to established specifications for the routine and type testing of high-voltage equipment. These specifications need to align with the unique conditions present in South Africa, necessitating ongoing exploration in this area. As the goal is to continually enhance testing methods, research is conducted, leading to the proposal of various testing techniques that may not yet be covered by existing specifications. Some of these proposed tests, particularly those related to discharge testing, are discussed. Additionally, the paper touches upon the testing facilities already in place within South African manufacturing companies. Lastly, the paper acknowledges the challenges involved in predicting future requirements in this field.”"
"""Human""","“This paper contains the results of an investigation into the possible suppression of subsynchronous resonance (SSR) instability of the 1 072 MVA nuclear powered turbo-generators to be installed at Koeberg Power Station. The auxiliary excitation control strategy considered in this paper feeds a signal derived from generator speed through a phase advance compensator into the rotary excitation system. The results presented in this paper show that the classical automatic voltage regulator and governor loops are of little use in suppressing subsynchronous resonance, and that the auxiliary excitation control signal is only of limited use. The two main limiting factors are the slow speed of response of the existing conventional exciter and its inadequate ceiling voltage levels”"
"""AI""","“This paper presents the findings of a study aimed at addressing the potential mitigation of subsynchronous resonance (SSR) instability affecting the 1,072 MVA nuclear-powered turbo-generators intended for installation at Koeberg Power Station. The paper focuses on evaluating an auxiliary excitation control strategy that involves introducing a signal derived from the generator's speed, processed through a phase advance compensator, into the rotary excitation system. The study demonstrates that the conventional automatic voltage regulator and governor loops are not significantly effective in suppressing subsynchronous resonance, and that the auxiliary excitation control signal offers only limited benefits. The primary limitations stem from the slow response rate of the existing conventional exciter and its inadequate upper voltage limits.”"
"""Human""","“The paper shows how an extension of analysis employed to determine the radial stress and unbalanced magnetic pull in induction motors(1) will yield all the torque components as well. Corresponding components of unbalanced magnetic pull and torque are compared in tables listing the useful and undesirable outputs of the motor for single sinusoidal excitation. The supply to the motor is assumed to be perfect while the analysis attempts to set out a basis allowing for imperfections in the motor due to its iron circuit, slotting, phase bands, eccentricity and irregularities in the rotor. Measurements taken on two motors are given to show the relative magnitude and influence of these effects on the shaft and output of the machines.”"
"""AI""","“The paper demonstrates an extension of the analysis method previously used to determine radial stress and unbalanced magnetic pull in induction motors. By expanding this approach, the paper is able to derive all torque components as well. The study compares the corresponding components of unbalanced magnetic pull and torque through tables that outline the desirable and undesirable outputs of the motor under single sinusoidal excitation. The analysis assumes an ideal power supply to the motor and aims to provide a framework that accommodates imperfections in the motor arising from factors like its iron circuit, slotting, phase bands, eccentricity, and rotor irregularities. The study includes measurements from two motors to illustrate the relative magnitude and influence of these effects on the shaft and output of the machines.”"
"""Human""","“The paper presents the results of in investigation into the subsynchronous resonance (SSR) behaviour of the 1 072 MVA nuclear powered turbo-generators to be Installed at Koeberg Power Station. A mathematical model describing the dynamics of the shaft system, the synchronous generator and the transmission network is presented. An eigenvalue analysis of a linearized model and the Fourier transformation of some of the time domain waveforms provide insight into the physical mechanism of SSR. The mathematical model is checked by measurements on a laboratory micro-alternator and transmission line simulator system. The results show that unless precautionary steps ere taken, unstable subsynchronous oscillations could occur at Koeberg.”"
"""AI""","“This paper outlines an investigation into the phenomenon of subsynchronous resonance (SSR) in the context of the upcoming installation of 1,072 MVA nuclear-powered turbo-generators at Koeberg Power Station. The study introduces a mathematical model that captures the interactions among the shaft system, synchronous generator, and transmission network. Through eigenvalue analysis of a linearized version of the model and Fourier transformation of selected time domain waveforms, the paper offers insights into the underlying mechanics of SSR. To validate the mathematical model, experiments are conducted using a laboratory micro-alternator and transmission line simulator system. The findings highlight the potential occurrence of unstable subsynchronous oscillations at Koeberg unless preemptive measures are taken.”"
"""Human""",“A general analysis of modulating techniques shows the limitation of present methods of changing the pole number of an induction motor by switching part of the winding. The paper describes extensions which have advantages in the case of 6 pole motors. Experimental results are presented that may be interpreted In terms of the tabulated harmonic components of the mmf distributions end compared with the performance of conventional windings”
"""AI""","“This paper conducts a comprehensive examination of modulating techniques, revealing constraints within current approaches for altering the pole number of an induction motor by selectively activating segments of the winding. The study introduces enhanced methods specifically tailored for 6-pole motors, leveraging advantages in this configuration. Through experimental data, the paper presents findings that can be analyzed using the tabulated harmonic components of the magnetomotive force (mmf) distributions. Comparisons are made between these outcomes and the performance of conventional windings, shedding light on the effectiveness of the proposed extensions.”"
"""Human""","“A linearized mathematical model of a 500 MW suberitical Benton boiler-turbine unit, developed from first principles, it outlined, One hundred and twenty-three linearized boiler-turbine equations describing boiler-turbine operation are manipulated into their 33rd-order linear time-invariant Mate vector form. The boiler-turbine equations are solved by two distinct methods. Open loop transients following various step input disturbances are presented. The conventional controller is compared with state controllers incorporating integral action, designed using linear quadratic optimal control theory and with a proportional-integral output feedback controller which positions the closed loop eigenvalues within a prescribed region in the complex s-plane. A state estimator design technique is given (or implementing those controller designs which required a knowledge of (he entire state vector. The conventional controller is found to be adequate but closed loop dynamic performance can be improved upon by using linear quadratic optimal control theory.”"
"""AI""","“This paper introduces a linearized mathematical model for a 500 MW subcritical Benson boiler-turbine unit, derived from fundamental principles. The model comprises 123 linearized equations that describe the operation of the boiler-turbine system, which are then transformed into a 33rd-order linear time-invariant state vector form. Two different methods are employed to solve the equations, and the paper presents open-loop transients resulting from various step input disturbances. A comparison is made between a conventional controller and state controllers featuring integral action. These state controllers are designed using linear quadratic optimal control theory, as well as a proportional-integral output feedback controller that positions the closed-loop eigenvalues within a predefined region in the complex s-plane. The paper also introduces a technique for designing a state estimator to implement the controller designs that require knowledge of the entire state vector. The results demonstrate that while the conventional controller is sufficient, the dynamic performance of the closed-loop system can be enhanced by leveraging linear quadratic optimal control theory.”"
"""Human""","“The thunderstorm day as a unit of measurement of lightning activity is known to be inadequate, especially for purposes of comparison of the operation of transmission systems in lightning areas; consequently means of measuring the lightning ground flash density directly are being urgently sought and developed. This paper describes a few forms of lightning flash counter, some of which are being considered for international use, and then proceeds to state and derive the theory upon which their successful application depends. The variations of the electrostatic field intensity with time In respect of the leader and return portions of a stroke of lightning are postulated, and the response of RC measuring circuits is discussed, especially in regard to the determination of the effective range. It is concluded that the measurement of the lightning ground flash density can be accomplished by means of lightning flash counters, but the design characteristics have to be carefully considered. The effective range can be determined only by a statistical analysis of observations and further work on this aspect Is proceeding”"
"""AI""","“This paper addresses the inadequacy of using the thunderstorm day as a measurement unit for lightning activity, particularly when comparing the performance of transmission systems in lightning-prone regions. To address this issue, the study explores the development of methods for directly measuring lightning ground flash density. Various forms of lightning flash counters are discussed, with some being considered for potential international use. The paper then delves into the theoretical framework necessary for the successful application of these counters. It examines the temporal variations of electrostatic field intensity during different stages of a lightning stroke, focusing on the leader and return portions. Additionally, the response of RC measuring circuits is analyzed, particularly in relation to determining the effective range of lightning flash counters. The study concludes that measuring lightning ground flash density can be achieved through the use of lightning flash counters, emphasizing the importance of careful design considerations. However, the determination of the effective range requires a statistical analysis of observations, and ongoing research in this area is being conducted. This work contributes to the advancement of methods for quantifying lightning activity and its implications for transmission systems in lightning-prone environments.”"
"""Human""","“The paper describes progress in research and applications in the Chinese CIMS program. For CIMS R&D, the research is application-oriented and the results guide the development of further applications. CIMS applications in China emphasize two aspects. First, CIMS projects are not only technology applications, but are a combination of technology and management related to enterprise reform and business process re-engineering. Second, they are not individual information technology applications, but an integrated application of information technology for the entire system. After describing some test-case at CIMS reference sites, the experience and principles used to guide successful implementation are discussed”"
"""AI""","“This paper provides an overview of the advancements made in research and applications within the context of the Chinese Computer-Integrated Manufacturing System (CIMS) program. The research conducted in the CIMS program is characterized by its application-oriented approach, where the outcomes of research efforts inform and guide the development of practical applications. The paper highlights two key aspects of CIMS applications in China. Firstly, these projects go beyond mere technological implementations, as they involve the integration of technology and management practices aimed at enterprise reform and business process re-engineering. Secondly, these applications are not isolated instances of information technology implementation; rather, they represent a comprehensive integration of information technology across the entire system. The paper goes on to present case studies from CIMS reference sites, illustrating real-world test cases. The insights gained from these experiences, as well as the guiding principles that have facilitated successful implementation, are discussed in the paper, shedding light on the strategies and principles that underlie the effective integration of CIMS applications within Chinese industries.”"
"""Human""","“Frequency synthesis presents one of the most challenging subsystems in a monolithic transceiver implementation. This paper presents a PLL (phase-locked loop) frequency synthesizer based on fractional-N frequency synthesis techniques, employing direct charge injection for Fractional spurious tone reduction and an integrated VCO (voltage-controlled oscillator) with an active inductor resonant tank. A 0.35 μm CMOS process is used for implementation.”"
"""AI""","“This paper addresses the intricate task of frequency synthesis, a demanding subsystem in the design of monolithic transceivers. The authors present a phase-locked loop (PLL) frequency synthesizer that utilizes fractional-N frequency synthesis techniques. The design incorporates direct charge injection to mitigate fractional spurious tone issues, and it integrates a voltage-controlled oscillator (VCO) featuring an active inductor resonant tank. The implementation is carried out using a 0.35 μm CMOS process.”"
"""Human""","“This study investigated the influence that industry structure, and in particular the degree of integration of firms, has on the priority of various factors involved in innovation management. The study has found empirical evidence in support of the proposition, based on the innovation network theory, that industry cooperation is one of the most important factors that drive innovation and diffusion of new technology in a non-integrated industry such as the cellular communications sector of the mobile communications industry. The study has also found evidence supporting the proposition that industry cooperation is not one of the most important innovation support factors in an integrated industry, such as the satellite communications sector. Some of the implications this has for innovation management are also presented”"
"""AI""","“This study delves into the impact of industry structure, particularly the level of firm integration, on the prioritization of factors within innovation management. Drawing from innovation network theory, the research provides empirical substantiation that industry collaboration plays a pivotal role in propelling innovation and the adoption of novel technology within non-integrated sectors, exemplified by the cellular communications domain within the mobile communications industry. Additionally, the study reveals that in integrated sectors, like the satellite communications sector, industry collaboration is not among the foremost innovation support factors. The study's findings carry implications for innovation management strategies.”"
"""Human""","“This paper presents CoScribe, a concept and prototype system for the combined work with printed and digital documents, which supports a large variety of knowledge work settings. It integrates novel pen-and-paper-based interaction techniques that enable users to collaboratively annotate, link and tag both printed and digital documents. CoScribe provides for a very seamless integration of paper with the digital world, as the same digital pen and the same interactions can be used on paper and displays. As our second contribution, we present empirical results of three field studies on learning at universities. These motivated the design of CoScribe and were abstracted to a generic framework for the design of intuitive pen-and-paper user interfaces. The resulting interaction design comprising collaboration support and multiuser visualizations has been implemented and evaluated in user studies. The results indicate that CoScribe imposes only minimal overhead on traditional annotation processes and provides for a more efficient structuring and retrieval of documents”"
"""AI""","“This paper introduces CoScribe, a concept and prototype system designed to facilitate collaborative work with both printed and digital documents across diverse knowledge work contexts. The system incorporates innovative pen-and-paper-based interaction techniques that allow users to collectively annotate, link, and tag both physical and digital documents. CoScribe seamlessly bridges the gap between paper and digital realms, employing the same digital pen and interaction methods for both media. The paper also presents the outcomes of three field studies in educational settings, which informed the development of CoScribe and contributed to the creation of a comprehensive framework for intuitive pen-and-paper user interfaces. The resulting interaction design, encompassing collaboration support and multiuser visualizations, was implemented and assessed through user studies. The findings reveal that CoScribe adds minimal complexity to conventional annotation processes while enhancing the organization and retrieval of documents.”"
"""Human""","“This paper gives a brief review of the research activities on space electronics done in Chinese Academy of Space Technology (CAST), especially in Xi'an Institute of Space Radio Technology (XISRT). XISRT undertakes the development and manufacturing of space borne electronic equipment and their relevant ground TT&C in CAST. This paper deals with satellite communication and remote sensing.”"
"""AI""","“This paper provides a concise overview of the research endeavors in space electronics conducted at the Chinese Academy of Space Technology (CAST), with a particular focus on the Xi'an Institute of Space Radio Technology (XISRT). XISRT is responsible for the design, production, and development of electronic equipment for space applications, including associated ground telemetry, tracking, and command (TT&C) systems within the framework of CAST. The paper's primary emphasis lies on satellite communication and remote sensing technologies.”"
"""Human""","""This paper outlines the rationale behind the construction of a modern outdoor high-voltage research laboratory at a high-altitude site in South Africa. One of the fundamental research objectives at this laboratory is to determine the dielectric properties of air insulation under the prevailing meteorological conditions of reduced air density (6: 0,8 - 0,85). To facilitate the requisite statistical studies the laboratory is equipped with a microcomputer-based data acquisition system for electrical and meteorological data. The instrumentation and computer equipment are described in detail, as are earthing and screening precautions in the laboratory, Current and future research activities are briefly reviewed and some preliminary results are shown”"
"""AI""","“This paper outlines the underlying reasons for the establishment of a modern outdoor high-voltage research laboratory situated at a high-altitude location in South Africa. The laboratory's primary research goal involves investigating the dielectric characteristics of air insulation in the context of the specific meteorological conditions characterized by reduced air density (approximately 0.8 - 0.85 times normal). To facilitate the necessary statistical analyses, the laboratory is equipped with a microcomputer-based data acquisition system designed to capture both electrical and meteorological data. The paper provides a comprehensive description of the instrumentation and computer equipment employed, as well as the protective measures taken regarding grounding and screening within the laboratory environment. Furthermore, the paper briefly outlines current and upcoming research initiatives and offers preliminary findings from the ongoing research activities.”"
"""Human""","“Identification of individuals using iris recognition is an emerging technology. Segmentation of the iris texture from an acquired digital image of the eye is not always accurate - the image contains noise elements such as skin, reflection and eyelashes that corrupt the iris region of interest. An accurate segmentation algorithm must localize and remove these noise components. Texture features are considered in this paper for describing iris and non-iris regions. These regions are classified using the Fisher linear discriminant and the iris region of interest is extracted. Four texture description methods are compared for segmenting iris texture using a region based pattern classification approach: Grey Level Co-occurrence Matrix (GLCM), Discrete Wavelet Transform (DWT), Gabor Fillers (GABOR) and Markov Random Fields (MRF). These techniques are evaluated according to their true and false classifications for iris and non-iris pixels”"
"""AI""","“This paper focuses on the application of iris recognition for individual identification, an emerging technology in biometrics. The accuracy of iris texture segmentation from acquired digital eye images can be compromised due to the presence of noise elements such as skin, reflections, and eyelashes that distort the iris region of interest. To achieve accurate segmentation, effective algorithms are required to localize and eliminate these noise components. The study explores texture features for describing both iris and non-iris regions. Utilizing the Fisher linear discriminant, the iris region of interest is isolated following region-based pattern classification. The paper compares four texture description methods – Grey Level Co-occurrence Matrix (GLCM), Discrete Wavelet Transform (DWT), Gabor Filters (GABOR), and Markov Random Fields (MRF) – for segmenting iris texture. The evaluation is based on their ability to correctly classify iris and non-iris pixels, considering true and false classifications.”"
"""Human""","“The basic concepts and models of weapon-target assignment (WTA) are introduced and the mathematical nature of the WTA models is also analyzed. A systematic survey of research on WTA problem is provided. The present research on WTA is focused on models and algorithms. In the research on models of WTA, the static WTA models are mainly studied and the dynamic WTA models are not fully studied in deed. In the research on algorithms of WTA, the intelligent algorithms are often used to solve the WTA problem. The small scale of static WTA problems has been solved very well, however, the large scale of dynamic WTA problems has not been solved effectively so far. Finally, the characteristics of dynamic WTA are analyzed and directions for the future research on dynamic WTA are discussed.”"
"""AI""","“The basic concepts and models of weapon-target assignment (WTA) are introduced and the mathematical nature of the WTA models is also analyzed. A systematic survey of research on WTA problem is provided. The present research on WTA is focused on models and algorithms. In the research on models of WTA, the static WTA models are mainly studied and the dynamic WTA models are not fully studied in deed. In the research on algorithms of WTA, the intelligent algorithms are often used to solve the WTA problem. The small scale of static WTA problems has been solved very well, however, the large scale of dynamic WTA problems has not been solved effectively so far. Finally, the characteristics of dynamic WTA are analyzed and directions for the future research on dynamic WTA are discussed.”"
"""Human""",“The electrical research division of the Electricity Supply Commission is concerned with investigating engineering problems encountered by the main engineering departments. The work of the division is indicated by describing briefly a number of current investigations.”
"""AI""",“The Electrical Research Division of the Electricity Supply Commission is dedicated to addressing engineering challenges faced by the primary engineering departments. The division's focus and activities are highlighted through a succinct overview of ongoing investigations.”
"""Human""","“This paper gives a brief review of the research activities on space electronics done in Chinese Academy of Space Technology (CAST), especially in Xi'an Institute of Space Radio Technology (XISRT). XISRT undertakes the development and manufacturing of space borne electronic equipment and their relevant ground TT&C in CAST. This paper deals with satellite communication and remote sensing”"
"""AI""","“This paper provides a concise overview of the research endeavors in the field of space electronics conducted at the Chinese Academy of Space Technology (CAST), with a particular emphasis on the Xi'an Institute of Space Radio Technology (XISRT). XISRT is actively engaged in the design, production, and ground operations of electronic systems for space missions within CAST. The paper's focus centers on satellite communication and remote sensing applications.”"
"""Human""","“For a convolutional encoding and Viterbi decoding system, two insertion/deletion/substitution (IDS) error correcting techniques are presented in this paper. In the first means, by using the pinned convolutional codes, a rate compatible encoding system can adapt the transmission according to the state of the channel having IDS errors. In the second means, a convolutional encoded sequence is decoded by a modified Viterbi decoding algorithm using the Levenshtein distance metric, in which the IDS errors can be corrected at the same time”"
"""AI""","“This paper introduces two insertion/deletion/substitution (IDS) error correction methods for a convolutional encoding and Viterbi decoding system. The first approach employs pinned convolutional codes to achieve rate-compatible encoding, allowing the system to adapt transmission based on the channel's IDS error state. The second method utilizes a modified Viterbi decoding algorithm that employs the Levenshtein distance metric for decoding a convolutionally encoded sequence. This approach enables simultaneous correction of IDS errors during decoding.”"
"""Human""","“The paper demonstrates the advantages arising from the simultaneous assessment of radio frequency Micro-Electromechanical Systems capacitive switches and metal-insulator-metal capacitors. The application of simple capacitive-voltage characteristics and thermally stimulated depolarization current methods, respectively, allow the discrimination of the contribution of injected charges induced space charge polarization and the dipolar polarization. The proposed method allows the direct determination of device degradation sources and can be used for the improvement of dielectric material”"
"""AI""","“This paper showcases the benefits derived from the concurrent evaluation of radio frequency Micro-Electromechanical Systems (MEMS) capacitive switches and metal-insulator-metal capacitors. Through the utilization of straightforward capacitive-voltage characteristics and thermally stimulated depolarization current techniques, the paper successfully distinguishes the influence of injected charges, induced space charge polarization, and dipolar polarization. This innovative approach facilitates the direct identification of degradation sources within the devices and holds the potential to enhance dielectric material performance and design improvements.”"
"""Human""",“The use of FDTD method has been investigated and applied for the calculation of longitudinal induced voltage caused by the high power overhead line into line-side signalling cables. FDTD has become a very important tool in the modelling of electro-magnetic field simulations. This paper has tried to show its effective application in railway environment.”
"""AI""","“The utilization of the Finite-Difference Time-Domain (FDTD) method has been explored and implemented to compute the longitudinal induced voltage resulting from high-power overhead lines affecting line-side signalling cables. The FDTD technique has emerged as a crucial resource for conducting electromagnetic field simulations. This paper demonstrates its effective application within the railway environment, particularly highlighting its significance for assessing induced voltage in such scenarios.”"
"""Human""","“For a single antenna channel, Shannon's classical capacity formula predicts a capacity increase of approximately 1 bit/cycle for every 3 dB increase in signal to noise ratio (SNR). Consequently in multiple antenna arrays, with n antenna elements, the increase in capacity can be in the order of the number of antenna elements, n more bits/cycle for each 3dB increase in SNR. This tremendous gain in capacity exhibited by MIMO channels over single antenna systems illustrates the enormous potential in wireless communications systems. This paper presents, for an indoor multielement antenna system, a geometric model incorporating local scatterers around the fixed receiver and transmitter. This model considers uniform scattering at the transmitter and non-isotropic scattering using the von Mises pdf at the receiver. The derived correlation function includes some key elements, such as antenna element spacing, degree of scattering, angle of arrival at user and array configuration in MIMO channels. The capacity of the MIMO fading channel with varying number of antenna elements, the degree of isotropic scattering at receiver, the spacing between the antenna elements and the effect of varying signal to noise ratio are investigated and presented for the chosen model.”"
"""AI""","“Shannon's classic capacity formula predicts that a single antenna channel's capacity increases by around 1 bit/cycle for every 3 dB rise in signal-to-noise ratio (SNR). Consequently, in multiple antenna arrays with n antenna elements, the capacity gain could potentially be on the order of n more bits/cycle for each 3 dB SNR increase. This remarkable capacity improvement exhibited by Multiple-Input Multiple-Output (MIMO) channels compared to single antenna systems underscores the vast potential in wireless communication. This paper introduces a geometric model for an indoor multielement antenna system that incorporates local scatterers around the fixed receiver and transmitter. The model considers uniform scattering at the transmitter and non-isotropic scattering using the von Mises probability density function at the receiver. The resulting correlation function includes crucial factors like antenna element spacing, scattering degree, angle of arrival at the user, and array configuration in MIMO channels. The paper investigates and presents the capacity of the MIMO fading channel under varying conditions such as the number of antenna elements, receiver's scattering isotropy, antenna element spacing, and changes in SNR for the specified model.”"
"""Human""","“Distance Preserving Mappings is a relatively new technique finding its way to a wide use in the coding field. New codes mapping convolutional code outputs onto spectral nulls codewords have the purpose of generating a code with a certain spectrum specification and error correction capabilities, which can be exploited by the Viterbi decoder. Combining this class of new codes with an Orthogonal Frequency Division Multiplexing modulation scheme is actually a tool to reduce Electromagnetic Compatibility Interference in selected subbands or to cancel the narrowband interference potentially caused by, amongst others, high frequency radio transmissions. Taking info consideration the fact that a narrow band noise source is almost similar to a modulated orthogonal frequency division multiplexing carrier, and also the periodicity of the nulls in our spectrum, we are able to cancel the narrowband signal and its sideband interferences without using traditional techniques of cancellation, which is based on filtering. Shaping our spectrum at the transmitter is a technique, which can be exploited at the receiver to suppress any narrowband signal interference, which is regarded as noise. We will present in this paper the technique of cancellation and the coding gain that can be obtained when comparing to the uncoded data.”"
"""AI""","“Distance Preserving Mappings (DPM) is an emerging technique gaining traction in the coding field. This method involves generating new codes that map convolutional code outputs onto spectral nulls codewords, resulting in codes with specific spectrum specifications and error correction capabilities that can be leveraged by the Viterbi decoder. When combined with Orthogonal Frequency Division Multiplexing (OFDM) modulation, these codes become a powerful tool to mitigate Electromagnetic Compatibility Interference within chosen subbands or eliminate narrowband interference potentially stemming from high-frequency radio transmissions. By considering the resemblance between a narrowband noise source and a modulated OFDM carrier and exploiting the periodicity of nulls in the spectrum, it becomes possible to cancel out narrowband signals and their sideband interferences without resorting to conventional filtering-based cancellation techniques. Transmitting data with a shaped spectrum allows for interference suppression at the receiver by treating narrowband signal interference as noise. In this paper, we present the cancellation technique and the coding gain achievable when compared to uncoded data.”"
"""Human""","“This paper presents design considerations for a low altitude long endurance solar powered Unmanned Aerial Vehicle. The considerations addressed include the determination of the available solar power, the design of the Unmanned Aerial Vehicle wing for long endurance flights and the determination of the electrical power and energy balance of the Unmanned Aerial Vehicle. Using these considerations, a solar powered Unmanned Aerial Vehicle was designed and a long endurance flight simulated. Based on the simulation results it is in principle possible to design and build a solar powered low altitude long endurance Unmanned Aerial Vehicle”"
"""AI""","“This paper delves into the intricacies of designing a solar-powered Unmanned Aerial Vehicle (UAV) tailored for extended flight duration at low altitudes. Key design aspects are meticulously examined, encompassing the assessment of available solar power, the formulation of a UAV wing design conducive to prolonged flight, and the meticulous calculation of the UAV's electrical power and energy equilibrium. Armed with these insights, a solar-powered UAV is meticulously designed, and its potential for enduring long flights is simulated. The simulation outcomes affirm the feasibility of crafting a solar-powered, low-altitude UAV optimized for enduring extended missions.”"
"""Human""","“This paper presents an adaptive hybrid Chase-like algorithm for Reed-Solomon codes, which is based on the list decoding algorithm. The adaptive hybrid algorithm is based on the reliability threshold to exclude the more reliable bits from being processed by the list decoding algorithm and reduce the complexity of the hybrid algorithm. Simulation results show that the decoding complexities of the adaptive hybrid algorithm for both (15.7) and (31.21) Reed-Solomon codes are almost the same as those of the list decoding algorithm (without Chase algorithm) at high signal-to-noise ratios, but there is a significant improvement in FER performance.”"
"""AI""","“This paper introduces an innovative adaptive hybrid Chase-like algorithm tailored for Reed-Solomon codes, founded upon the list decoding principle. The approach adapts by utilizing a reliability threshold to eliminate more dependable bits from undergoing list decoding, thereby streamlining the complexity of the hybrid algorithm. Extensive simulations demonstrate that the adaptive hybrid algorithm's decoding complexities closely align with those of the list decoding algorithm (excluding Chase) for both (15.7) and (31.21) Reed-Solomon codes under high signal-to-noise ratios. Importantly, there's a notable enhancement in the FER (Frame Error Rate) performance, signifying the effectiveness of the proposed approach.”"
"""Human""","“HIV Therapy is considered to be effective if it can reduce the viral load by 90% in less than 8 weeks and continue to suppress it to below 50 copies per mL of plasma in less than 6 months [1]. This response time to therapy as well as the time to suppress the viral load can be estimated towards the development of an interruptible control strategy for the chemotherapy of HIV. The time estimates are parameter dependent, and as such, will vary from one individual to the other. These time estimates therefore, will be useful and can be used to aid clinicians with therapy and measurement scheduling. The model used in this paper has a total of ten parameters, but in order to determine these time estimates, one only needs to know estimates for the death rate constants for the virus and actively infected CD4+ T cells, the combined drug efficacy, as well as the reproductive ratio”"
"""AI""","“Effective HIV therapy is gauged by its ability to curtail viral load by 90% within less than 8 weeks and then maintain suppression below 50 copies per mL of plasma within 6 months [1]. These response and suppression durations are pivotal in devising an interruptible control plan for HIV chemotherapy. The timelines are contingent on various parameters, making them subject to interindividual variability. As such, these estimations can be invaluable for clinicians in managing therapy and scheduling measurements. The model employed in this study comprises ten parameters, yet to determine these timelines, knowledge of the virus and actively infected CD4+ T cell death rate constants, combined drug efficacy, and the reproductive ratio suffices.”"
"""Human""","“Rapid Single Flux Quantum (RSFQ) logic is a digital circuit technology that in recent years has presented itself as an alternative to semiconductors in the application of ultra high speed, very low power applications. The optimal timing of digital circuits operating at hundreds of Gigahertz is still a complex problem for both RSFQ and semiconductor technologies. The fact that most RSFQ gates require a clock signal to function makes this even more complex. Various RSFQ liming schemes have been adapted from semiconductor design methodologies, and some have been designed specifically for RSFQ. Currently, synchronous clocking schemes outperform other schemes, but with the scale of RSFQ circuits ever increasing, the proper use of timing schemes are becoming more crucial. This paper describes a new asynchronous self-timing scheme where the details of clock distribution and clocking are built into the logic gates. Tests were done on the newly developed asynchronous logic-gales and a asynchronous full adder was implemented and tested”"
"""AI""","“Rapid Single Flux Quantum (RSFQ) logic, an emerging digital circuit technology, offers an alternative to semiconductors, particularly for ultra-high-speed and ultra-low-power applications. The precise timing of digital circuits operating at hundreds of gigahertz remains a complex challenge for both RSFQ and semiconductor technologies. The need for clock signals in most RSFQ gates adds to this complexity. Various timing schemes, adapted from semiconductor design methods or tailored for RSFQ, have been employed. Currently, synchronous clocking yields better results, but as RSFQ circuits grow larger, the effective utilization of timing becomes increasingly critical. This paper introduces a novel asynchronous self-timing approach wherein clock distribution and timing details are embedded within the logic gates. The study involves testing the new asynchronous logic gates, with an implemented and tested asynchronous full adder serving as an example.”"
"""Human""","“In this paper the torque performance of optimally designed three- and live-phase reluctance synchronous machines with different normal laminated rotor structures are studied. Both the round rotor with internal flux barriers and salient-pole rotor with no internal flux barriers but only cut-outs are investigated. The effect on the torque performance by adding third harmonic current component to the phase currents in a live-phase reluctance synchronous machine is also studied. The magnetostatic finite-element field solution with skew taken into account is used directly by an optimisation algorithm to optimise in multi-dimensions the design of the machines under same copper losses and volume. It is found that the torque increase due to third harmonic current injection is only 4% in the case of the live-phase machine with salient-pole rotor; the three-phase machine with round, internal-flux-barrier rotor is shown to outperform this machine in terms of torque by 28%. The measured torque results of the three-phase machine with round, internal-flux-barrier rotor are presented and compared with calculated results”"
"""AI""","“This paper investigates the torque performance of optimally designed three- and five-phase reluctance synchronous machines featuring various normal laminated rotor structures. Both a round rotor with internal flux barriers and a salient-pole rotor with cut-outs but no internal flux barriers are analyzed. The impact of introducing a third harmonic current component to the phase currents in a live-phase reluctance synchronous machine is also examined. The study employs a magnetostatic finite-element field solution, accounting for skew, which is used within an optimization algorithm to simultaneously optimize the design of the machines while keeping copper losses and volume constant. The findings reveal that the addition of a third harmonic current component leads to a mere 4% increase in torque for the live-phase machine with a salient-pole rotor. Moreover, the three-phase machine with a round rotor featuring internal flux barriers surpasses this machine's torque performance by 28%. The paper also presents and compares the measured torque results of the three-phase machine with the round rotor, internal-flux-barrier design with the calculated outcomes.”"
"""Human""","“The performance of turbo codes has been shown to be near the theoretical limit in the additive while Gaussian noise channel. By using orthogonal signaling, which allows for bandwidth expansion, the performance of the turbo coding scheme can be improved even further. Since this is a low-rate code, the code is mainly suitable for spread-spectrum modulation applications. In classical turbo codes the frame length is set equal to the interleaver size, however, the codeword distance spectrum of turbo codes improves with an increasing interleaver size. It has been reported that by using repetition and puncturing the performance of turbo codes can be improved. Repeat-Punctured Turbo Codes has shown a significant increase in performance at moderate to high signal-to-noise ratios. In this paper, we study the use of orthogonal signaling and parallel concatenation together with repetition and puncturing to improve the performance of superorthogonal convolutional turbo codes for reliable and effective communications. Simulation results for the additive white Gaussian noise channel are presented together with analytical upper bounds, which have been derived using transfer function bounding techniques”"
"""AI""","“This paper explores the enhancement of turbo code performance beyond the theoretical limit in the presence of additive white Gaussian noise. By leveraging orthogonal signaling, which permits bandwidth expansion, the efficiency of the turbo coding scheme can be further heightened. Given its low-rate nature, this code is particularly suitable for spread-spectrum modulation applications. While conventional turbo codes set the frame length equal to the interleaver size, the codeword distance spectrum of these codes actually improves as the interleaver size increases. Previous studies have demonstrated that employing repetition and puncturing can boost the performance of turbo codes. Specifically, Repeat-Punctured Turbo Codes have exhibited significant performance gains, particularly at moderate to high signal-to-noise ratios. This paper delves into the combined use of orthogonal signaling, parallel concatenation, repetition, and puncturing to amplify the performance of superorthogonal convolutional turbo codes for robust and efficient communications. The research includes simulation results for the additive white Gaussian noise channel, along with analytically derived upper bounds achieved through transfer function bounding techniques.”"
"""Human""","“This paper proposes a reduced complexity Maximum-Likelihood (ML) decoding Algorithm for Linear Block Codes based on the Kaneko decoder and incorporating ruling out conditions for useless iteration steps. The proposed decoding scheme is evaluated over the Additive White Gaussian Noise (AGWN) channel using Binary Phase Shift Key (BPSK) signalling by simulation. Simulations results show that for negligible performance loss, there is significant reduction in the complexity of decoding.”"
"""AI""","“This paper introduces a novel approach to enhance the efficiency of Maximum-Likelihood (ML) decoding for Linear Block Codes by leveraging the Kaneko decoder and integrating ruling out conditions to eliminate unnecessary iteration steps. The proposed decoding algorithm is assessed under the influence of Additive White Gaussian Noise (AWGN) in the context of Binary Phase Shift Key (BPSK) signaling through simulation. The simulation outcomes reveal that while maintaining virtually no degradation in performance, there is a notable reduction in the complexity of the decoding process.”"
"""Human""","“The need for greater bandwidth and higher data rates will inevitably lead to the usage of the upper portion of the k-band in terrestrial communication systems. The frequency band from 18 GHz to 20 GHz has been identified as a suitable candidate for short-haul line-of sight links. As a consequence, a 6.73 km terrestrial LOS link centered at 19.5 GHz has recently been established between the Howard College and Westville campuses of the University of KwaZulu-Natal (UKZN), with the primary objective to investigate the effect of rain attenuation over the link. Multipath propagation and the resultant frequency selective fading is another major problem afflicting such LOS links. In order to evaluate the effect of multipath propagation over the link statistical models of the channel transfer functions are required. In this paper the research into the simulation and statistical modeling of the multipath propagation for the above-mentioned LOS link will be discussed”"
"""AI""","“The demand for increased bandwidth and higher data rates is driving the exploration of the upper spectrum of the k-band in terrestrial communication systems. Specifically, the frequency range between 18 GHz and 20 GHz is being considered for short-distance line-of-sight (LOS) links. To this end, a terrestrial LOS link spanning 6.73 km and centered at 19.5 GHz has been established between the University of KwaZulu-Natal's (UKZN) Howard College and Westville campuses. The primary aim of this link is to investigate the impact of rain attenuation. Additionally, multipath propagation and the resulting frequency-selective fading pose significant challenges for such LOS links. To assess the effects of multipath propagation on the link, statistical models of channel transfer functions are essential. This paper focuses on the research related to the simulation and statistical modeling of multipath propagation for the mentioned LOS link.”"
"""Human""","“The multicell converter topology possesses a natural voltage balancing property. This paper uses models obtained previously to perform a stability analysis of multicell converters. It uses a generic model in steady-state- and time constant analyses to determine the stability of multicell converters. In this paper stability refers to balanced cell capacitor voltages, whereas instability refers to unbalanced cell capacitor voltages. The stability analysis is performed for both sinusoidal modulation as well as fixed duty-cycle modulation. The model used in the stability analyses in this paper is valid for any modulation method. The conditions that lead to stability of the cell capacitor voltages as well as that leading to instability are presented. Theoretical results are included to verify the presented stability analyses and properties”"
"""AI""","“The inherent voltage balancing capability of the multicell converter topology is explored in this study. Drawing from previously obtained models, a comprehensive stability analysis of multicell converters is conducted. The analysis employs a generic model to investigate both steady-state and time constant behaviors, aiming to ascertain the stability of multicell converters. Stability in this context pertains to the attainment of balanced cell capacitor voltages, while instability refers to unbalanced voltages. The assessment encompasses both sinusoidal modulation and fixed duty-cycle modulation scenarios. Notably, the model employed in these stability analyses holds relevance across various modulation methods. The paper outlines the conditions that lead to voltage stability, as well as those that result in instability. Theoretical outcomes are presented to validate the proposed stability analyses and their associated properties.”"
"""Human""","“As the core algorithm and the most time consuming part of almost every modern network intrusion management system (NIMS), string matching is essential for the inspection of network flows at the line speed. This paper presents a memory and time efficient string matching algorithm specifically designed for NIMS on commodity processors. Modifications of the Aho-Corasick (AC) algorithm based on the distribution characteristics of NIMS patterns drastically reduce the memory usage without sacrificing speed in software implementations. In tests on the Snort pattern set and traces that represent typical NIMS workloads, the Snort performance was enhanced 1.480/0–200/0 compared to other well-known alternatives with an automaton size reduction of 4.86–6.11 compared to the standard AC implementation. The results show that special characteristics of the NIMS can be used into a very effective method to optimize the algorithm design”"
"""AI""","“This paper focuses on the significance of string matching algorithms in network intrusion management systems (NIMS), which play a crucial role in inspecting network flows at line speed. The paper introduces a novel, memory-efficient, and time-effective string matching algorithm tailored specifically for NIMS on commonly used processors. By leveraging the distribution characteristics of NIMS patterns, modifications are made to the Aho-Corasick (AC) algorithm, resulting in significantly reduced memory consumption without compromising speed in software implementations. Through tests conducted on the Snort pattern set and representative NIMS workloads, the performance of Snort was notably enhanced by 1.480/0–200/0 in comparison to other renowned alternatives, while simultaneously achieving an automaton size reduction of 4.86–6.11 when compared to the standard AC implementation. These outcomes underscore the potential of harnessing unique NIMS characteristics to optimize algorithm design effectively.”"
"""Human""","“Embedded fuzzy controllers enhance and complement existing conventional controllers with intelligent functions derived from human control operator experience. Whenever the system to be controlled is highly nonlinear, parameters time-variant with, and certain features are too difficult or cumbersome to model, fuzzy controllers circumvent the difficulties associated with conventional mathematical modelling. This paper discusses the recently developed embedded fuzzy controller of Spoomet's Class 6E/6E1 3kV DC locomotive, which was developed to prevent the damage during starting as a result of uncontrolled wheel slip and incorrect taking up of slack. The results presented herein were achieved by using locomotive and load models and verified by both simulation and exhaustive in-service testing on a real locomotive.”"
"""AI""","“Embedded fuzzy controllers serve as valuable additions to conventional controllers by incorporating intelligent functions based on human control operator expertise. These controllers are particularly useful when dealing with highly nonlinear systems, time-varying parameters, and complex features that are challenging to model using traditional mathematical methods. This paper explores the application of an embedded fuzzy controller in Spoomet's Class 6E/6E1 3kV DC locomotive. The controller was designed to prevent damage during locomotive starting caused by uncontrolled wheel slip and improper slack management. The outcomes discussed in the paper were achieved through the utilization of locomotive and load models, and these results were validated using simulations and exhaustive real-world testing on an actual locomotive.”"
"""Human""","“This paper presents a model predictive control (MPC) scheme for the retrieval of an electrodynamic tethered sub-satellite in an inclined orbit. The scheme accounts for in-plane as well as out-of-plane motions. The control action is realized by adjusting only the tensional and electro-dynamic forces in the tether. Even though the proposed feedback law is not analytically explicit, it is easy to determine it by using a rapid re-computation of open-loop optimal control online and updating the control command at a fixed sampling interval. For each online step, the open-loop optimal control problem is solved by discretizing the continuous control problem first and then numerically solving the resulting large-scale optimization problem via nonlinear programming. The design of the feedback controller is based on a simple model which enjoys the advantage of low computational effort required for optimization, whereas the effectiveness and robustness of the proposed strategy are demonstrated by using a multi-body dynamics model of much higher fidelity.”"
"""AI""","“This paper introduces a model predictive control (MPC) approach for maneuvering an electrodynamic tethered sub-satellite in an inclined orbit. The MPC scheme accommodates both in-plane and out-of-plane motions of the sub-satellite. The control action involves adjusting the tensional and electrodynamic forces within the tether. While the feedback law is not explicitly formulated, it can be readily determined through rapid re-computation of open-loop optimal control in real-time and updating control commands at fixed intervals. This involves discretizing the continuous control problem and then solving the resulting optimization problem using nonlinear programming. The feedback controller is designed based on a simplified model to ensure low computational complexity for optimization. The effectiveness and robustness of this strategy are demonstrated using a higher fidelity multi-body dynamics model.”"
"""Human""","“The traffic performance of urban expressway is subject to non-recurring and recurring events, which may cause heavy congestion and vehicles long queuing on ramps. The low performance may bring more traffic delay to the whole network of urban road. This paper presents a new method, the joint control of variable speed control and on-ramp metering, which attempts to improve the level of traffic operations on urban expressway. By analyzing traffic flow on urban expressway, an optimum control strategy of variable speed and on-ramp metering is established in the paper”"
"""AI""","“This paper addresses the traffic performance challenges faced by urban expressways due to both non-recurring and recurring events, leading to severe congestion and extended vehicle queues on ramps. Such performance issues can cause significant traffic delays across the entire urban road network. To mitigate these problems, the paper introduces a novel approach called ""joint control of variable speed control and on-ramp metering."" By thoroughly analyzing the traffic flow dynamics on urban expressways, the study establishes an optimized control strategy that combines variable speed control and on-ramp metering. This integrated approach aims to enhance the overall traffic operations and efficiency of urban expressways, effectively managing congestion and minimizing delays.”"
"""Human""","“This paper proposes an artificial neural network maximum power point tracker (M PPT) for solar electric vehicles. The MPPT is based on a highly efficient boost converter with insulated gate bipolar transistor (IGBT) power switch. The reference voltage for MPPT is obtained by artificial neural network (ANN) with gradient descent momentum algorithm. The tracking algorithm changes the duty-cycle of the converter so that the PV-module voltage equals the voltage corresponding to the MPPT at any given insolation, temperature, and load conditions. For fast response, the system is implemented using digital signal processor (DSP). The overall system stability is improved by including a proportional-integral-derivative (PID) controller, which is also used to match the reference and battery voltage levels. The controller, based on the information supplied by the ANN, generates the boost converter duty-cycle. The energy obtained is used to charge the lithium ion battery stack for the solar vehicle. The experimental and simulation results show that the proposed scheme is highly efficient”"
"""AI""","“In this paper, an innovative approach is introduced for enhancing the efficiency of solar electric vehicles through the integration of an artificial neural network maximum power point tracker (MPPT). This MPPT system is designed around a high-efficiency boost converter featuring an insulated gate bipolar transistor (IGBT) power switch. The core of the MPPT's functionality lies in an artificial neural network (ANN) combined with the gradient descent momentum algorithm, which calculates the reference voltage. This reference voltage guides the MPPT algorithm to adjust the converter's duty-cycle, ensuring that the photovoltaic (PV) module voltage aligns with the MPPT under varying insolation, temperature, and load conditions. To achieve swift response times, a digital signal processor (DSP) is employed. To bolster system stability, a proportional-integral-derivative (PID) controller is incorporated, serving the dual purpose of matching the reference and battery voltage levels. The controller, drawing insights from the ANN, manages the duty-cycle of the boost converter, harnessing energy to charge the lithium-ion battery stack for the solar vehicle. The paper showcases experimental and simulation results affirming the efficiency and effectiveness of the proposed system.”"
"""Human""","“Virtual enterprises (VEs) are the most promising structure for future enterprises. The cooperative environment of VEs is investigated in this paper. The cooperation in a VE is a process and the depth of the cooperation and the information transparency change in different phases of the cooperation. A tri-layered cooperative structure pattern is discussed with a tri-layered cooperative process model which includes the conceptual cooperation model, the detailed cooperation model, and the executive cooperation model. Analysis of the tri-layered process modeling provides a foundation for the cooperation environment”"
"""AI""","“This paper delves into the realm of virtual enterprises (VEs), which are poised to be the cornerstone of future enterprise structures. The study focuses on the collaborative landscape within VEs. Cooperation within a VE is regarded as a dynamic process, with the extent of collaboration and the level of information transparency undergoing fluctuations across various cooperation phases. A tri-layered cooperative structure pattern is examined, accompanied by a tri-layered cooperative process model. This model encompasses the conceptual cooperation model, the detailed cooperation model, and the executive cooperation model. Through an in-depth analysis of this tri-layered process model, the paper establishes a solid groundwork for understanding and navigating the cooperation dynamics within the VE environment.”"
"""Human""","“Binary optics, where the optical element can be fabricated on a thin glass plate with micro-ion-etching film layer, has been widely applied in recent years. A novel optical scanning system for gene disease diagnostics described in this paper has four kinds of optical devices, including beam splitters, an array lens, an array filter and detection arrays. A software was developed to design the binary optics system using an iterative method. Two beam splitters were designed and fabricated, which can divide a beam into a 9 × 9 array or into a 13 × 13 array. The beam splitters have good diffraction efficiencies (>70%) and an even energy distribution. The gene disease diagnostic system is a portable biochip and binary optics technology. The binary optical devices in the non-confocal scanning system can raise the fluorescence detection sensitivity of the micro-array hybrid biochip”"
"""AI""","“In recent times, binary optics has emerged as a versatile approach, enabling the creation of optical components on thin glass plates with micro-ion-etched film layers. This paper introduces a pioneering optical scanning system tailored for gene disease diagnostics, comprising four distinct optical components: beam splitters, an array lens, an array filter, and detection arrays. The development process involved the creation of software employing an iterative approach for designing the binary optics system. The outcome encompasses two beam splitters designed and fabricated to partition a beam into a 9×9 array or a 13×13 array. These beam splitters exhibit commendable diffraction efficiencies (>70%) and an equitable energy distribution. The resultant gene disease diagnostic system embodies a portable biochip integrated with binary optics technology. By employing binary optical devices in the non-confocal scanning setup, the fluorescence detection sensitivity of the micro-array hybrid biochip is substantially enhanced.”"
"""Human""","“The focus of this paper is on a new concept framework and an architecture of an intelligent decision support syetem generator (DSSG). The framework results from a synthesis of two existing frameworks: Spragae and Bonczek, while the architecture is a rooted partial order network. From our experience which comes out of the project of DSSG, we consider that they are keys of further research and development of DSS.”"
"""AI""","“This paper centers around introducing a novel conceptual framework and architecture for an intelligent decision support system generator (DSSG). The framework is an amalgamation of two established frameworks, namely Spragae and Bonczek, while the architecture is founded on a rooted partial order network. Drawing from insights gleaned during our involvement in the DSSG project, we view these elements as pivotal for advancing the realm of decision support system research and development.”"
"""Human""","“State of the art speaker identification systems use the Gaussian mixture models (GMM) classifier. Support vector machines (SVM) offers a competing classification algorithm. Both classification methods have been evaluated on speaker recognition tasks and have shown to produce uncor-related errors with sometimes similar performance. In this paper their performance is compared on the amount of data required for optimal performance and effect of different spectral nonlinearities in the feature-sets. The results show that for limited training data the SVM classifier is better but as the data is increased the GMM classifier outperforms it. The SVM classifier is also very expensive. This suggest that the GMM classifier will continue to be the popular classification engine for speaker recognition tasks. SVM can be used in special cases such as when there is limited data or as a secondary classifier, otherwise it is computationally very expensive and offers little in return."""
"""AI""","“The current landscape of speaker identification systems predominantly relies on the Gaussian mixture models (GMM) classifier, although the Support Vector Machines (SVM) classifier presents a competing alternative. These two classification methods have undergone evaluation in the context of speaker recognition tasks, revealing the generation of uncorrelated errors and sometimes comparable performance. This paper focuses on a comparison between these classifiers with respect to the data volume necessary for optimal performance and the impact of varying spectral nonlinearities in the feature sets. The findings indicate that the SVM classifier outperforms the GMM classifier when training data is limited, but as data quantity increases, the GMM classifier surpasses it. It's also noted that the SVM classifier is computationally intensive. Consequently, it can be inferred that the GMM classifier will remain the more favored choice for speaker recognition tasks. However, SVM may find applicability in scenarios with constrained data or as a supplementary classifier, as its computational demands outweigh its benefits in most cases.”"
"""Human""","“The paper describes work done towards the construction of a two dimensional analogue field plotter, where the field to be investigated is modelled in electrically conducting paint on conductive paper, or represented by an electrolytic tank. The detecting head of the device moves along any preselected equipotential in the analogue field, and the lines are reproduced on an X-Y recorder, By repeating the process line by line the entire field can be resolved. The process is semi-automatic, in that a preselected line will be followed automatically, but successive lines are selected manually. The movement of the detecting head is controlled from a mini-computer which has as input error signals from the probes on the head”"
"""AI""","“The paper outlines the development of a two-dimensional analogue field plotter, designed to investigate electric fields. The technique involves using electrically conductive paint on conductive paper or an electrolytic tank to represent the field under study. The device employs a detecting head that moves along chosen equipotential lines within the analogue field. These lines are subsequently transcribed onto an X-Y recorder. The complete field is obtained by repeating this process systematically. While the method is semi-automated, wherein a predetermined line is automatically followed, the subsequent lines require manual selection. The movement of the detecting head is governed by a mini-computer, which receives error signals from the probes on the head as input. This innovation serves as a valuable tool for understanding and visualizing electric fields.”"
"""Human""","“Personalized service systems are an effective way to help users obtain recommendations for unseen items, within the enormous volume of information available based on their preferences. The most commonly used personalized service system methods are collaborative filtering, content-based filtering, and hybrid filtering. Unfortunately, each method has its drawbacks. This paper proposes a new method which unified partition-based collaborative filtering and meta-information filtering. In partition-based collaborative filtering the user-item rating matrix can be partitioned into low-dimensional dense matrices using a matrix clustering algorithm. Recommendations are generated based on these low-dimensional matrices. Additionally, the very low ratings problem can be solved using meta-information filtering. The unified method is applied to a digital resource management system. The experimental results show the high efficiency and good performance of the new approach”"
"""AI""","“Personalized service systems play a crucial role in offering tailored recommendations to users amidst the vast sea of available information, aligning with their preferences. Existing approaches like collaborative filtering, content-based filtering, and hybrid filtering have their limitations. To address this, this paper introduces a novel method that combines partition-based collaborative filtering and meta-information filtering. The partition-based approach involves breaking down the user-item rating matrix into smaller, low-dimensional dense matrices through a matrix clustering algorithm. Recommendations are then generated using these condensed matrices. Additionally, the challenge of very low ratings is tackled through meta-information filtering. This unified method is applied to a digital resource management system, and experimental results showcase its remarkable efficiency and strong performance.”"
"""Human""","“Engineers tend to use different software to perform tasks such as geometry modeling, database management, numerical analysis, and visualization. This may cause decrease of productivity and loss of information during the conversion process between different data file formats. This paper presents a computer aided design (CAD) and computer aided engineering (CAE) system integration using scientific visualization tools and techniques. It deals with the development of a 3D CAD add-in for lighting analysis which uses the CAD model as 3D interface for creating a lighting scheme, processing, and visualizing 2D or 3D illuminance fields. Visualization features as color and contour mapping were developed using the visualization toolkit (VTK) toolkit. The application integrates all functionalities of the 3D CAD with tools for light sources database management, pre-processing, processing, and post-processing of illuminance fields in a single environment. This approach increases productivity and eliminates the need for different software”"
"""AI""","“Engineers commonly employ various software tools for tasks like geometry modeling, database management, numerical analysis, and visualization. However, using different software can lead to reduced productivity and information loss during the conversion of data between formats. This paper introduces a solution by integrating computer aided design (CAD) and computer aided engineering (CAE) systems through scientific visualization techniques. Specifically, it focuses on developing a 3D CAD add-in for lighting analysis, utilizing the CAD model as a 3D interface for creating lighting schemes and visualizing illuminance fields in 2D or 3D. To enhance visualization, features like color and contour mapping are implemented using the Visualization Toolkit (VTK). The application seamlessly merges 3D CAD functionalities with tools for managing light sources, pre-processing, processing, and post-processing illuminance fields within a single environment. This approach enhances productivity and eliminates the need for multiple software tools.”"
"""Human""","“For text-independent speaker verification, the Gaussian mixture model (GMM) using a universal background model strategy and the GMM using support vector machines are the two most commonly used methodologies. Recently, a new SVM-based speaker verification method using GMM super vectors has been proposed. This paper describes the construction of a new speaker verification system and investigates the use of nuisance attribute projection and test normalization to further enhance performance. Experiments were conducted on the core test of the 2006 NIST speaker recognition evaluation corpus. The experimental results indicate that an SVM-based speaker verification system using GMM super vectors can achieve appealing performance. With the use of nuisance attribute projection and test normalization, the system performance can be significantly improved, with improvements in the equal error rate from 7.780/0 to 4.920/0 and detection cost function from 0.0376 to 0.0251”"
"""AI""","“In the realm of text-independent speaker verification, the two prevalent methodologies are the Gaussian mixture model (GMM) approach using a universal background model strategy and the GMM approach employing support vector machines (SVM). A novel SVM-based speaker verification method utilizing GMM super vectors has recently emerged. This paper outlines the development of a new speaker verification system and explores the integration of nuisance attribute projection and test normalization to further enhance performance. The study was conducted on the core test of the 2006 NIST speaker recognition evaluation corpus. The results indicate that an SVM-based speaker verification system using GMM super vectors can achieve commendable performance. Incorporating nuisance attribute projection and test normalization can significantly bolster the system's performance, leading to reductions in equal error rate from 7.78% to 4.92% and in detection cost function from 0.0376 to 0.0251.”"
"""Human""","“This paper represents a design and development of a mobile sensing unit as well as its prototype implementation for railway track monitoring. The unit consists of an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer and an ADC (Analog/Digital Converter) so that the unit can trace the route while capturing an acceleration response of a passenger vehicle. The unit enables more frequent and qualitative data acquisition compared with traditional and the state of the practice railway track inspection equipments. Locating disorder is the key of our unit, which has a reasonable accuracy of positioning with GPS data, existing facilities landmarks, and car acceleration responses. The proposed unit is a promising device for efficient properties management of railway agencies. The prototype implementation shows a result that car acceleration responses are related with the track displacements in low frequencies. The results also imply that sensor settlement on a vehicle floor, not axes or bogies, is effective for capturing track vertical displacements”"
"""AI""","“This paper presents the design and development of a mobile sensing unit, along with its prototype implementation, for the purpose of monitoring railway tracks. The unit is comprised of several components, including an ultra-small personal computer (PC), a global positioning system (GPS) receiver, an accelerometer, and an Analog/Digital Converter (ADC). These components work together to track the route of a passenger vehicle while capturing its acceleration response. This approach allows for more frequent and higher-quality data collection compared to traditional railway track inspection equipment. The unit's primary function is to detect track irregularities, and it achieves this by combining GPS data, landmarks, and vehicle acceleration responses. The proposed unit offers a practical solution for efficiently managing railway infrastructure. The prototype implementation demonstrates that the acceleration responses of the vehicle are linked to track displacements at lower frequencies. Additionally, the results suggest that placing sensors on the vehicle's floor, rather than its axes or bogies, is effective for capturing vertical track displacements.”"
"""Human""","“Topology design of artificial neural networks (ANNs) is an important problem for large scale applications. This paper describes a new efficient pruning method, the multi-weight optimal brain surgeon (MWOBS) method, to optimize neural network topologies. The advantages and disadvantages of the OBS and unit-OBS were analyzed to develop the method. Actually, optimized topologies are difficult to get within reasonable times for complex problems. Motivating by the mechanism of natural neurons, the MW-OBS method balances the accuracy and the time complexity to achieve better neural network performance. The method will delete multiple connections among neurons according to the second derivative of the error function, so the arithmetic converges rapidly while the accuracy of the neural network remains high. The stability and generalization ability of the method are illustrated in a Java program. The results show that the MWOBS method has the same accuracy as OBS, but time is similar to that of unit-OBS. Therefore, the MWOBS method can be used to efficiently optimize structures of neural networks for large scale applications.”"
"""AI""","“This paper introduces a novel and efficient pruning method called the Multi-Weight Optimal Brain Surgeon (MWOBS) method, aimed at optimizing the topology of artificial neural networks (ANNs) for large-scale applications. The paper analyzes the advantages and disadvantages of existing pruning methods, such as OBS and unit-OBS, to develop this new approach. Optimizing neural network topologies for complex problems can be challenging within reasonable time frames. Inspired by the workings of natural neurons, the MW-OBS method balances accuracy and time complexity to achieve improved neural network performance. The method involves eliminating multiple connections among neurons based on the second derivative of the error function, leading to rapid arithmetic convergence while maintaining high network accuracy. The stability and generalization capabilities of the method are demonstrated through a Java program. Results indicate that the MWOBS method achieves accuracy similar to OBS while maintaining a time complexity comparable to unit-OBS. As a result, the MWOBS method offers an efficient solution for optimizing neural network structures in large-scale applications."
"""Human""","“In this paper, we study the problem of rule extraction from data sets using the rough set method. For inconsistent rules due to improper selection of split-points during discretization, and/or to lack of information, we propose two methods to remove their inconsistency based on irregular decision tables. By using these methods, inconsistent rules are eliminated as far as possible, without affecting the remaining consistent rules. Experimental test indicates that use of the new method leads to an improvement in the mean accuracy of the extracted rules.”"
"""AI""","“This paper addresses the challenge of extracting rules from data sets using the rough set method. In cases where inconsistent rules arise due to incorrect split-point selection during discretization or insufficient information, two approaches are introduced to mitigate this inconsistency using irregular decision tables. These methods aim to effectively eliminate inconsistent rules while preserving the consistency of the remaining rules. Through experimental testing, it is demonstrated that the application of these new methods results in an enhanced mean accuracy of the extracted rules.”"
"""Human""","“With the development of information technology, the fire safety assessment of whole structure or region based on the computer simulation has become a hot topic. However, traditionally, the concerned studies are performed separately for different objectives and difficult to perform an overall evaluation. A new multi-dimensional integration model and methodology for fire safety assessment were presented and two newly developed integrated systems were introduced to demonstrate the function of integration simulation technology in this paper. The first one is the analysis on the fire-resistant behaviors of whole structure under real fire loads. The second one is the study on fire evaluation and emergency rescue of campus based on geography information technology (GIS). Some practical examples are presented to illuminate the advantages of computer integration technology on fire safety assessment and emphasize some problems in the simulation. The results show that the multi-dimensional integration model offers a new way and platform for the integrating fire safety assessment of whole structure or region, and the integrated software developed is the useful engineering tools for cost-saving and safe design”"
"""AI""","“With the advancement of information technology, the use of computer simulations for fire safety assessment of entire structures or regions has gained significant attention. However, traditionally, these studies have been conducted independently for various objectives, making it challenging to achieve a comprehensive evaluation. This paper introduces a novel multi-dimensional integration model and methodology for fire safety assessment, along with the introduction of two integrated systems to showcase the capabilities of integrated simulation technology. The first system focuses on analyzing the fire-resistant behavior of entire structures under real fire loads, while the second system involves studying fire evaluation and emergency response on a campus using geographical information technology (GIS). Practical examples are provided to highlight the benefits of computer integration technology in fire safety assessment, while also addressing some challenges in the simulation process. The findings illustrate that the multi-dimensional integration model offers a fresh approach and platform for integrating fire safety assessments of entire structures or regions. The developed integrated software serves as valuable engineering tools for efficient and safe design, resulting in cost savings.”"
"""Human""","“The paper describes a first attempt at using reactively loaded, pwm converters as fast dynamic filters in a power network. The principles used are described, the current fed pwm converter used discussed and experimental results given as obtained on a variable speed drive as a non — linear load. The simulated and experimental results compare very well, and the technique appears promising for further investigation”"
"""AI""","“The paper presents an initial exploration of employing reactively loaded PWM (pulse-width modulation) converters as rapid dynamic filters within a power network. The underlying principles are outlined, focusing on the utilization of a current-fed PWM converter. The specific PWM converter utilized is discussed, and the experimental outcomes are provided, stemming from tests conducted on a variable speed drive that functions as a nonlinear load. Notably, the simulated outcomes align closely with the experimental results, suggesting substantial potential for future investigations into this technique.”"
"""Human""","“This paper attempts to explain the Discrete Fourier Transform in terms of its action on real pulse and sinusoidal data. Even real data is seen to give rise to even real spectra, and odd real data produces odd imaginary spectra. The idea of aliases, leakage and windows are examined. The Fast Fourier Transform is a method of performing the DFT and it is approached firstly by decimation in time and then by decimation in frequency. The terms twiddle factor, bit reversal, shuffling, butterflies and radix are Introduced. Application of the FFT in correlation, convolution, band limited interpolation and coherence functions is discussed. The Chirp Z Transform performed by convolution in a transversal filter is examined, as it seems to be the cheapest and fastest method of performing the DFT.”"
"""AI""","“This paper aims to provide an elucidation of the Discrete Fourier Transform (DFT) by illustrating its effects on real pulse and sinusoidal data. The paper delves into the behavior of even real spectra emerging from real data and the generation of odd imaginary spectra from odd real data. The concepts of aliases, leakage, and windows are explored. The Fast Fourier Transform (FFT) method, a technique for executing the DFT, is introduced through two approaches: decimation in time and decimation in frequency. The paper introduces terms like twiddle factor, bit reversal, shuffling, butterflies, and radix in the context of FFT. Practical applications of FFT in correlation, convolution, band-limited interpolation, and coherence functions are discussed. The Chirp Z Transform, executed through convolution in a transversal filter, is also examined, as it seems to be an efficient and fast alternative for performing the DFT.”"
"""Human""","“This paper describes a novel balanced transmission line antenna where the maximum separation between the two conductors is only a fraction of a wavelength at the lowest operating frequency. The balanced transmission line is formed by exponentially extending the ridges of a double-ridged waveguide. The poor VSWR and patterns caused by reflections from the tips of the ridge extensions are improved by a combination of aperture matching and the use of microwave absorber. Measured VSWR, radiation pattern and gain data are presented for this compact, broadband antenna over the 2 to 11 GHz frequency range”"
"""AI""","“This paper presents a unique balanced transmission line antenna design characterized by a remarkably short separation between its two conductors, which remains a small fraction of a wavelength even at the lowest operating frequency. The balanced transmission line structure is established through the exponential extension of ridges within a double-ridged waveguide. To address challenges related to high VSWR (Voltage Standing Wave Ratio) and unfavorable radiation patterns caused by reflections from the ridge extensions, a combination of aperture matching and microwave absorber is employed. The paper provides experimental measurements of VSWR, radiation patterns, and gain for this compact and wideband antenna across the frequency range of 2 to 11 GHz.”"
"""Human""","“This paper describes a computer program for the prediction of radio path loss over irregular terrain in the frequency range 20 to 10 000 MHz. Propagation modes modelled are line of sight, diffraction and troposcatter. The operation of the program it described briefly. The prediction accuracy has been analysed and the deviation between the active and predicted loss is shown to be of the same order of magnitude as the natural signal level fluctuations. Typical uses of the program and an example of system analysis are presented”"
"""AI""","“This paper introduces a computer program designed to forecast radio path loss over uneven terrain across the frequency range of 20 to 10,000 MHz. The program takes into account various propagation modes including line of sight, diffraction, and troposcatter. The paper provides a concise overview of how the program operates. The accuracy of predictions is assessed, revealing that the difference between the actual and predicted path loss aligns with the natural fluctuations in signal levels. The paper highlights potential applications of the program and includes an illustrative instance of its usage in system analysis.”"
"""Human""","“Various types of cutting tools are known and are in use for machining parts. The dimensional parameters associated with cutting tools need to be estimated and compared to the desired values for determining their cutting performance. In this paper, a data analysis methodology for extracting parameters from a measured point set corresponding to the surface of a cutting tool is provided. We propose that the 3-D data can be simplified into 2-D data or regular data by virtually slicing it at a predetermined section or by projecting it onto a same axial plane after a simple fixed-axis rotation. A plurality of curves can be generated and optimized based on the obtained 2-D points on a cross section for calculating the section parameters, including radial (axial) rake angle, relief angle, and land width. Other dimensional parameters can also be extracted from the contour of the presented rotary axial projection data. The experimental results have shown that the approaches elaborated in this paper are effective and robust, which can be potentially extended to other applications such as the inspection of similar parts and their parameters extraction”"
"""AI""","“This paper addresses the evaluation of cutting tools utilized in machining processes, focusing on their dimensional parameters and cutting performance. A data analysis methodology is proposed for extracting relevant parameters from the measured surface of a cutting tool. The approach involves simplifying the 3D data into 2D data or regular data by slicing it at a predetermined section or projecting it onto a common axial plane following a fixed-axis rotation. This yields a series of curves that can be optimized based on the 2D points from a cross section. These optimized curves aid in calculating section parameters such as radial (axial) rake angle, relief angle, and land width. Additionally, other dimensional parameters can be extracted from the contour of the rotary axial projection data. The effectiveness and robustness of these methods are demonstrated through experimental results, suggesting their applicability in similar contexts, including the inspection of similar parts and the extraction of their parameters.”"
"""Human""","“Nowadays, people are in need for continuous learning in order to keep up to date or to be upgraded in their job. An infrastructure for life-long learning requires continuous adaptation to learners' needs and must also provide flexible ways for students to use and personalize them. Controlling who can access a document, specifying when a student may be contacted for interactive instant messaging or periodical reminders in order to increase motivation for collaboration are just some examples of typical statements that may be specified by e.g., learners and learning management system administrators. This paper investigates how existing work in the area of policy representation and reasoning can be used in order to express these statements while at the same time obtaining the extra benefits policies provide (e.g., flexibility, dynamicity and interoperability). The paper analyzes existing policy languages and integrates one of them as part of a demonstration of its feasibility in providing more advanced and flexible eLearning environments”"
"""AI""","“In the current dynamic landscape, continuous learning has become imperative to remain up-to-date and excel in professional roles. To facilitate lifelong learning, a flexible infrastructure is required, tailored to learners' evolving needs and preferences. This paper explores how policy representation and reasoning concepts can be harnessed to address these requirements effectively. By incorporating policies, which enable adaptable and interoperable rules, this approach allows for personalized control over access to resources, communication methods, and collaborative engagements in eLearning environments. This study analyzes various policy languages and demonstrates the feasibility of implementing such policies to enhance the sophistication and flexibility of lifelong learning systems.”"
"""Human""","""This paper presents ND (Noise function detection) steganalysis algorithm to detect LSB steganography. The purpose of constructing the noise function is to quantify the smoothness or “regularity” of the images. ND method is based on the property that LSB embedding will increase the noise functional value of the image, and then the LSB embedding message ratio is estimated by constructing the simple line equation with the statistics of noise function in image. Experimental results show that this algorithm is more accurate and has a lower missing rate and false alarm rate than the conventional RS. Compared with RS method and some other powerful steganalysis approaches presented recently, ND method directly use the noise function to estimate the LSB embedding message ratio, neither fixed nor dynamic mask is needed. Thus, more running time is saved. The ND method is relatively faster, simpler and has good detection result”"
"""AI""","“This paper introduces an algorithm called ND (Noise function detection) for steganalysis to identify Least Significant Bit (LSB) steganography. The algorithm focuses on constructing a noise function that quantifies the regularity or smoothness of images. ND operates based on the observation that LSB embedding leads to an increase in the noise functional value within the image. It estimates the ratio of LSB-embedded messages by creating a simple linear equation using the noise function statistics from the image. The experimental outcomes demonstrate that ND achieves greater accuracy with lower rates of both missing and false alarms compared to conventional RS methods. Unlike other recent steganalysis approaches, ND doesn't require fixed or dynamic masks and directly utilizes the noise function to estimate LSB embedding message ratios, making it more efficient in terms of both runtime and simplicity.”"
"""Human""","“High frequency surface wave radar (HFSWR) is well proved to have over the horizon (OTH) detection capability to weak aerial targets, such as concealed airplanes or cruise missiles. The most important problem of detection of fast and small targets using HFSWR is earlier warning, i. e. enlargement of detection range of targets. Therefore, the detection threshold should be decreased as low as possible, but numerous false alarms are brought about at the same time. On this condition, conventional track initiation techniques, which normally require the probability of false alarm to be at the level of 10–6, will initiate enormous false tracks and lead to abnormal operation of tracking system. An adaptive modified hough transform (AMHT) track initiator is proposed accordingly and the relation of detection range to the performance of track initiator is analyzed in this paper. Simulations are performed to confirm the capability of track initiation to fast and small targets in dense clutter by AMHT track initiator. The tolerable probability of false alarm of detector can reach the level of 10 −3 . And it performs better than track initiator based on modified hough transform (MHT)”"
"""AI""","“High Frequency Surface Wave Radar (HFSWR) has proven its ability to detect weak aerial targets beyond the horizon, such as concealed airplanes or cruise missiles. A key challenge in detecting fast and small targets using HFSWR is achieving early warning and extending the detection range for these targets. This requires lowering the detection threshold as much as possible, which can result in numerous false alarms. This situation poses a problem for conventional track initiation techniques that usually demand a very low false alarm probability, around 10^-6, as they can generate excessive false tracks and disrupt the tracking system's operation. To address this, the paper introduces an Adaptive Modified Hough Transform (AMHT) track initiator and examines how the detection range relates to the track initiator's performance. Through simulations, the paper demonstrates that the AMHT track initiator is capable of initiating tracks for fast and small targets in dense clutter. It achieves a tolerable false alarm probability of around 10^-3, outperforming track initiators based on Modified Hough Transform (MHT).”"
"""Human""","“The effect of range-Dapper coupling caused by aircraft moving at very high speed makes trouble on selection of waveform parameters by using frequency-modulated interrupted continuous wave (FNHCW) or frequency-coded pulse (FCP). It also limits the increasing of coherent intergration time. In this paper, application of coherent phase-coded pluse train (CPCPT) solves range-Doppler coupling well. Relevant processing of CPCPT consists of three parts: Dopper preprocessing, pulse compression, and Doppler post-processing. The velocity information obtained by Doppler preprocessing is used for better pulse compression and range tracking. Doppler post-processing with range tracking could make longer coherent accumulation for better detection of target and higher velocity resolution. Finally, examples of data simulation are given to demonstrate the achievements mentioned above.”"
"""AI""","“The issue of range-Doppler coupling caused by aircraft moving at high speeds poses challenges for selecting waveform parameters in applications like frequency-modulated interrupted continuous wave (FNHCW) or frequency-coded pulse (FCP). This limitation also hampers the ability to increase coherent integration time. This paper introduces the use of Coherent Phase-Coded Pulse Train (CPCPT) to effectively address the range-Doppler coupling problem. The processing of CPCPT involves three main stages: Doppler preprocessing, pulse compression, and Doppler post-processing. The velocity information obtained through Doppler preprocessing enhances pulse compression and range tracking. Doppler post-processing coupled with range tracking allows for longer coherent accumulation, resulting in improved target detection and higher velocity resolution. The paper concludes with simulation examples illustrating the benefits achieved through these methods.”"
"""Human""","“The multimedia conferencing system is an important tool for Computer supported Cooperative Work (CSCW), and also one of the basic techniques supporting team work in the Concurrent Engineering. In this paper the architecture and function modules of the reported multimedi a conferencing system are described. Then in order to reduce the network bandwidth required by the conferencing system, two methods for sending the conferencing data by using multicasting are discussed”"
"""AI""","“The multimedia conferencing system plays a crucial role in Computer Supported Cooperative Work (CSCW) and serves as a fundamental technique for facilitating team collaboration in Concurrent Engineering. This paper outlines the architecture and functional components of the multimedia conferencing system under consideration. To address the challenge of minimizing the network bandwidth consumption of the conferencing system, the paper investigates two approaches for transmitting conferencing data through multicasting methods.”"
"""Human""","“Grey sequence generation can draw out and develop implied rules of the original data. Different kinds of generation methods were summarized and classified into two types: partial generation and whole generation. The average generation and stepwise ratio generation is disussed, the preference generation is regard as a special case of proportional division based on analysis geometric theory, propose an idea of using concave and convex status of discrete data to determine the generation coefficient. Based on the stepwise and smooth ratio generation, a tendency average generation is proposed and have a comparison using the data provided in papers listed in the references. The comparison proves that the new generation is better than the other two generations and errors are obviously reduced.”"
"""AI""","“The process of generating grey sequences holds the potential to reveal and elucidate implicit patterns within the original data. Various methods for generating grey sequences have been identified and categorized into two main types: partial generation and whole generation. This paper delves into the details of average generation and stepwise ratio generation methods, while also exploring the concept of preference generation as a specific instance of proportional division rooted in geometric theory analysis. Additionally, the paper introduces a novel concept of employing the concave and convex characteristics of discrete data to determine the generation coefficient. Building upon the principles of stepwise and smooth ratio generation, a novel approach known as tendency average generation is put forward. To substantiate its effectiveness, this new method is compared against the other two methods using data extracted from the referenced papers. The comparative analysis demonstrates that the proposed tendency average generation outperforms the other methods, leading to a notable reduction in errors.”"
"""Human""",“A new aim-points-choosing model for attacking mobile missile systems in sector region is presented and realized in this paper. The model has the generality for attacking mobile target and is also useful for making an operational decision.”
"""AI""","“In this paper, a novel model for selecting aim points when attacking mobile missile systems within a specific sector region is introduced and implemented. This model possesses a versatile capability for targeting mobile objects and is equally valuable for informing operational decisions.”"
"""Human""","“Job planning (JP) systems shop-oriented provide a basis for job shop scheduling and control in organizing short-term production activities. This paper presents a method based on timed Petri net (TPN) method that is used to program optimal JP for assembly shop. It includes three parts further. Firstly, an architecture of solutions to JP problems for any kind of shop oriented is presented to define a particular JP for a designated JP problem. Secondly, Petri net model is specified for aircraft-part assembly processes. Finally, algorithms for optimizing generation of dynamic mechanism and a simulating case are then discussed. In comparison with traditional methods such as PERT or CPM, it is obviously convenient for planners or schedulers to schedule and manage assembly processes.”"
"""AI""","“This paper proposes an optimized job planning (JP) method for assembly shops through the utilization of timed Petri nets (TPN), offering a tailored approach to address short-term production activities and enhance job shop scheduling and control. The paper comprises three key segments: Firstly, it introduces a comprehensive solution framework applicable to diverse shop-oriented scenarios, enabling the customization of JP solutions for specific challenges. Secondly, a Petri net model is devised to depict the intricacies of aircraft-part assembly processes. Lastly, the paper discusses algorithms for dynamic optimization of mechanism generation and presents a simulation case study, highlighting the comparative advantage of the TPN approach over traditional methods like Program Evaluation and Review Technique (PERT) or Critical Path Method (CPM) in simplifying assembly process management and scheduling for planners and schedulers.”"
"""Human""","“Enterprise management information system (EMIS) in Manufacturing CIMS Integrating Platform (MACIP), refers to a computer system that manages the information for running an enterprise. A typical EMIS consists of a group of closely connected functions such as production planning, material management, accounting, quality management, etc. The EMIS exchanges information with the CAD/CAPP system in the design department, and the shop floor controller (SFC) in the manufacturing department, while the global information system (GIS) of MACIP supplies the mechanism for information sharing within the enterprise. This paper introduces the EMIS model for a typical manufacturing enterprise, then analyses the interface of the EMIS with the CAD/CAPP system and the SFC. A technical scheme for integrating the EMIS with the GIS is given. This scheme considers the integration of some MRPI systems in the market, and adopts advanced industrial standards to ensure its flexibility and reusability.”"
"""AI""",“In this paper a wavelet-based fuzzy neural network for interval estimation of processed data with its interval learning algorithm is proposed. It is also proved to be an efficient approach to calculate the wavelet coefficient.”
"""Human""","“The intensive research work into Surface Acoustic Wave (SAW) technology over the last decade has introduced many new possibilities to the system designer. Currently most of the major electronic systems firms in the USA and a large number in Europe, have some research effort devoted to SAW. The aim of this paper is to indicate some of the application areas of SAW devices both in the military and civil fields with specific reference to radar and communication applications.”"
"""AI""","“Over the past decade, extensive research has been conducted in the realm of Surface Acoustic Wave (SAW) technology, opening up numerous opportunities for system designers. Presently, numerous prominent electronics companies in the USA and a substantial portion in Europe are dedicating research to SAW technology. This paper aims to highlight the diverse application domains of SAW devices, spanning both military and civilian sectors, with a particular focus on their relevance in radar and communication applications.”"
"""Human""","“Today's firewalls and security gateways are required to not only block unauthorized accesses by authenticating packet headers, but also inspect flow payloads against malicious intrusions. Deep inspection emerges as a seamless integration of packet classification for access control and pattern matching for intrusion prevention. The two function blocks are linked together via well-designed session lookup schemes. This paper presents an architecture-aware session lookup scheme for deep inspection on network processors (NPs). Test results show that the proposed session data structure and integration approach can achieve the OC-48 line rate (2.5 Gbps) with inline stateful content inspection on the Intel IXP2850 NP. This work provides an insight into application design and implementation on NPs and principles for performance tuning of NP-based programming such as data allocation, task partitioning, latency hiding, and thread synchronization”"
"""AI""","“Modern firewalls and security gateways are confronted with the dual challenge of not only thwarting unauthorized access through packet header authentication but also scrutinizing flow payloads for potential malicious intrusions. The concept of deep inspection has emerged as a unified approach, seamlessly combining packet classification for access control with pattern matching for intrusion prevention. This paper introduces a session lookup mechanism tailored to the architecture for deep inspection on network processors (NPs). The integration of the two crucial functions relies on well-structured session lookup techniques. Experimental results demonstrate that the proposed session data structure and integration strategy achieve remarkable performance, attaining OC-48 line rates (2.5 Gbps) with real-time stateful content inspection on the Intel IXP2850 NP. This work not only offers valuable insights into the design and implementation of applications on NPs but also outlines guiding principles for optimizing NP-based programming encompassing aspects such as data allocation, task distribution, latency management, and synchronization of threads.”"
"""Human""","“This paper presents a brief summary of the three development stages of investigation the transpiration cooling and its control for aircraft, missiles and electromagnetic gun, then the control problem of the distributed parameters system with a moving boundary is derived. It introduces the mathematical model of the transpiration cooling control, its control characteristics, and the present situation of the experimental and theoretical study on this problem. This paper also describes the main study results and the existing problems. The prospective application is also reported. The major references in every developing stage are listed systematically for further study.”"
"""AI""","“This paper offers a concise overview of the evolution of transpiration cooling investigation through three developmental stages for applications in aircraft, missiles, and electromagnetic guns. It subsequently delves into the control challenges associated with distributed parameter systems featuring moving boundaries. The mathematical model of transpiration cooling control is presented, encompassing its control attributes, along with an exploration of ongoing experimental and theoretical research in this domain. The paper further highlights key research outcomes and lingering challenges, providing insights into potential future applications. A systematic compilation of primary references across each developmental phase is also included to facilitate further exploration of the topic.”"
"""Human""","“This paper examines the problem of designing an underground mine so as to optimise the development and haulage costs. It focusses particularly on the costs associated with the ramps and shafts which provide passage to and from the ore zones. This mine optimisation problem is modelled as a weighted network. The controls (variables for the optimisation process) and operational constraints are described. A main constraint in mining networks is that all ramps have gradients no more than a given maximum value m. In this paper we describe the mine design problem as an optimisation problem, and prove that under reasonable conditions the cost function of an underground mining network with maximum gradient constraint m is convex. The convexity of the objective function ensures the existence of minimum cost mining networks, and theoretically any descent algorithms for finding minimal points can be applied to the design of minimum cost mining networks.”"
"""AI""","“This paper explores the optimization challenge of designing an underground mine to efficiently manage development and haulage costs, with a specific emphasis on the expenses linked to ramps and shafts facilitating movement within ore zones. The optimization task is conceptualized as a weighted network, wherein control variables and operational limitations play a crucial role. Notably, mining networks are bound by the constraint that all ramps maintain gradients within a defined maximum value denoted as ""m."" The paper formulates the mine design dilemma as an optimization quandary and establishes that, under reasonable conditions, the cost function associated with an underground mining network adhering to the maximum gradient constraint ""m"" is convex. This convexity characteristic of the objective function ensures the presence of minimum cost mining networks, thereby enabling the utilization of descent algorithms to identify optimal solutions for the design of cost-effective mining networks.”"
"""Human""","“This paper theoretically analyzes a deficiency of the existing scheme, and proposes a distributed multi-hop certification authority scheme for mobile Ad Hoc networks. In our design, we distribute the certification authority functions through a threshold secret sharing mechanism, in which each node holds a secret share and multiple nodes jointly provide complete services. Certification authority is not limited in a local neighborhood but can be completed within multi-hop location. In addition, we replace broadcast by multicast to improve system performance and reduce communication overhead. This paper resolves some technical problems of ubiquitous certification authority services, and presents a wieldy multi-hop certification authority algorithm. Simulation results confirm the availability and effectiveness of our design.”"
"""AI""","“This paper offers a theoretical analysis of an existing scheme's limitations and introduces a novel approach called the distributed multi-hop certification authority scheme for mobile Ad Hoc networks. The proposed design involves distributing certification authority functions using a threshold secret sharing mechanism, where individual nodes possess secret shares and collective nodes collaborate to offer comprehensive services. This novel scheme extends the reach of certification authority beyond local neighborhoods, enabling its operation across multi-hop distances. Furthermore, the conventional broadcasting method is replaced with multicast, enhancing system efficiency and reducing communication overhead. By addressing challenges related to ubiquitous certification authority services, the paper introduces a practical multi-hop certification authority algorithm. Simulation results substantiate the viability and efficacy of the proposed design."